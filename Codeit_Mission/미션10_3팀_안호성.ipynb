{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bcZc42Acap0"
      },
      "source": [
        "# 텍스트 임베딩과 텍스트 분류"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5JWyAnQdj2b"
      },
      "source": [
        "## 개요\n",
        "\n",
        "- 사용 데이터: News Group 20\n",
        "\n",
        "- 설계\n",
        "| 태스크 | 기술 |\n",
        "|----------------|-----------------------------|\n",
        "| 데이터 전처리 | 데이터 로드, 전처리 |\n",
        "| 토큰화 | nltk, BPE |\n",
        "| 임베딩 | World2Vec, FastText, Glove  |\n",
        "| 시퀀스 처리 | LSTM, GRU  |\n",
        "| 출력 | 소프트맥스 분류  |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho_-cURBi2An"
      },
      "source": [
        "# 1. 데이터 전처리 및 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goSLr18EJcad",
        "outputId": "37aa1a0b-fc71-4b48-ff1f-9f20ba3aa1ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UONlBt_AbFnB"
      },
      "outputs": [],
      "source": [
        "# 파일 경로 및 파일 읽기 라이브러리\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# 모델 다운로드\n",
        "from gensim.models import Word2Vec, FastText\n",
        "\n",
        "# 토큰 관련 라이브러리\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from tokenizers.normalizers import Lowercase, Sequence\n",
        "\n",
        "# 사이킷런 관련 라이브러리\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# 파이토치 관련 라이브러리\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 훈련 시 시각화\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Gx0MqW8QXZi"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Config 클래스 정의\n",
        "\n",
        "- 상수 변수 정의\n",
        "- @dataclass 데코레이터 추가\n",
        "\"\"\"\n",
        "@dataclass\n",
        "class Config:\n",
        "    # 데이터 폴더 생성\n",
        "    root = Path(\".\")\n",
        "    raw_dir = root / \"data\"\n",
        "    data_dir = raw_dir / \"20news-bydate\"\n",
        "    train_dir = data_dir / \"20news-bydate-train\"\n",
        "    test_dir = data_dir / \"20news-bydate-test\"\n",
        "    model_dir = root / \"models\"\n",
        "\n",
        "    # 데이터로더용 변수 선언\n",
        "    batch_size = 16\n",
        "    num_workers = 2\n",
        "    max_len = 512\n",
        "\n",
        "    # 데이터 분리용 변수 선언\n",
        "    seed = 42\n",
        "\n",
        "    # 학습용 변수 선언: 학습률, 에포크, 인내심, 최소 개선 폭\n",
        "    lr = 1e-3\n",
        "    epochs = 11\n",
        "    patience = 3\n",
        "    min_delta = 0.01\n",
        "\n",
        "    # 디바이스 설정\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wf-eejLTbJAh"
      },
      "outputs": [],
      "source": [
        "# 변수 생성 및 폴더 생성\n",
        "cfg = Config()\n",
        "\n",
        "# 모델, 데이터 저장 폴더\n",
        "cfg.model_dir.mkdir(parents=True, exist_ok=True)\n",
        "cfg.raw_dir.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjqq30Q1Jcaf"
      },
      "source": [
        "## 1.1. 데이터 다운로드 및 데이터셋 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNjYAU5fJcag",
        "outputId": "a5e0e4b3-2596-4f40-c9a7-af460af81897"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-273363852.py:7: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
            "  t.extractall(path=f\"{str(cfg.raw_dir)}/20news-bydate\")\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "데이터 다운로드\n",
        "\n",
        "- 다운로드 이슈로 직접 다운로드\n",
        "- https://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz\n",
        "\"\"\"\n",
        "with tarfile.open(f\"{str(cfg.raw_dir)}/20news-bydate.tar.gz\", \"r:gz\") as t:\n",
        "    t.extractall(path=f\"{str(cfg.raw_dir)}/20news-bydate\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Pr5-IDBJcag"
      },
      "outputs": [],
      "source": [
        "# 데이터셋 클래스 정의\n",
        "class DocData(Dataset):\n",
        "    def __init__(self, data_dir: Path):\n",
        "        \"\"\"\n",
        "        데이터셋 클래스 초기화\n",
        "\n",
        "        Args:\n",
        "            data_dir (Path): 데이터 디렉토리 경로\n",
        "        \"\"\"\n",
        "        self.data_dir = data_dir\n",
        "\n",
        "        # 데이터셋 내 모든 파일 경로 수집\n",
        "        self.file_paths = [p for p in self.data_dir.rglob(\"*\") if p.is_file()]\n",
        "\n",
        "        # 클래스 정수 라벨 매핑\n",
        "        self.classes = sorted({p.parent.name for p in self.file_paths})\n",
        "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        데이터셋 크기 반환\n",
        "\n",
        "        Returns:\n",
        "            int: 데이터셋 크기\n",
        "        \"\"\"\n",
        "        return len(self.file_paths)\n",
        "\n",
        "    def __getitem__(self, idx: int) ->  tuple[str, str]:\n",
        "        \"\"\"\n",
        "        인덱스에 해당하는 문서 반환\n",
        "\n",
        "        Args:\n",
        "            idx (int): 문서 인덱스\n",
        "\n",
        "        Returns:\n",
        "            str: 문서 내용\n",
        "        \"\"\"\n",
        "        file_path = self.file_paths[idx]\n",
        "        label = self.class_to_idx[file_path.parent.name]\n",
        "        with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
        "            content = f.read()\n",
        "        return content, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10-FVlcDJcag"
      },
      "outputs": [],
      "source": [
        "train_data = DocData(cfg.train_dir)\n",
        "texts = train_data[0]\n",
        "labels = train_data.classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fFZdZiJJcah",
        "outputId": "3fab7d0f-fa11-42d3-8364-8c2621111b59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "문서 길이: 2\n",
            "문서 샘플: \n",
            "('From: mtt@kepler.unh.edu (Matthew T Thompson)\\nSubject: music censorship survey - please fill out\\nOrganization: University of New Hampshire  -  Durham, NH\\nLines: 68\\nNNTP-Posting-Host: kepler.unh.edu\\n\\nHello, I\\'m doing a paper on censorship in music and I would appreciate it if you took the time to participate in this survey.  Please answer as each question asks (\\'why?\\' simply means that you have room to explain your answer, if you chose.).  The last question is for any comments, questions, or suggestions.  Thank you in advance, please E-mail to the address at the end.\\n\\nI)  are you [male/female]\\nII) what is your age? \\nIII)what is your major/occupation?\\nIV) what type of music do you listen to (check all that apply)?\\n      a.  hard rock   b.  metal   c.  alternative   d.  blues    e.  rap\\n      f.  jazz    g.  soft rock   h.  easy listening   i.  country   \\n      j.  classical   k.  hard core   l.  dance   m.  new age\\n      n.  others (did I miss any?)____________\\n\\n1)  Do you think recordings with objectionable or offensive lyrics be labeled? [yes/no] Why?\\n\\n\\n\\n\\n2)  Do you think certain recordings should be banned from minors (under 18 years of age)? [yes/no] why?\\n\\n\\n\\n\\n3)  Do you think certain recordings should be banned.  Period.  [yes/no]  Why?\\n\\n\\n\\n\\n4)  If yes to any of the above, who should decide:\\n       a. parents\\n       b. government\\n       c. music industry\\n       d. other________________\\n\\nfeel free to add any comments on this.\\n\\n\\n\\n\\n\\n5)  Do you think [more/less] should be done for controling record sales, or do you think the present labeling system is enough?  \\n\\n\\n\\n\\n\\n6)  What is your definition of censorship?  Also, feel free to add comments, suggestions, questions, or further explanations.\\n\\n\\n\\n\\n\\n\\n\\n\\nPlease E-mail at: mtt@kepler.unh.edu or hit \\'R\\' to reply.\\n\\nthanks.\\nMatthew T. Thompson\\n\\n\\ndisclaimer:  if any responses are used in paper, they will be anoynamous (sp?) unless the person specifies they what their name to be used.\\n\\n\\n-- \\n*************This .sig is closed for repairs********************************\\n-----------------------------------------------------------------------------\\n ution,|  } Matthew T. Thompson rrrrrrr!   *pound, pound, thud* \"OUCH\"$%#@\"duh?\"\\nE-mail at mtt@kepler.unh.edu or shazam@unh.edu\\n\\n', 9)\n",
            "라벨: 20\n"
          ]
        }
      ],
      "source": [
        "print(f\"문서 길이: {len(texts)}\")\n",
        "print(f\"문서 샘플: \\n{texts}\")\n",
        "print(f\"라벨: {len(labels)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcAXv6HqJcah",
        "outputId": "9be46aa3-fe82-43bc-bed8-4866c94dd209"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "테스트 데이터 길이: 7532\n"
          ]
        }
      ],
      "source": [
        "test_data = DocData(cfg.test_dir)\n",
        "print(f\"테스트 데이터 길이: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGE9WrpDJcah"
      },
      "source": [
        "## 1.2. NLTK 기반 단어 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yoUJHf3xJcah",
        "outputId": "c8a6938a-0c84-460c-abcc-c9f09f332f41"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/ahnhs2k/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     /home/ahnhs2k/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/ahnhs2k/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# 필요한 NLTK 리소스 다운로드\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# 불용어 집합 (한 번만 생성)\n",
        "stop_words = set(stopwords.words(\"english\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhJ_qAc_Jcah"
      },
      "outputs": [],
      "source": [
        "# 텍스트 전처리 함수 정의\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    텍스트 전처리 함수\n",
        "    - 소문자 변환\n",
        "    - 불필요한 공백 정리\n",
        "    - 과도한 특수문자 제거는 하지 않음\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiZpdo0YJcah"
      },
      "outputs": [],
      "source": [
        "# 토큰화 함수\n",
        "def tokenize(text: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    토큰화 함수\n",
        "    - NLTK word_tokenize 사용\n",
        "    - 불용어 제거\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [t for t in tokens if t not in stop_words]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bp0qi7aJcah"
      },
      "source": [
        "## 1.3. BPE 기반 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6hDTUREJcah"
      },
      "outputs": [],
      "source": [
        "# BPE 토크나이저 학습 함수\n",
        "def train_bpe_tokenizer(\n",
        "    texts: list[str],\n",
        "    vocab_size: int = 20000,\n",
        "    min_freq: int = 2\n",
        ") -> Tokenizer:\n",
        "    \"\"\"\n",
        "    BPE 토크나이저 학습\n",
        "    - vocab_size: 서브워드 vocab 최대 크기\n",
        "    - min_freq: 최소 등장 빈도\n",
        "    \"\"\"\n",
        "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
        "\n",
        "    # 정규화: 소문자화\n",
        "    tokenizer.normalizer = Sequence([Lowercase()])\n",
        "\n",
        "    # 1차 분리: 공백 기준\n",
        "    tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "    trainer = BpeTrainer(\n",
        "        vocab_size=vocab_size,\n",
        "        min_frequency=min_freq,\n",
        "        special_tokens=[\"[PAD]\", \"[UNK]\"]\n",
        "    )\n",
        "\n",
        "    tokenizer.train_from_iterator(texts, trainer)\n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUMJXL18Jcah"
      },
      "outputs": [],
      "source": [
        "# BPE 토크나이저를 위한 텍스트 리스트 생성\n",
        "texts_for_bpe = [text for text, _ in train_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88Mf6C2DJcah",
        "outputId": "84863edd-909d-48d4-8357-b6a7752889e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "BPE 토크나이저 vocab size: 20000\n"
          ]
        }
      ],
      "source": [
        "# BPE 토크나이저 생성\n",
        "bpe_tokenizer = train_bpe_tokenizer(texts_for_bpe)\n",
        "bpe_tokenizer_path = cfg.model_dir / \"bpe_tokenizer.json\"\n",
        "bpe_tokenizer.save(str(bpe_tokenizer_path))\n",
        "print(f\"BPE 토크나이저 vocab size: {bpe_tokenizer.get_vocab_size()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ceh7cNPJcah"
      },
      "outputs": [],
      "source": [
        "# BPE 토큰화 함수\n",
        "def bpe_tokenize(text: str) -> list[int]:\n",
        "    \"\"\"\n",
        "    BPE 토큰화 함수\n",
        "    - 텍스트를 BPE 토크나이저로 토큰화\n",
        "    \"\"\"\n",
        "    encoding = bpe_tokenizer.encode(text)\n",
        "    return encoding.ids\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_B5OOiNJcah",
        "outputId": "bd459f15-2f4c-4e66-885d-0336ccad718c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "원본 텍스트: From: mtt@kepler.unh.edu (Matt\n",
            "토큰 아이디: [193, 34, 59, 1023, 40, 17681, 22, 14622, 22, 196, 16, 3278, 66, 9417, 17, 259, 34, 4312, 15380, 6357, 21, 755, 2949, 211, 271, 34, 410, 130, 438, 14834]\n"
          ]
        }
      ],
      "source": [
        "# 인코딩 테스트\n",
        "test_t, test_l = train_data[0]\n",
        "test_ids = bpe_tokenize(test_t)\n",
        "\n",
        "print(f\"원본 텍스트: {test_t[:30]}\")\n",
        "print(f\"토큰 아이디: {test_ids[:30]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piURUJ61UaQx"
      },
      "source": [
        "# 2. 임베딩"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBdREhudJcai"
      },
      "source": [
        "## 2.1. Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hwEgb0lJcai"
      },
      "outputs": [],
      "source": [
        "# 학습용 코퍼스 준비\n",
        "train_sentences = [tokenize(text) for text, _ in train_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCKIemrMJcai",
        "outputId": "07668a8e-651e-4b04-916a-82315072910c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec vocab 사이즈: 88479\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Word2Vec 모델 생성 및 학습\n",
        "- sg=1 : Skip-gram 방식\n",
        "- vector_size : 임베딩 차원\n",
        "- window : 주변 단어 탐색 범위\n",
        "- min_count : 최소 등장 빈도 (희귀 단어 제거)\n",
        "- workers : 멀티프로세싱 스레드 수\n",
        "\"\"\"\n",
        "\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=train_sentences,  # 토큰 시퀀스\n",
        "    vector_size=300,            # 임베딩 차원\n",
        "    window=5,                   # 컨텍스트 윈도우 크기\n",
        "    min_count=2,                # 최소 등장 빈도\n",
        "    workers=4,                  # 병렬 처리\n",
        "    sg=1                        # Skip-gram 사용\n",
        ")\n",
        "\n",
        "# 학습 완료 후 vocab 크기 출력\n",
        "print(\"Word2Vec vocab 사이즈:\", len(w2v_model.wv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc066LKVJcai",
        "outputId": "197deee6-0dbb-44ff-a776-7ff2c620d32a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word2Vec vector shape: (300,)\n"
          ]
        }
      ],
      "source": [
        "# Word2Vec 임베딩 벡터\n",
        "vec_w2v = w2v_model.wv[\"name\"]\n",
        "print(\"Word2Vec vector shape:\", vec_w2v.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv9z7SxoJcai",
        "outputId": "99f6f2c8-44e4-4aac-ab04-293fff41e70e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "최종 Vocab 사이즈: 88481\n"
          ]
        }
      ],
      "source": [
        "# 특수 토큰 먼저 정의\n",
        "w2v_token_to_id = {\n",
        "    \"[PAD]\": 0,\n",
        "    \"[UNK]\": 1,\n",
        "}\n",
        "\n",
        "# Word2Vec vocab 그대로 추가\n",
        "for token in w2v_model.wv.index_to_key:\n",
        "    w2v_token_to_id[token] = len(w2v_token_to_id)\n",
        "\n",
        "w2v_vocab_size = len(w2v_token_to_id)\n",
        "print(\"최종 Vocab 사이즈:\", w2v_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRCcjZp_Jcai"
      },
      "outputs": [],
      "source": [
        "# Word2Vec에서 학습된 임베딩 차원\n",
        "w2v_embed_dim = w2v_model.vector_size\n",
        "\n",
        "# 임베딩 행렬 초기화\n",
        "# 각 행 = 하나의 토큰 ID에 대응하는 임베딩 벡터\n",
        "w2v_embedding_matrix = np.zeros(\n",
        "    (w2v_vocab_size, w2v_embed_dim),\n",
        "    dtype=np.float32\n",
        ")\n",
        "\n",
        "# w2v_token_to_id 순서에 맞춰 임베딩 채우기\n",
        "for token, idx in w2v_token_to_id.items():\n",
        "    if token in w2v_model.wv:\n",
        "        # Word2Vec이 학습한 단어면 해당 벡터 사용\n",
        "        w2v_embedding_matrix[idx] = w2v_model.wv[token]\n",
        "    else:\n",
        "        # [PAD], [UNK] 같은 특수 토큰은\n",
        "        # 작은 랜덤값으로 초기화\n",
        "        w2v_embedding_matrix[idx] = np.random.normal(\n",
        "            scale=0.01,\n",
        "            size=(w2v_embed_dim,)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aI9g9t4Jcai"
      },
      "outputs": [],
      "source": [
        "# numpy -> torch tensor 변환\n",
        "w2v_embedding_tensor = torch.tensor(w2v_embedding_matrix)\n",
        "\n",
        "# 사전 학습된 임베딩을 사용하는 Embedding 레이어\n",
        "w2v_embedding_layer = nn.Embedding.from_pretrained(\n",
        "    w2v_embedding_tensor,\n",
        "    freeze=False,\n",
        "    padding_idx=w2v_token_to_id[\"[PAD]\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jm40XxZDJcai"
      },
      "outputs": [],
      "source": [
        "# 모델 입력용 토큰-아이디 변환 함수\n",
        "def w2v_tokens_to_ids(tokens: list[str]) -> list[int]:\n",
        "    \"\"\"\n",
        "    토큰 리스트를 정수 ID 리스트로 변환\n",
        "    - 사전에 없는 토큰은 [UNK] ID로 치환\n",
        "    \"\"\"\n",
        "    unk_id = w2v_token_to_id[\"[UNK]\"]\n",
        "    return [w2v_token_to_id.get(token, unk_id) for token in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otncwD1qJcai"
      },
      "source": [
        "## 2.2. FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guYfLexpJcai",
        "outputId": "008752ff-ad93-43fe-e5db-8fcfb833fe19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FastText vocab 사이즈: 88479\n"
          ]
        }
      ],
      "source": [
        "# FastText는 n-gram(subword)을 함께 학습.\n",
        "\n",
        "ft_model = FastText(\n",
        "    sentences=train_sentences,   # 토큰 시퀀스\n",
        "    vector_size=300,             # 임베딩 차원\n",
        "    window=5,                    # 컨텍스트 윈도우 크기\n",
        "    min_count=2,                 # 최소 등장 빈도\n",
        "    workers=4                    # 병렬 처리\n",
        ")\n",
        "\n",
        "# 학습 완료 후 vocab 크기 출력\n",
        "print(\"FastText vocab 사이즈:\", len(ft_model.wv))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA4KPcoAJcai",
        "outputId": "c4efeb3d-da51-4520-9475-bef5c9237091"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FastText vector shape: (300,)\n"
          ]
        }
      ],
      "source": [
        "vec_ft = ft_model.wv[\"computer\"]\n",
        "print(\"FastText vector shape:\", vec_ft.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTZYPGvfJcai"
      },
      "outputs": [],
      "source": [
        "# 특수 토큰 먼저 정의\n",
        "ft_token_to_id = {\n",
        "    \"[PAD]\": 0,\n",
        "    \"[UNK]\": 1,\n",
        "}\n",
        "\n",
        "# FastText가 학습한 vocab 그대로 사용\n",
        "for token in ft_model.wv.index_to_key:\n",
        "    ft_token_to_id[token] = len(ft_token_to_id)\n",
        "\n",
        "ft_vocab_size = len(ft_token_to_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYnIusogJcai"
      },
      "outputs": [],
      "source": [
        "# FastText에서 학습된 임베딩 차원\n",
        "ft_embed_dim = ft_model.vector_size\n",
        "\n",
        "ft_embedding_matrix = np.zeros((ft_vocab_size, ft_embed_dim), dtype=np.float32)\n",
        "\n",
        "for token, idx in ft_token_to_id.items():\n",
        "    if token in ft_model.wv:\n",
        "        ft_embedding_matrix[idx] = ft_model.wv[token]\n",
        "    else:\n",
        "        # [PAD], [UNK]\n",
        "        ft_embedding_matrix[idx] = np.random.normal(\n",
        "            scale=0.01,\n",
        "            size=(ft_embed_dim,)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VygPNSf1Jcai"
      },
      "outputs": [],
      "source": [
        "# numpy -> torch tensor 변환\n",
        "ft_embedding_tensor = torch.tensor(ft_embedding_matrix)\n",
        "\n",
        "ft_embedding_layer = nn.Embedding.from_pretrained(\n",
        "    ft_embedding_tensor,\n",
        "    freeze=False,\n",
        "    padding_idx=ft_token_to_id[\"[PAD]\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVGmZemkJcai"
      },
      "outputs": [],
      "source": [
        "# 모델 입력용 토큰-아이디 변환 함수\n",
        "def ft_tokens_to_ids(tokens: list[str]) -> list[int]:\n",
        "    \"\"\"\n",
        "    토큰 리스트를 정수 ID 리스트로 변환\n",
        "    - 사전에 없는 토큰은 [UNK] ID로 치환\n",
        "    \"\"\"\n",
        "    unk_id = ft_token_to_id[\"[UNK]\"]\n",
        "    return [ft_token_to_id.get(token, unk_id) for token in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNFxJY6uJcal"
      },
      "source": [
        "## 2.3. Glove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR87tJCRJcal",
        "outputId": "63025130-7fc7-4c2a-c01a-f4b449ba4692"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GloVe embeddings downloaded and extracted.\n"
          ]
        }
      ],
      "source": [
        "# GloVe 다운로드\n",
        "GLOVE_URL = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
        "GLOVE_ZIP = cfg.model_dir / \"glove.6B.zip\"\n",
        "GLOVE_FILE = cfg.model_dir / \"glove.6B.300d.txt\"\n",
        "\n",
        "urllib.request.urlretrieve(GLOVE_URL, GLOVE_ZIP)\n",
        "with zipfile.ZipFile(GLOVE_ZIP, 'r') as zip_ref:\n",
        "    zip_ref.extractall(cfg.model_dir)\n",
        "\n",
        "print(\"GloVe embeddings downloaded and extracted.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spDfXrgIJcal"
      },
      "outputs": [],
      "source": [
        "# GloVe 임베딩 로드 함수 정의\n",
        "def load_glove_embeddings(glove_path: str, embedding_dim: int) -> dict[str, np.ndarray]:\n",
        "    \"\"\"\n",
        "    GloVe 텍스트 파일을 읽어\n",
        "    {단어(str): 임베딩 벡터(np.ndarray)} 형태로 변환한다.\n",
        "\n",
        "    GloVe 파일 한 줄의 실제 형태:\n",
        "        word 0.123 0.134 ... 0.532\n",
        "\n",
        "    Args:\n",
        "        glove_path (str): GloVe 임베딩 파일 경로\n",
        "        embedding_dim (int): 임베딩 차원\n",
        "\n",
        "    Returns:\n",
        "        dict[str, np.ndarray]: 단어와 해당 임베딩 벡터의 딕셔너리\n",
        "\n",
        "    처리 순서:\n",
        "    1. 한 줄을 공백 기준으로 분리\n",
        "    2. 첫 번째 인덱스: 단어 문자열\n",
        "    3. 나머지: 임베딩 벡터 값들\n",
        "    \"\"\"\n",
        "    glove_dict = {}\n",
        "\n",
        "    with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.rstrip().split(\" \")\n",
        "\n",
        "            # GloVe 포맷: 첫 칸은 단어\n",
        "            word = parts[0]\n",
        "\n",
        "            # 나머지는 해당 단어의 임베딩 벡터 값\n",
        "            vector = np.asarray(parts[1:], dtype=np.float32)\n",
        "\n",
        "            # 차원이 맞지 않으면 무시 (파일/설정 불일치 방어)\n",
        "            if vector.shape[0] != embedding_dim:\n",
        "                continue\n",
        "\n",
        "            glove_dict[word] = vector\n",
        "\n",
        "    return glove_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vR3ehYRJcal"
      },
      "outputs": [],
      "source": [
        "# 특수 토큰 먼저 정의\n",
        "gl_token_to_id = {\n",
        "    \"[PAD]\": 0,\n",
        "    \"[UNK]\": 1,\n",
        "}\n",
        "\n",
        "# Glove 사전 객체 생성\n",
        "gl_dict = load_glove_embeddings(GLOVE_FILE, embedding_dim=300)\n",
        "\n",
        "# Glove용 token_to_id 생성\n",
        "for token in gl_dict.keys():\n",
        "    gl_token_to_id[token] = len(gl_token_to_id)\n",
        "\n",
        "gl_vocab_size = len(gl_token_to_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZDHlqu4Jcal"
      },
      "outputs": [],
      "source": [
        "# (vocab_size, embed_dim) 크기의 임베딩 행렬 생성\n",
        "gl_embedding_matrix = np.zeros((gl_vocab_size, 300), dtype=np.float32)\n",
        "\n",
        "# PAD 토큰은 0 설정\n",
        "pad_id = gl_token_to_id[\"[PAD]\"]\n",
        "gl_embedding_matrix[pad_id] = 0.0\n",
        "\n",
        "# GloVe에 존재하는 단어만 사전학습 벡터로 덮어씀\n",
        "for token, idx in gl_token_to_id.items():\n",
        "    if token in gl_dict:\n",
        "        gl_embedding_matrix[idx] = gl_dict[token]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1qjti5bJcal"
      },
      "outputs": [],
      "source": [
        "# numpy -> torch tensor 변환\n",
        "gl_embedding_tensor = torch.tensor(gl_embedding_matrix)\n",
        "\n",
        "# Glove 임베딩 레이어 생성\n",
        "gl_embedding_layer = nn.Embedding.from_pretrained(\n",
        "    embeddings=torch.tensor(gl_embedding_matrix),\n",
        "    freeze=False,\n",
        "    padding_idx=pad_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_UNyNk0Jcal"
      },
      "outputs": [],
      "source": [
        "# 모델 입력용 토큰-아이디 변환 함수\n",
        "def gl_tokens_to_ids(tokens: list[str]) -> list[int]:\n",
        "    \"\"\"\n",
        "    토큰 리스트를 정수 ID 리스트로 변환\n",
        "    - 사전에 없는 토큰은 [UNK] ID로 치환\n",
        "    \"\"\"\n",
        "    unk_id = gl_token_to_id[\"[UNK]\"]\n",
        "    return [gl_token_to_id.get(token, unk_id) for token in tokens]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDBSOTX_Jcam"
      },
      "source": [
        "## 2.3. collate_fn 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n0EmK3PSJcam"
      },
      "outputs": [],
      "source": [
        "# DataLoader용 collate_fn 함수 정의 (Word2Vec, FastText, GloVe 공통)\n",
        "def make_collate_fn(tokens_to_ids, token_to_id, max_len: int = cfg.max_len):\n",
        "    \"\"\"\n",
        "    DataLoader용 collate_fn 생성 함수\n",
        "\n",
        "    - cfg.max_len으로 지나치게 긴 텍스트 truncate\n",
        "    - cfg.max_len보다 짧은 텍스트는 pad 추가 + 실제 길이도 반환\n",
        "\n",
        "    Args:\n",
        "        tokens_to_ids (callable):\n",
        "            list[str] -> list[int] 변환 함수\n",
        "        token_to_id (dict[str, int]):\n",
        "            토큰 -> ID 매핑 딕셔너리\n",
        "        max_len (int):\n",
        "            문서당 최대 토큰 길이 (OOM 방지용)\n",
        "\n",
        "    Returns:\n",
        "        collate_fn (callable)\n",
        "    \"\"\"\n",
        "    pad_id = token_to_id[\"[PAD]\"]\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        # batch = [(text, label), ...]\n",
        "        texts, labels = zip(*batch)\n",
        "\n",
        "        # 1. 텍스트 → 토큰 (길이 제한!!)\n",
        "        token_lists = [\n",
        "            tokenize(clean_text(text))[:max_len]\n",
        "            for text in texts\n",
        "        ]\n",
        "\n",
        "        # 2. 토큰 -> ID\n",
        "        id_lists = [tokens_to_ids(tokens) for tokens in token_lists]\n",
        "\n",
        "        # 3. 실제 길이 (last_valid_hidden / pack용)\n",
        "        lengths = torch.tensor(\n",
        "            [len(ids) for ids in id_lists],\n",
        "            dtype=torch.long\n",
        "        )\n",
        "\n",
        "        # 4. padding\n",
        "        cur_max_len = max(lengths).item()\n",
        "        padded_ids = [\n",
        "            ids + [pad_id] * (cur_max_len - len(ids))\n",
        "            for ids in id_lists\n",
        "        ]\n",
        "\n",
        "        # 5. tensor 변환\n",
        "        input_ids = torch.tensor(padded_ids, dtype=torch.long)\n",
        "        labels_t = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "        return input_ids, labels_t, lengths\n",
        "\n",
        "    return collate_fn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wakeMotwJcam"
      },
      "outputs": [],
      "source": [
        "# BPE DataLoader용 collate_fn 함수 정의\n",
        "def make_bpe_collate_fn(bpe_tokenizer, max_len: int = cfg.max_len):\n",
        "    \"\"\"\n",
        "    BPE용 collate_fn\n",
        "\n",
        "    Args:\n",
        "        bpe_tokenizer: HuggingFace Tokenizer (BPE)\n",
        "        max_len (int): 문서당 최대 subword 길이 (OOM 방지용)\n",
        "\n",
        "    Returns:\n",
        "        collate_fn\n",
        "    \"\"\"\n",
        "    pad_id = bpe_tokenizer.token_to_id(\"[PAD]\")\n",
        "\n",
        "    def collate_fn(batch):\n",
        "        texts, labels = zip(*batch)\n",
        "\n",
        "        # 1. BPE 토큰화 + ID 변환 + 길이 제한\n",
        "        id_lists = [\n",
        "            bpe_tokenizer.encode(text).ids[:max_len]\n",
        "            for text in texts\n",
        "        ]\n",
        "\n",
        "        # 2. 실제 길이\n",
        "        lengths = torch.tensor(\n",
        "            [len(ids) for ids in id_lists],\n",
        "            dtype=torch.long\n",
        "        )\n",
        "\n",
        "        # 3. batch 내 최대 길이\n",
        "        cur_max_len = max(lengths).item()\n",
        "\n",
        "        # 4. padding\n",
        "        padded_ids = [\n",
        "            ids + [pad_id] * (cur_max_len - len(ids))\n",
        "            for ids in id_lists\n",
        "        ]\n",
        "\n",
        "        # 5. tensor 변환\n",
        "        input_ids = torch.tensor(padded_ids, dtype=torch.long)\n",
        "        labels_t = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "        return input_ids, labels_t, lengths\n",
        "\n",
        "    return collate_fn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksYSUXwRJcam"
      },
      "source": [
        "## 2.4. 데이터로더 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_YnRZaeJcam"
      },
      "outputs": [],
      "source": [
        "# Word2Vec용 collate_fn 생성\n",
        "w2v_collate_fn = make_collate_fn(w2v_tokens_to_ids, w2v_token_to_id)\n",
        "\n",
        "# Word2Vec용 훈련 DataLoader 생성\n",
        "w2v_train_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=cfg.batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=w2v_collate_fn,\n",
        "    num_workers=cfg.num_workers,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "# Word2Vec용 테스트 DataLoader 생성\n",
        "w2v_test_loader = DataLoader(\n",
        "    test_data,\n",
        "    batch_size=cfg.batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=w2v_collate_fn,\n",
        "    num_workers=cfg.num_workers,\n",
        "    drop_last=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRyugvTuJcam",
        "outputId": "42a46668-2ae0-4db5-97c4-20b3e03e1e09"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 391])\n",
            "torch.Size([16])\n",
            "tensor([248, 223,  52, 138, 223, 132, 263, 391, 339, 382, 122,  96, 168, 143,\n",
            "        150,  85])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "# Word2Vec DataLoader 테스트\n",
        "wx, wy, wz  = next(iter(w2v_train_loader))\n",
        "\n",
        "print(wx.shape)  # (batch_size, seq_len)\n",
        "print(wy.shape)  # (batch_size,)\n",
        "print(wz)        # (batch_size,) 실제 길이 *cfg.max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axkUFxkUJcam"
      },
      "outputs": [],
      "source": [
        "# FastText용 collate_fn 생성\n",
        "ft_collate_fn = make_collate_fn(ft_tokens_to_ids, ft_token_to_id)\n",
        "\n",
        "# FastText용 훈련 DataLoader 생성\n",
        "ft_train_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=cfg.batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=ft_collate_fn,\n",
        "    num_workers=cfg.num_workers,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "# FastText용 테스트 DataLoader 생성\n",
        "ft_test_loader = DataLoader(\n",
        "    test_data,\n",
        "    batch_size=cfg.batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=ft_collate_fn,\n",
        "    num_workers=cfg.num_workers,\n",
        "    drop_last=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B--LLDyEJcam",
        "outputId": "9322347d-e54e-4cfe-eef0-e950ea2ed07d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 512])\n",
            "torch.Size([16])\n",
            "tensor([152, 244, 111, 214, 156, 447, 237, 105, 135, 382,  60, 512, 217, 322,\n",
            "        136, 229])\n"
          ]
        }
      ],
      "source": [
        "# FastText DataLoader 테스트\n",
        "fx, fy, fz = next(iter(ft_train_loader))\n",
        "\n",
        "print(fx.shape)  # (batch_size, seq_len)\n",
        "print(fy.shape)  # (batch_size,)\n",
        "print(fz)  # (batch_size,) 실제 길이"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APFD4uPqJcam"
      },
      "outputs": [],
      "source": [
        "# Glove용 collate_fn 생성\n",
        "gl_collate_fn = make_collate_fn(gl_tokens_to_ids, gl_token_to_id)\n",
        "\n",
        "# Glove용 훈련 DataLoader 생성\n",
        "gl_train_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=cfg.batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=gl_collate_fn,\n",
        "    num_workers=cfg.num_workers,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "# Glove용 테스트 DataLoader 생성\n",
        "gl_test_loader = DataLoader(\n",
        "    test_data,\n",
        "    batch_size=cfg.batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=gl_collate_fn,\n",
        "    num_workers=cfg.num_workers,\n",
        "    drop_last=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7QnKx4QJcam",
        "outputId": "c3bf8733-3dc4-48b9-a7ff-de2d32e902cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 512])\n",
            "torch.Size([16])\n",
            "tensor([512, 138, 176, 103, 276, 365, 164,  93, 132, 512, 397, 144, 119, 512,\n",
            "        205,  42])\n"
          ]
        }
      ],
      "source": [
        "# Glove DataLoader 테스트\n",
        "gx, gy, gz = next(iter(gl_train_loader))\n",
        "\n",
        "print(gx.shape)  # (batch_size, seq_len)\n",
        "print(gy.shape)  # (batch_size,)\n",
        "print(gz)  # (batch_size,) 실제 길이"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bz8gmLSNJcam"
      },
      "outputs": [],
      "source": [
        "# bpe용 collate_fn 생성\n",
        "bpe_collate_fn = make_bpe_collate_fn(bpe_tokenizer)\n",
        "\n",
        "# BPE용 훈련 DataLoader 생성\n",
        "bpe_train_loader = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=cfg.batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=bpe_collate_fn,\n",
        "    drop_last=True\n",
        ")\n",
        "\n",
        "# BPE용 테스트 DataLoader 생성\n",
        "bpe_test_loader = DataLoader(\n",
        "    test_data,\n",
        "    batch_size=cfg.batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=bpe_collate_fn,\n",
        "    drop_last=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m08h1jfUJcam",
        "outputId": "d1ff3fcb-ae15-4ed7-e94f-353c6a1da45a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 349])\n",
            "torch.Size([16])\n",
            "tensor([180, 171, 105, 193, 128, 230, 118, 127, 202, 281, 349,  67, 147, 296,\n",
            "        272, 141])\n"
          ]
        }
      ],
      "source": [
        "# BPE DataLoader 테스트\n",
        "bx, by, bz = next(iter(bpe_train_loader))\n",
        "\n",
        "print(bx.shape)  # (batch_size, seq_len)\n",
        "print(by.shape)  # (batch_size,)\n",
        "print(bz)  # (batch_size,) 실제 길이"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLwPFiiaJcam"
      },
      "source": [
        "# 3. 분류 모델링"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhPfc512Jcam"
      },
      "source": [
        "## 3.1. 공통 유틸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWG-VlMAJcam"
      },
      "outputs": [],
      "source": [
        "# 시퀀스 출력에서 유효한 마지막 타임스텝의 은닉 상태 추출 함수\n",
        "def last_valid_hidden(outputs, lengths):\n",
        "    \"\"\"\n",
        "    outputs: (batch, seq_len, hidden)\n",
        "    lengths: (batch,) 실제 길이\n",
        "    \"\"\"\n",
        "    # gather 연상을 위해 view 사용 (batch, 1, 1)\n",
        "    idx = (lengths - 1).view(-1, 1, 1)\n",
        "    idx = idx.expand(\n",
        "        outputs.size(0),  # batch size\n",
        "        1,                # 하나의 time dimension\n",
        "        outputs.size(2)   # hidden dim\n",
        "    )\n",
        "    # 시간 축 기준으로 연산\n",
        "    return outputs.gather(1, idx).squeeze(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yPhiKnQJcam"
      },
      "source": [
        "## 3.2. LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCj9imewJcan"
      },
      "outputs": [],
      "source": [
        "# LSTM 분류기 정의 (w2v, ft, bpe 교체 허용)\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, embedding, hidden_dim, num_classes, dropout=0.3):\n",
        "        \"\"\"\n",
        "        LSTM 모델 설계\n",
        "        Args:\n",
        "            embedding: 앞서 정의한 w2v, ft, glove, bpe 임베딩 레이어 중 하나\n",
        "            hidden_dim: 모델의 은닉 차원\n",
        "            num_classes: 마지막 선형 층에서 사용할 클래스 수\n",
        "\n",
        "        Returns:\n",
        "            logits: 전체 클래스 수에 대한 점수\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embedding = embedding                 # w2v, ft, glove, bpe 교체 지점\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding.embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    # collate_fn의 lengths를 넘겨 패딩을 제외한 마지막 은닉 차원을 선형 변환\n",
        "    def forward(self, input_ids, lengths):\n",
        "        x = self.embedding(input_ids)              # (B, T, D)\n",
        "        x = x.contiguous()\n",
        "        out, (h_n, c_n) = self.lstm(x)             # (B, T, H)\n",
        "        last = last_valid_hidden(out, lengths)     # (B, H)\n",
        "        last = self.dropout(last)\n",
        "        return self.fc(last)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NglU5GbTJcan"
      },
      "source": [
        "## 3.3. GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sk7cUsyJcan"
      },
      "outputs": [],
      "source": [
        "# GRU 분류기 정의 (w2v, ft, bpe 교체 허용)\n",
        "class GRUClassifier(nn.Module):\n",
        "    def __init__(self, embedding, hidden_dim, num_classes, dropout=0.3, bidir=False):\n",
        "        \"\"\"\n",
        "        GRU 모델 설계\n",
        "        Args:\n",
        "            embedding: 앞서 정의한 w2v, ft, glove, bpe 임베딩 레이어 중 하나\n",
        "            hidden_dim: 모델의 은닉 차원\n",
        "            num_classes: 마지막 선형 층에서 사용할 클래스 수\n",
        "\n",
        "        Returns:\n",
        "            logits: 전체 클래스 수에 대한 점수\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.embedding = embedding                 # w2v, ft, bpe 교체 지점\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=embedding.embedding_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True,\n",
        "            bidirectional=bidir\n",
        "        )\n",
        "        out_dim = hidden_dim * (2 if bidir else 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(out_dim, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, lengths):\n",
        "        x = self.embedding(input_ids)\n",
        "        x = x.contiguous()\n",
        "        out, _ = self.gru(x)\n",
        "        last = last_valid_hidden(out, lengths)\n",
        "        last = self.dropout(last)\n",
        "        return self.fc(last)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTqVDugJJcan"
      },
      "source": [
        "## 3.4. 모델 인스턴스화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87CsL25CJcan"
      },
      "source": [
        "### A. Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfTqjcXuJcan"
      },
      "outputs": [],
      "source": [
        "# 앞서 만든 w2v_embedding_layer 사용\n",
        "w2v_lstm = LSTMClassifier(\n",
        "    embedding=w2v_embedding_layer,\n",
        "    hidden_dim=128,\n",
        "    num_classes=20\n",
        ").to(cfg.device)\n",
        "\n",
        "w2v_gru = GRUClassifier(\n",
        "    embedding=w2v_embedding_layer,\n",
        "    hidden_dim=128,\n",
        "    num_classes=20,\n",
        "    bidir=True\n",
        ").to(cfg.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtuhkR16Jcan"
      },
      "source": [
        "### B. FastText"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omm_plTCJcan"
      },
      "outputs": [],
      "source": [
        "# 앞서 만든 ft_embedding_layer 사용\n",
        "ft_lstm = LSTMClassifier(\n",
        "    embedding=ft_embedding_layer,\n",
        "    hidden_dim=128,\n",
        "    num_classes=20\n",
        ").to(cfg.device)\n",
        "\n",
        "ft_gru = GRUClassifier(\n",
        "    embedding=ft_embedding_layer,\n",
        "    hidden_dim=128,\n",
        "    num_classes=20,\n",
        "    bidir=True\n",
        ").to(cfg.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPTsLEMOJcan"
      },
      "source": [
        "### C. Glove"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_w_kEFEdJcan"
      },
      "outputs": [],
      "source": [
        "# 앞서 만든 gl_embedding_layer 사용\n",
        "gl_lstm = LSTMClassifier(\n",
        "    embedding=gl_embedding_layer,\n",
        "    hidden_dim=128,\n",
        "    num_classes=20\n",
        ").to(cfg.device)\n",
        "\n",
        "gl_gru = GRUClassifier(\n",
        "    embedding=gl_embedding_layer,\n",
        "    hidden_dim=128,\n",
        "    num_classes=20,\n",
        "    bidir=True\n",
        ").to(cfg.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJhqi2U8Jcan"
      },
      "source": [
        "### D. BPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkhOBqaaJcan"
      },
      "outputs": [],
      "source": [
        "# BPE vocab_size, embed_dim\n",
        "bpe_embedding_bpe = nn.Embedding(\n",
        "    num_embeddings=bpe_tokenizer.get_vocab_size(),\n",
        "    embedding_dim=300,\n",
        "    padding_idx=bpe_tokenizer.token_to_id(\"[PAD]\")\n",
        ")\n",
        "\n",
        "bpe_lstm = LSTMClassifier(\n",
        "    embedding=bpe_embedding_bpe,\n",
        "    hidden_dim=128,\n",
        "    num_classes=20\n",
        ").to(cfg.device)\n",
        "\n",
        "bpe_gru = GRUClassifier(\n",
        "    embedding=bpe_embedding_bpe,\n",
        "    hidden_dim=128,\n",
        "    num_classes=20,\n",
        "    bidir=True\n",
        ").to(cfg.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh8VDiYQJcan"
      },
      "source": [
        "# 4. 훈련"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LV9_53aJcan"
      },
      "source": [
        "## 4.1. 훈련용 공용 유틸"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBjN1kEDJcan"
      },
      "outputs": [],
      "source": [
        "# 조기 종료 클래스 (w2v, ft, glove용)\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience: int=cfg.patience, min_delta: float = 0.0, save_path: str | Path = \"model.pt\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): 개선이 없을 때 허용 epoch 수\n",
        "            min_delta (float): 최소 개선 폭\n",
        "            save_path (str | Path): best model 저장 경로\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = float(\"inf\")\n",
        "        self.counter = 0\n",
        "        self.save_path = Path(save_path)\n",
        "\n",
        "    def step(self, val_loss: float, model: torch.nn.Module, token_to_id, embedding_matrix) -> bool:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            val_loss (float): 현재 epoch의 validation loss\n",
        "            model (nn.Module): 현재 모델\n",
        "\n",
        "        Returns:\n",
        "            bool: True면 학습 중단, False면 계속\n",
        "        \"\"\"\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            # 성능 개선\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "\n",
        "            # best model 저장\n",
        "            torch.save({\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"token_to_id\": token_to_id,\n",
        "                \"embedding_matrix\": embedding_matrix,\n",
        "            }, self.save_path)\n",
        "            print(f\"Validation loss improved. Best model saved to {self.save_path}\")\n",
        "\n",
        "            return False\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            print(f\"EarlyStopping counter: {self.counter} / {self.patience}\")\n",
        "\n",
        "            return self.counter >= self.patience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-ebwAwC9d33"
      },
      "outputs": [],
      "source": [
        "# bpe용 EarlyStopping\n",
        "class EarlyStopping_bpe:\n",
        "    def __init__(\n",
        "        self,\n",
        "        patience: int = cfg.patience,\n",
        "        min_delta: float = 0.0,\n",
        "        save_path: str | Path = \"best_bpe_model.pt\",\n",
        "        tokenizer_path: str | Path = \"best_bpe_tokenizer.json\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): 개선이 없을 때 허용 epoch 수\n",
        "            min_delta (float): 최소 개선 폭\n",
        "            save_path (str | Path): best model 저장 경로 (.pt)\n",
        "            tokenizer_path (str | Path): BPE tokenizer 저장 경로 (.json)\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.best_loss = float(\"inf\")\n",
        "        self.counter = 0\n",
        "\n",
        "        self.save_path = Path(save_path)\n",
        "        self.tokenizer_path = Path(tokenizer_path)\n",
        "\n",
        "    def step(self, val_loss: float, model: torch.nn.Module, tokenizer) -> bool:\n",
        "        if val_loss < self.best_loss - self.min_delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "\n",
        "            # 1. 모델 가중치 저장\n",
        "            torch.save(\n",
        "                {\"model_state\": model.state_dict()},\n",
        "                self.save_path\n",
        "            )\n",
        "\n",
        "            # 2. BPE 토크나이저 저장 (JSON)\n",
        "            tokenizer.save(str(self.tokenizer_path))\n",
        "\n",
        "            print(\n",
        "                f\"[BPE] Best model saved to {self.save_path}, \"\n",
        "                f\"tokenizer saved to {self.tokenizer_path}\"\n",
        "            )\n",
        "            return False\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            print(f\"EarlyStopping counter: {self.counter}/{self.patience}\")\n",
        "            return self.counter >= self.patience\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUoG507NJcan"
      },
      "outputs": [],
      "source": [
        "# 훈련 함수 정의\n",
        "def my_trainer(model, dataloader, optimizer, loss_fn, device=cfg.device):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        # batch: (input_ids, labels, lengths)\n",
        "        input_ids, labels, lengths = batch\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device)\n",
        "        lengths = lengths.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits = model(input_ids, lengths)          # (B, C)\n",
        "        loss = loss_fn(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total\n",
        "    acc = correct / total\n",
        "    return avg_loss, acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APg6fUBXJcan"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(model, dataloader, loss_fn, device=cfg.device):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        input_ids, labels, lengths = batch\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device)\n",
        "        lengths = lengths.to(device)\n",
        "\n",
        "        logits = model(input_ids, lengths)\n",
        "        loss = loss_fn(logits, labels)\n",
        "\n",
        "        total_loss += loss.item() * labels.size(0)\n",
        "\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total\n",
        "    acc = correct / total\n",
        "    return avg_loss, acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZiDCKaMJcan"
      },
      "source": [
        "## 4.2. Word2Vec LSTM & GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSucWKe7Jcan"
      },
      "outputs": [],
      "source": [
        "# 공용\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "early_stopping = EarlyStopping()\n",
        "bpe_early_stopping = EarlyStopping_bpe()\n",
        "\n",
        "# w2v 전용\n",
        "wl_optimizer = torch.optim.Adam(w2v_lstm.parameters(), lr=cfg.lr)\n",
        "wg_optimizer = torch.optim.Adam(w2v_gru.parameters(), lr=cfg.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6ylvj8aJcan",
        "outputId": "da55dbc1-8e63-4838-8e11-6f9724540f22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1] train_loss=2.7942 acc=0.1239 | val_loss=2.9971 acc=0.1000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  10%|█         | 1/10 [01:04<09:44, 64.97s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss improved. Best model saved to best_w2v_lstm.pt\n",
            "Epoch 2/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2] train_loss=2.0597 acc=0.3273 | val_loss=1.9028 acc=0.3561\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  20%|██        | 2/10 [02:10<08:40, 65.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss improved. Best model saved to best_w2v_lstm.pt\n",
            "Epoch 3/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3] train_loss=1.3034 acc=0.5706 | val_loss=1.4140 acc=0.5376\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  30%|███       | 3/10 [03:13<07:31, 64.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss improved. Best model saved to best_w2v_lstm.pt\n",
            "Epoch 4/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4] train_loss=0.5764 acc=0.8141 | val_loss=1.1132 acc=0.6512\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  40%|████      | 4/10 [04:16<06:22, 63.77s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss improved. Best model saved to best_w2v_lstm.pt\n",
            "Epoch 5/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "processing:  50%|█████     | 5/10 [05:19<05:17, 63.60s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5] train_loss=0.2541 acc=0.9242 | val_loss=1.2582 acc=0.6455\n",
            "EarlyStopping counter: 1 / 3\n",
            "Epoch 6/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "processing:  60%|██████    | 6/10 [06:23<04:14, 63.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[6] train_loss=0.1143 acc=0.9702 | val_loss=1.2118 acc=0.7034\n",
            "EarlyStopping counter: 2 / 3\n",
            "Epoch 7/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "processing:  60%|██████    | 6/10 [07:29<04:59, 74.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[7] train_loss=0.0704 acc=0.9824 | val_loss=1.1443 acc=0.7224\n",
            "EarlyStopping counter: 3 / 3\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# LSTM 모델 훈련\n",
        "wl_early_stopping = EarlyStopping(\n",
        "    patience=cfg.patience,\n",
        "    min_delta=cfg.min_delta,\n",
        "    save_path=\"best_w2v_lstm.pt\"\n",
        ")\n",
        "\n",
        "\n",
        "for epoch in tqdm(range(1, cfg.epochs), desc=f\"processing\"):\n",
        "    print(f\"Epoch {epoch}/{cfg.epochs}\")\n",
        "    train_loss, train_acc = my_trainer(\n",
        "        w2v_lstm, w2v_train_loader, wl_optimizer, loss_fn, cfg.device\n",
        "    )\n",
        "    val_loss, val_acc = evaluate(\n",
        "        w2v_lstm, w2v_test_loader, loss_fn, cfg.device\n",
        "    )\n",
        "\n",
        "    print(f\"[{epoch}] \"\n",
        "          f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "          f\"val_loss={val_loss:.4f} acc={val_acc:.4f}\")\n",
        "\n",
        "    if wl_early_stopping.step(val_loss, model=w2v_lstm, token_to_id=w2v_token_to_id, embedding_matrix=w2v_embedding_matrix):\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IrADzOPJcan"
      },
      "outputs": [],
      "source": [
        "w2v_lstm_ckpt = torch.load(\"best_w2v_lstm.pt\", map_location=cfg.device, weights_only=False)\n",
        "\n",
        "w2v_lstm.load_state_dict(w2v_lstm_ckpt[\"model_state\"])\n",
        "w2v_lstm.to(cfg.device)\n",
        "\n",
        "w2v_token_to_id = w2v_lstm_ckpt[\"token_to_id\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOUJmYkVJcan",
        "outputId": "d4b097a4-20f9-4b4a-e1a0-92a6eca92063"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1] train_loss=1.2687 acc=0.6244 | val_loss=1.0317 acc=0.6875\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  10%|█         | 1/10 [00:59<08:54, 59.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss improved. Best model saved to best_w2v_gru.pt\n",
            "Epoch 2/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2] train_loss=0.2168 acc=0.9404 | val_loss=0.9559 acc=0.7362\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  20%|██        | 2/10 [01:56<07:46, 58.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss improved. Best model saved to best_w2v_gru.pt\n",
            "Epoch 3/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "processing:  30%|███       | 3/10 [02:53<06:42, 57.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3] train_loss=0.0682 acc=0.9836 | val_loss=1.0550 acc=0.7306\n",
            "EarlyStopping counter: 1 / 3\n",
            "Epoch 4/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "processing:  40%|████      | 4/10 [03:50<05:42, 57.13s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4] train_loss=0.0321 acc=0.9927 | val_loss=1.1292 acc=0.7423\n",
            "EarlyStopping counter: 2 / 3\n",
            "Epoch 5/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "processing:  40%|████      | 4/10 [04:48<07:12, 72.11s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5] train_loss=0.0215 acc=0.9943 | val_loss=1.2397 acc=0.7254\n",
            "EarlyStopping counter: 3 / 3\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# GRU 모델 훈련\n",
        "wg_early_stopping = EarlyStopping(\n",
        "    patience=cfg.patience,\n",
        "    min_delta=cfg.min_delta,\n",
        "    save_path=\"best_w2v_gru.pt\"\n",
        ")\n",
        "\n",
        "for epoch in tqdm(range(1, cfg.epochs), desc=f\"processing\"):\n",
        "    print(f\"Epoch {epoch}/{cfg.epochs}\")\n",
        "    train_loss, train_acc = my_trainer(\n",
        "        w2v_gru, w2v_train_loader, wg_optimizer, loss_fn, cfg.device\n",
        "    )\n",
        "    val_loss, val_acc = evaluate(\n",
        "        w2v_gru, w2v_test_loader, loss_fn, cfg.device\n",
        "    )\n",
        "\n",
        "    print(f\"[{epoch}] \"\n",
        "          f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "          f\"val_loss={val_loss:.4f} acc={val_acc:.4f}\")\n",
        "\n",
        "    if wg_early_stopping.step(val_loss, model=w2v_gru, token_to_id=w2v_token_to_id, embedding_matrix=w2v_embedding_matrix):\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qbfhi3R1Jcao"
      },
      "outputs": [],
      "source": [
        "w2v_gru_ckpt = torch.load(\"best_w2v_gru.pt\", map_location=cfg.device, weights_only=False)\n",
        "\n",
        "w2v_gru.load_state_dict(w2v_gru_ckpt[\"model_state\"])\n",
        "w2v_gru.to(cfg.device)\n",
        "\n",
        "w2v_token_to_id = w2v_gru_ckpt[\"token_to_id\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjiQfkX_Jcao"
      },
      "source": [
        "## 4.3. FastText LSTM & GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhVov7fgJcao"
      },
      "outputs": [],
      "source": [
        "fl_optimizer = torch.optim.Adam(ft_lstm.parameters(), lr=cfg.lr)\n",
        "fg_optimizer = torch.optim.Adam(ft_gru.parameters(), lr=cfg.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xrPsmGCJcao",
        "outputId": "0756af35-e9b4-43c5-d94c-35882d0ce030"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1] train_loss=2.7241 acc=0.1535 | val_loss=2.4164 acc=0.2306\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  10%|█         | 1/10 [00:33<05:03, 33.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss improved. Best model saved to best_ft_lstm.pt\n",
            "Epoch 2/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2] train_loss=2.1609 acc=0.2969 | val_loss=2.1046 acc=0.3005\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  20%|██        | 2/10 [01:03<04:11, 31.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss improved. Best model saved to best_ft_lstm.pt\n",
            "Epoch 3/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3] train_loss=1.7706 acc=0.4155 | val_loss=1.8123 acc=0.3995\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  30%|███       | 3/10 [01:33<03:36, 30.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss improved. Best model saved to best_ft_lstm.pt\n",
            "Epoch 4/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4] train_loss=1.4021 acc=0.5290 | val_loss=1.6834 acc=0.4456\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  40%|████      | 4/10 [02:05<03:08, 31.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss improved. Best model saved to best_ft_lstm.pt\n",
            "Epoch 5/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5] train_loss=1.0765 acc=0.6446 | val_loss=1.5103 acc=0.4942\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  50%|█████     | 5/10 [02:35<02:34, 30.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss improved. Best model saved to best_ft_lstm.pt\n",
            "Epoch 6/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[6] train_loss=0.8134 acc=0.7284 | val_loss=1.4501 acc=0.5262\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  60%|██████    | 6/10 [03:07<02:04, 31.15s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss improved. Best model saved to best_ft_lstm.pt\n",
            "Epoch 7/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "processing:  70%|███████   | 7/10 [03:35<01:30, 30.22s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[7] train_loss=0.6514 acc=0.7874 | val_loss=1.5243 acc=0.5182\n",
            "EarlyStopping counter: 1 / 3\n",
            "Epoch 8/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "processing:  80%|████████  | 8/10 [04:04<00:59, 29.59s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[8] train_loss=0.5340 acc=0.8248 | val_loss=1.5274 acc=0.5377\n",
            "EarlyStopping counter: 2 / 3\n",
            "Epoch 9/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "processing:  80%|████████  | 8/10 [04:32<01:08, 34.03s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[9] train_loss=0.3972 acc=0.8761 | val_loss=1.5143 acc=0.5645\n",
            "EarlyStopping counter: 3 / 3\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# LSTM 모델 훈련\n",
        "fl_early_stopping = EarlyStopping(\n",
        "    patience=cfg.patience,\n",
        "    min_delta=cfg.min_delta,\n",
        "    save_path=\"best_ft_lstm.pt\"\n",
        ")\n",
        "\n",
        "for epoch in tqdm(range(1, cfg.epochs), desc=f\"processing\"):\n",
        "    print(f\"Epoch {epoch}/{cfg.epochs}\")\n",
        "    train_loss, train_acc = my_trainer(\n",
        "        ft_lstm, ft_train_loader, fl_optimizer, loss_fn, cfg.device\n",
        "    )\n",
        "    val_loss, val_acc = evaluate(\n",
        "        ft_lstm, ft_test_loader, loss_fn, cfg.device\n",
        "    )\n",
        "\n",
        "    print(f\"[{epoch}] \"\n",
        "          f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "          f\"val_loss={val_loss:.4f} acc={val_acc:.4f}\")\n",
        "\n",
        "    if fl_early_stopping.step(val_loss, model=ft_lstm, token_to_id=ft_token_to_id, embedding_matrix=ft_embedding_matrix):\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htl3gL8YJcao"
      },
      "outputs": [],
      "source": [
        "ft_lstm_ckpt = torch.load(\"best_ft_lstm.pt\", map_location=cfg.device, weights_only=False)\n",
        "\n",
        "ft_lstm.load_state_dict(ft_lstm_ckpt[\"model_state\"])\n",
        "ft_lstm.to(cfg.device)\n",
        "\n",
        "ft_token_to_id = ft_lstm_ckpt[\"token_to_id\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yija9NxfJcao",
        "outputId": "44a55c94-7c3f-4bba-8004-e0efaacdfe4c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "# GRU 모델 훈련\n",
        "fg_early_stopping = EarlyStopping(\n",
        "    patience=cfg.patience,\n",
        "    min_delta=cfg.min_delta,\n",
        "    save_path=\"best_ft_gru.pt\"\n",
        ")\n",
        "\n",
        "for epoch in tqdm(range(1, cfg.epochs), desc=f\"processing\"):\n",
        "    print(f\"Epoch {epoch}/{cfg.epochs}\")\n",
        "    train_loss, train_acc = my_trainer(\n",
        "        ft_gru, ft_train_loader, fg_optimizer, loss_fn, cfg.device\n",
        "    )\n",
        "    val_loss, val_acc = evaluate(\n",
        "        ft_gru, ft_test_loader, loss_fn, cfg.device\n",
        "    )\n",
        "\n",
        "    print(f\"[{epoch}] \"\n",
        "          f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "          f\"val_loss={val_loss:.4f} acc={val_acc:.4f}\")\n",
        "\n",
        "    if fg_early_stopping.step(val_loss, model=ft_gru, token_to_id=ft_token_to_id, embedding_matrix=ft_embedding_matrix):\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHiBsGBXJcao"
      },
      "outputs": [],
      "source": [
        "ft_gru_ckpt = torch.load(\"best_ft_gru.pt\", map_location=cfg.device, weights_only=False)\n",
        "\n",
        "ft_gru.load_state_dict(ft_gru_ckpt[\"model_state\"])\n",
        "ft_gru.to(cfg.device)\n",
        "\n",
        "ft_token_to_id = ft_gru_ckpt[\"token_to_id\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ7zwZopJcao"
      },
      "source": [
        "## 4.4. Glove LSTM & GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLdx7UO2Jcao"
      },
      "outputs": [],
      "source": [
        "gl_optimizer = torch.optim.Adam(gl_lstm.parameters(), lr=cfg.lr)\n",
        "gg_optimizer = torch.optim.Adam(gl_gru.parameters(), lr=cfg.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "7vYDkPo-Jcao",
        "outputId": "63471019-b4e0-432e-da74-bca9bcd00afc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1] train_loss=2.6503 acc=0.1600 | val_loss=2.2749 acc=0.2718\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  10%|█         | 1/10 [00:20<03:00, 20.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss improved. Best model saved to best_gl_lstm.pt\n",
            "Epoch 2/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2] train_loss=1.9275 acc=0.3725 | val_loss=1.9015 acc=0.4077\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  20%|██        | 2/10 [00:39<02:37, 19.73s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss improved. Best model saved to best_gl_lstm.pt\n",
            "Epoch 3/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3] train_loss=1.1423 acc=0.6170 | val_loss=1.1068 acc=0.6337\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  30%|███       | 3/10 [00:59<02:18, 19.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss improved. Best model saved to best_gl_lstm.pt\n",
            "Epoch 4/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4] train_loss=0.6201 acc=0.7918 | val_loss=0.9553 acc=0.6945\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  40%|████      | 4/10 [01:20<02:00, 20.10s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss improved. Best model saved to best_gl_lstm.pt\n",
            "Epoch 5/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5] train_loss=0.3677 acc=0.8857 | val_loss=0.9273 acc=0.7363\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  50%|█████     | 5/10 [01:39<01:40, 20.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss improved. Best model saved to best_gl_lstm.pt\n",
            "Epoch 6/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "processing:  60%|██████    | 6/10 [01:54<01:12, 18.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[6] train_loss=0.1932 acc=0.9455 | val_loss=1.0099 acc=0.7370\n",
            "EarlyStopping counter: 1 / 3\n",
            "Epoch 7/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "processing:  70%|███████   | 7/10 [02:08<00:50, 16.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[7] train_loss=0.0906 acc=0.9775 | val_loss=1.0134 acc=0.7576\n",
            "EarlyStopping counter: 2 / 3\n",
            "Epoch 8/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "processing:  70%|███████   | 7/10 [02:23<01:01, 20.45s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[8] train_loss=0.0454 acc=0.9898 | val_loss=1.2161 acc=0.7256\n",
            "EarlyStopping counter: 3 / 3\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# LSTM 모델 훈련\n",
        "gl_early_stopping = EarlyStopping(\n",
        "    patience=cfg.patience,\n",
        "    min_delta=cfg.min_delta,\n",
        "    save_path=\"best_gl_lstm.pt\"\n",
        ")\n",
        "\n",
        "for epoch in tqdm(range(1, cfg.epochs), desc=f\"processing\"):\n",
        "    print(f\"Epoch {epoch}/{cfg.epochs}\")\n",
        "    train_loss, train_acc = my_trainer(\n",
        "        gl_lstm, gl_train_loader, gl_optimizer, loss_fn, cfg.device\n",
        "    )\n",
        "    val_loss, val_acc = evaluate(\n",
        "        gl_lstm, gl_test_loader, loss_fn, cfg.device\n",
        "    )\n",
        "\n",
        "    print(f\"[{epoch}] \"\n",
        "          f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "          f\"val_loss={val_loss:.4f} acc={val_acc:.4f}\")\n",
        "\n",
        "    if gl_early_stopping.step(val_loss, model=gl_lstm, token_to_id=gl_token_to_id, embedding_matrix=gl_embedding_matrix):\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpLrUupmJcao"
      },
      "outputs": [],
      "source": [
        "gl_lstm_ckpt = torch.load(\"best_gl_lstm.pt\", map_location=cfg.device, weights_only=False)\n",
        "\n",
        "gl_lstm.load_state_dict(gl_lstm_ckpt[\"model_state\"])\n",
        "gl_lstm.to(cfg.device)\n",
        "\n",
        "gl_token_to_id = gl_lstm_ckpt[\"token_to_id\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ohlrc1f2Jcao",
        "outputId": "b954719d-84eb-4a9d-9923-fd4846204817"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1] train_loss=1.5873 acc=0.5121 | val_loss=0.9870 acc=0.6925\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  10%|█         | 1/10 [00:18<02:49, 18.84s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss improved. Best model saved to best_gl_gru.pt\n",
            "Epoch 2/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2] train_loss=0.2894 acc=0.9205 | val_loss=0.8626 acc=0.7576\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  20%|██        | 2/10 [00:38<02:32, 19.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation loss improved. Best model saved to best_gl_gru.pt\n",
            "Epoch 3/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "processing:  30%|███       | 3/10 [00:52<01:59, 17.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3] train_loss=0.0825 acc=0.9810 | val_loss=0.9848 acc=0.7540\n",
            "EarlyStopping counter: 1 / 3\n",
            "Epoch 4/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "processing:  40%|████      | 4/10 [01:07<01:36, 16.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4] train_loss=0.0341 acc=0.9932 | val_loss=1.1111 acc=0.7539\n",
            "EarlyStopping counter: 2 / 3\n",
            "Epoch 5/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "processing:  40%|████      | 4/10 [01:21<02:01, 20.33s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5] train_loss=0.0222 acc=0.9952 | val_loss=1.0883 acc=0.7562\n",
            "EarlyStopping counter: 3 / 3\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# GRU 모델 훈련\n",
        "gg_early_stopping = EarlyStopping(\n",
        "    patience=cfg.patience,\n",
        "    min_delta=cfg.min_delta,\n",
        "    save_path=\"best_gl_gru.pt\"\n",
        ")\n",
        "\n",
        "for epoch in tqdm(range(1, cfg.epochs), desc=f\"processing\"):\n",
        "    print(f\"Epoch {epoch}/{cfg.epochs}\")\n",
        "    train_loss, train_acc = my_trainer(\n",
        "        gl_gru, gl_train_loader, gg_optimizer, loss_fn, cfg.device\n",
        "    )\n",
        "    val_loss, val_acc = evaluate(\n",
        "        gl_gru, gl_test_loader, loss_fn, cfg.device\n",
        "    )\n",
        "\n",
        "    print(f\"[{epoch}] \"\n",
        "          f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "          f\"val_loss={val_loss:.4f} acc={val_acc:.4f}\")\n",
        "\n",
        "    if gg_early_stopping.step(val_loss, model=gl_gru, token_to_id=gl_token_to_id, embedding_matrix=gl_embedding_matrix):\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cWMblIPTJcao"
      },
      "outputs": [],
      "source": [
        "gl_gru_ckpt = torch.load(\"best_gl_gru.pt\", map_location=cfg.device, weights_only=False)\n",
        "\n",
        "gl_gru.load_state_dict(gl_gru_ckpt[\"model_state\"])\n",
        "gl_gru.to(cfg.device)\n",
        "\n",
        "gl_token_to_id = gl_gru_ckpt[\"token_to_id\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMdfl2f4Jcao"
      },
      "source": [
        "## 4.5. BPE LSTM & GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeS1HDm9Jcao"
      },
      "outputs": [],
      "source": [
        "bl_optimizer = torch.optim.Adam(bpe_lstm.parameters(), lr=cfg.lr)\n",
        "bg_optimizer = torch.optim.Adam(bpe_gru.parameters(), lr=cfg.lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsgefgKZJcao",
        "outputId": "4d9dc4ea-7867-4fd7-e6b8-5ae2bda8fcb4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  10%|█         | 1/10 [00:10<01:30, 10.02s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1] train_loss=2.7858 acc=0.1780 | val_loss=2.6660 acc=0.2083\n",
            "[BPE] Best model saved to best_bpe_lstm.pt, tokenizer saved to best_bpe_lstm_tokenizer.json\n",
            "Epoch 2/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  20%|██        | 2/10 [00:19<01:19,  9.94s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2] train_loss=2.1969 acc=0.3625 | val_loss=2.4549 acc=0.2796\n",
            "[BPE] Best model saved to best_bpe_lstm.pt, tokenizer saved to best_bpe_lstm_tokenizer.json\n",
            "Epoch 3/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  30%|███       | 3/10 [00:29<01:09,  9.90s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3] train_loss=1.6138 acc=0.5198 | val_loss=2.3925 acc=0.3096\n",
            "[BPE] Best model saved to best_bpe_lstm.pt, tokenizer saved to best_bpe_lstm_tokenizer.json\n",
            "Epoch 4/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  40%|████      | 4/10 [00:39<00:59,  9.86s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4] train_loss=1.1469 acc=0.6614 | val_loss=2.4112 acc=0.3396\n",
            "EarlyStopping counter: 1/3\n",
            "Epoch 5/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  50%|█████     | 5/10 [00:49<00:49,  9.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5] train_loss=0.7930 acc=0.7719 | val_loss=2.5143 acc=0.3372\n",
            "EarlyStopping counter: 2/3\n",
            "Epoch 6/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  50%|█████     | 5/10 [00:59<00:59, 11.82s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[6] train_loss=0.5323 acc=0.8502 | val_loss=2.5861 acc=0.3581\n",
            "EarlyStopping counter: 3/3\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# LSTM 모델 훈련\n",
        "bl_early_stopping = EarlyStopping_bpe(\n",
        "    patience=cfg.patience,\n",
        "    min_delta=cfg.min_delta,\n",
        "    save_path=\"best_bpe_lstm.pt\",\n",
        "    tokenizer_path=\"best_bpe_lstm_tokenizer.json\"\n",
        ")\n",
        "\n",
        "for epoch in tqdm(range(1, cfg.epochs), desc=f\"processing\"):\n",
        "    print(f\"Epoch {epoch}/{cfg.epochs}\")\n",
        "    train_loss, train_acc = my_trainer(\n",
        "        bpe_lstm, bpe_train_loader, bl_optimizer, loss_fn, cfg.device\n",
        "    )\n",
        "    val_loss, val_acc = evaluate(\n",
        "        bpe_lstm, bpe_test_loader, loss_fn, cfg.device\n",
        "    )\n",
        "\n",
        "    print(f\"[{epoch}] \"\n",
        "          f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "          f\"val_loss={val_loss:.4f} acc={val_acc:.4f}\")\n",
        "\n",
        "    if bl_early_stopping.step(val_loss, model=bpe_lstm, tokenizer=bpe_tokenizer):\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOTmc1GEJcao",
        "outputId": "8fa14999-c474-4448-9c1c-17aee0831371"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LSTMClassifier(\n",
              "  (embedding): Embedding(20000, 300, padding_idx=0)\n",
              "  (lstm): LSTM(300, 128, batch_first=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (fc): Linear(in_features=128, out_features=20, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# checkpoint 로드\n",
        "bpe_lstm_ckpt = torch.load(\"best_bpe_lstm.pt\", map_location=cfg.device)\n",
        "\n",
        "# state_dict만 꺼내서 로드\n",
        "bpe_lstm.load_state_dict(bpe_lstm_ckpt[\"model_state\"])\n",
        "\n",
        "# 토크나이저 로드\n",
        "bpe_tokenizer = Tokenizer.from_file(\"best_bpe_lstm_tokenizer.json\")\n",
        "\n",
        "# 디바이스 이동\n",
        "bpe_lstm.to(cfg.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxBsb5N3Jcao",
        "outputId": "82da4cb1-023a-4bd8-de0a-30a671e58d79"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:   0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  10%|█         | 1/10 [00:10<01:30, 10.08s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1] train_loss=2.5704 acc=0.2335 | val_loss=2.4949 acc=0.2484\n",
            "[BPE] Best model saved to best_bpe_gru.pt, tokenizer saved to best_bpe_gru_tokenizer.json\n",
            "Epoch 2/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  20%|██        | 2/10 [00:19<01:19,  9.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2] train_loss=1.7498 acc=0.4699 | val_loss=2.3085 acc=0.3282\n",
            "[BPE] Best model saved to best_bpe_gru.pt, tokenizer saved to best_bpe_gru_tokenizer.json\n",
            "Epoch 3/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  30%|███       | 3/10 [00:29<01:09,  9.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3] train_loss=1.0993 acc=0.6592 | val_loss=2.1053 acc=0.3796\n",
            "[BPE] Best model saved to best_bpe_gru.pt, tokenizer saved to best_bpe_gru_tokenizer.json\n",
            "Epoch 4/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  40%|████      | 4/10 [00:39<00:59,  9.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4] train_loss=0.6573 acc=0.7951 | val_loss=2.2055 acc=0.3933\n",
            "EarlyStopping counter: 1/3\n",
            "Epoch 5/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  50%|█████     | 5/10 [00:49<00:49,  9.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5] train_loss=0.3983 acc=0.8838 | val_loss=2.1645 acc=0.4424\n",
            "EarlyStopping counter: 2/3\n",
            "Epoch 6/11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "processing:  50%|█████     | 5/10 [00:59<00:59, 11.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[6] train_loss=0.2251 acc=0.9382 | val_loss=2.2963 acc=0.4539\n",
            "EarlyStopping counter: 3/3\n",
            "Early stopping triggered.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# GRU 모델 훈련\n",
        "bg_early_stopping = EarlyStopping_bpe(\n",
        "    patience=cfg.patience,\n",
        "    min_delta=cfg.min_delta,\n",
        "    save_path=\"best_bpe_gru.pt\",\n",
        "    tokenizer_path=\"best_bpe_gru_tokenizer.json\"\n",
        ")\n",
        "\n",
        "for epoch in tqdm(range(1, cfg.epochs), desc=f\"processing\"):\n",
        "    print(f\"Epoch {epoch}/{cfg.epochs}\")\n",
        "    train_loss, train_acc = my_trainer(\n",
        "        bpe_gru, bpe_train_loader, bg_optimizer, loss_fn, cfg.device\n",
        "    )\n",
        "    val_loss, val_acc = evaluate(\n",
        "        bpe_gru, bpe_test_loader, loss_fn, cfg.device\n",
        "    )\n",
        "\n",
        "    print(f\"[{epoch}] \"\n",
        "          f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "          f\"val_loss={val_loss:.4f} acc={val_acc:.4f}\")\n",
        "\n",
        "    if bg_early_stopping.step(val_loss, model=bpe_gru, tokenizer=bpe_tokenizer):\n",
        "        print(\"Early stopping triggered.\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LyVlEt0IJcao",
        "outputId": "9c93d9a1-7a51-431a-eb9f-c54b42cc7f11"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GRUClassifier(\n",
              "  (embedding): Embedding(20000, 300, padding_idx=0)\n",
              "  (gru): GRU(300, 128, batch_first=True, bidirectional=True)\n",
              "  (dropout): Dropout(p=0.3, inplace=False)\n",
              "  (fc): Linear(in_features=256, out_features=20, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# checkpoint 로드\n",
        "bpe_gru_ckpt = torch.load(\"best_bpe_gru.pt\", map_location=cfg.device)\n",
        "\n",
        "# state_dict만 꺼내서 로드\n",
        "bpe_gru.load_state_dict(bpe_gru_ckpt[\"model_state\"])\n",
        "\n",
        "# 토크나이저 로드\n",
        "bpe_tokenizer = Tokenizer.from_file(\"best_bpe_gru_tokenizer.json\")\n",
        "\n",
        "# 디바이스 이동\n",
        "bpe_gru.to(cfg.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWH8KU4GJcao"
      },
      "source": [
        "# 5. 평가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eySOdF8RJcao"
      },
      "outputs": [],
      "source": [
        "# 평가 전용 infer 함수\n",
        "@torch.no_grad()\n",
        "def evaluate_metrics(model, dataloader, device):\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for batch in dataloader:\n",
        "        # batch: (input_ids, labels, lengths)\n",
        "        input_ids, labels, lengths = batch\n",
        "\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device)\n",
        "        lengths = lengths.to(device)\n",
        "\n",
        "        logits = model(input_ids, lengths)   # (B, num_classes)\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        all_preds.append(preds.cpu())\n",
        "        all_labels.append(labels.cpu())\n",
        "\n",
        "    all_preds = torch.cat(all_preds).numpy()\n",
        "    all_labels = torch.cat(all_labels).numpy()\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        all_labels,\n",
        "        all_preds,\n",
        "        average=\"macro\",\n",
        "        zero_division=0\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1\": f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ailtWY0HJcao",
        "outputId": "824e3b96-da66-4241-b991-5493b5a6f7e9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "results_r = {}\n",
        "\n",
        "results_r[\"Word2Vec-LSTM\"] = evaluate_metrics(\n",
        "    w2v_lstm, w2v_test_loader, cfg.device\n",
        ")\n",
        "\n",
        "results_r[\"FastText-LSTM\"] = evaluate_metrics(\n",
        "    ft_lstm, ft_test_loader, cfg.device\n",
        ")\n",
        "\n",
        "results_r[\"GloVe-LSTM\"] = evaluate_metrics(\n",
        "    gl_lstm, gl_test_loader, cfg.device\n",
        ")\n",
        "\n",
        "results_r[\"BPE-LSTM\"] = evaluate_metrics(\n",
        "    bpe_lstm, bpe_test_loader, cfg.device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yx1LEAN9Jcap",
        "outputId": "d3244cea-a196-4593-9830-ea0a27969ff5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Word2Vec-LSTM</th>\n",
              "      <td>0.673394</td>\n",
              "      <td>0.673130</td>\n",
              "      <td>0.661502</td>\n",
              "      <td>0.661256</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FastText-LSTM</th>\n",
              "      <td>0.538237</td>\n",
              "      <td>0.542024</td>\n",
              "      <td>0.529939</td>\n",
              "      <td>0.531160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GloVe-LSTM</th>\n",
              "      <td>0.754647</td>\n",
              "      <td>0.752910</td>\n",
              "      <td>0.745128</td>\n",
              "      <td>0.743761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BPE-LSTM</th>\n",
              "      <td>0.319835</td>\n",
              "      <td>0.326344</td>\n",
              "      <td>0.317058</td>\n",
              "      <td>0.305976</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               accuracy  precision    recall        f1\n",
              "Word2Vec-LSTM  0.673394   0.673130  0.661502  0.661256\n",
              "FastText-LSTM  0.538237   0.542024  0.529939  0.531160\n",
              "GloVe-LSTM     0.754647   0.752910  0.745128  0.743761\n",
              "BPE-LSTM       0.319835   0.326344  0.317058  0.305976"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_results_r = pd.DataFrame(results_r).T\n",
        "df_results_r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-LxAGopJcap",
        "outputId": "83c60124-8d8a-4376-9353-73ff534c7261"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "results_g = {}\n",
        "\n",
        "results_g[\"Word2Vec-GRU\"] = evaluate_metrics(\n",
        "    w2v_gru, w2v_test_loader, cfg.device\n",
        ")\n",
        "\n",
        "results_g[\"FastText-GRU\"] = evaluate_metrics(\n",
        "    ft_gru, ft_test_loader, cfg.device\n",
        ")\n",
        "\n",
        "results_g[\"GloVe-GRU\"] = evaluate_metrics(\n",
        "    gl_gru, gl_test_loader, cfg.device\n",
        ")\n",
        "\n",
        "results_g[\"BPE-GRU\"] = evaluate_metrics(\n",
        "    bpe_gru, bpe_test_loader, cfg.device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFPzuAI9Jcap",
        "outputId": "095ffa97-0523-49f5-b414-0de8caa8edbb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Word2Vec-GRU</th>\n",
              "      <td>0.736192</td>\n",
              "      <td>0.741849</td>\n",
              "      <td>0.726332</td>\n",
              "      <td>0.724996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>FastText-GRU</th>\n",
              "      <td>0.675783</td>\n",
              "      <td>0.678990</td>\n",
              "      <td>0.667696</td>\n",
              "      <td>0.667967</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GloVe-GRU</th>\n",
              "      <td>0.757568</td>\n",
              "      <td>0.753262</td>\n",
              "      <td>0.747903</td>\n",
              "      <td>0.748167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BPE-GRU</th>\n",
              "      <td>0.379580</td>\n",
              "      <td>0.376939</td>\n",
              "      <td>0.374700</td>\n",
              "      <td>0.365848</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              accuracy  precision    recall        f1\n",
              "Word2Vec-GRU  0.736192   0.741849  0.726332  0.724996\n",
              "FastText-GRU  0.675783   0.678990  0.667696  0.667967\n",
              "GloVe-GRU     0.757568   0.753262  0.747903  0.748167\n",
              "BPE-GRU       0.379580   0.376939  0.374700  0.365848"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_results_g = pd.DataFrame(results_g).T\n",
        "df_results_g"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2xAWxLa9d36"
      },
      "source": [
        "## 5.1. 코멘트"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyTsDX_-9d36"
      },
      "source": [
        "- BPE 기반 LSTM, GRU 모델은 Word2Vec, FastText, GloVe 대비 현저히 낮은 성능을 보였습니다.\n",
        "- 이는 BPE가 사전학습된 의미 임베딩을 제공하지 않으며, 서브워드 단위 토큰화로 인해 시퀀스 길이가 크게 증가한 상태에서 최대 길이를 512로 제한하여 입력을 절단함으로써 문맥 정보 손실이 크게 발생했기 때문으로 해석할 수 있습니다.\n",
        "- 동일한 모델 구조와 학습 조건 하에서 비교했을 때, 사전학습 임베딩을 사용하는 Word2Vec, FastText, GloVe는 RNN 계열 모델과 상대적으로 높은 적합성을 보인 반면, 사전학습이 되지 않은 BPE는 RNN 기반 구조와 가장 부적합한 조합으로 나타났습니다.\n",
        "- BPE는 사전학습되지 않았다는 점을 고려하여 최대 길이와 훈련 에포크를 늘리면 성능이 크게 개선될 것으로 보입니다"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}