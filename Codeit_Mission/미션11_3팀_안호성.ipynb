{"cells":[{"cell_type":"markdown","metadata":{"id":"9bcZc42Acap0"},"source":["# 한영 기계번역"]},{"cell_type":"markdown","metadata":{"id":"p5JWyAnQdj2b"},"source":["## 개요\n","\n","- 사용 데이터: 일상 생활 및 구어체 한영 번역 데이터\n","\n","- 설계\n","| 태스크 | 기술 |\n","|----------------|-----------------------------|\n","| 데이터 전처리 | 데이터 로드, 전처리 |\n","| 토큰화 | BPE (Byte Pair Encoding) |\n","| 사용 모델 | Seq2Seq (GRU), Seq2Seq with Attention (GRU)  |\n","| 사용 어텐션 | Scaled Luong Attention |"]},{"cell_type":"markdown","metadata":{"id":"Ho_-cURBi2An"},"source":["# 1. 데이터 전처리 및 토큰화"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"erBGaccBECdf","outputId":"d7603081-5022-4bf5-a33a-967f0cf542d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: sentencepiece in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (0.2.1)\n"]}],"source":["!pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UONlBt_AbFnB"},"outputs":[],"source":["# 파일 경로 및 파일 읽기 라이브러리\n","from pathlib import Path\n","from dataclasses import dataclass\n","import math\n","import zipfile\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import json\n","from typing import Dict, List, Any, Tuple, Optional\n","\n","# 토큰 관련 라이브러리\n","import sentencepiece as spm\n","\n","# 파이토치 관련 라이브러리\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","# 평가용\n","import sacrebleu\n","\n","# 훈련 시 시각화\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Gx0MqW8QXZi"},"outputs":[],"source":["\"\"\"\n","Config 클래스 정의\n","\n","- 상수 변수 정의\n","- @dataclass 데코레이터 추가\n","\"\"\"\n","@dataclass\n","class Config:\n","    # 데이터 폴더 생성\n","    root = Path(\".\")\n","    raw_dir = root / \"data\"\n","    data_dir = raw_dir / \"mission11_koen\"\n","    model_dir = root / \"models\"\n","\n","    # 토크나이저용 변수\n","    vocab_size = 4000\n","    coverage = 1.0\n","\n","    # 데이터로더용 변수 선언\n","    max_len = 100\n","    batch_size = 32\n","    num_workers = 2\n","\n","    # 데이터 분리용 변수 선언\n","    seed = 42\n","\n","    # 모델용 변수 선언\n","    emb_dim = 256\n","    hid_dim = 512\n","    num_layers = 1\n","    dropout = 0.1\n","\n","    # 학습용 변수 선언: 학습률, 에포크, 인내심, 최소 개선 폭\n","    lr = 3e-4\n","    epochs = 10\n","    patience = 3\n","    min_delta = 0.01\n","    teacher_forcing = 0.5\n","    grad_clip = 1.0\n","\n","    # 평가용 변\n","\n","    # 디바이스 설정\n","    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wf-eejLTbJAh"},"outputs":[],"source":["# 변수 생성 및 폴더 생성\n","cfg = Config()\n","\n","# 모델, 데이터 저장 폴더\n","cfg.model_dir.mkdir(parents=True, exist_ok=True)\n","cfg.raw_dir.mkdir(parents=True, exist_ok=True)"]},{"cell_type":"markdown","metadata":{"id":"1lGX5X9sECdi"},"source":["## 2. 데이터 다운로드"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mVqg86vyECdj"},"outputs":[],"source":["\"\"\"\n","데이터 다운로드\n","\n","- 미션 페이지에서 데이터 다운로드\n","\"\"\"\n","with zipfile.ZipFile(f\"{str(cfg.raw_dir)}/mission11_koen.zip\") as z:\n","    z.extractall(path=f\"{str(cfg.raw_dir)}/mission11_koen\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V0LxATXHECdj"},"outputs":[],"source":["# JSON 파일 로드 함수\n","def load_json(json_path: Path):\n","    with open(str(json_path), \"r\", encoding=\"utf-8\") as j:\n","        data = json.load(j)\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gx79IrFwECdk"},"outputs":[],"source":["# 훈련, 검증 데이터 불러오기\n","train_data = load_json(cfg.data_dir / \"일상생활및구어체_한영_train_set.json\")\n","val_data = load_json(cfg.data_dir / \"일상생활및구어체_한영_valid_set.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9XN8ye2QECdk","outputId":"e70eb297-3a20-4196-fba6-88661924aa4e"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'sn': 'INTSALDSUT062119042703238', 'data_set': '일상생활및구어체', 'domain': '해외영업', 'subdomain': '도소매유통', 'ko_original': '원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.', 'ko': '원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.', 'mt': 'If you reply to the color you want, we will start making it right away.', 'en': 'If you reply to the color you want, we will start making it right away.', 'source_language': 'ko', 'target_language': 'en', 'word_count_ko': 7, 'word_count_en': 15, 'word_ratio': 2.143, 'file_name': 'INTSAL_DSUT.xlsx', 'source': '크라우드소싱', 'license': 'open', 'style': '구어체', 'included_unknown_words': False, 'ner': None}\n","원문: 원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.\n","번역: If you reply to the color you want, we will start making it right away.\n","<class 'dict'>\n"]}],"source":["# 훈련 데이터 샘플 출력\n","print(train_data[\"data\"][0])\n","print(f\"원문: {train_data[\"data\"][0].get(\"ko\")}\")\n","print(f\"번역: {train_data[\"data\"][0].get(\"en\")}\")\n","print(type(train_data))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7S9Z__hZECdl"},"outputs":[],"source":["# 데이터 정규화 함수 생성\n","def extract_pairs(json_data: dict):\n","    \"\"\"\n","    데이터셋을 \"list[{'ko':..., 'en':...}]\" 형태로 정규화(페어 추출)\n","\n","    arg:\n","        dict: json 데이터\n","\n","    반환:\n","        pairs: 각 원소는 {'ko': str, 'en': str}\n","    \"\"\"\n","    if isinstance(json_data, dict):\n","        pairs_list = []\n","        for d in json_data[\"data\"]:\n","            kor = d.get(\"ko\")\n","            eng = d.get(\"en\")\n","            pairs_list.append({\"ko\": kor, \"en\": eng})\n","\n","    else:\n","        raise TypeError(f\"Unsupported JSON root type: {type(json_data)}\")\n","\n","\n","    # 검증\n","    normalized_pairs: List[Dict[str, str]] = []\n","    for item in pairs_list:\n","\n","        # 불량 데이터 거르기\n","        ko_text = item.get(\"ko\", None)\n","        en_text = item.get(\"en\", None)\n","\n","        if ko_text is None or en_text is None:\n","            continue\n","\n","        # 문자열이 아니면 강제로 변환\n","        normalized_pairs.append({\"ko\": str(ko_text), \"en\": str(en_text)})\n","\n","    return normalized_pairs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8i7JoGkfECdl"},"outputs":[],"source":["# json 데이터 정규화\n","train_normalized = extract_pairs(train_data)\n","val_normalized = extract_pairs(val_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fl37xCobECdl","outputId":"e403a044-2846-420a-be1c-c978c544e7ae"},"outputs":[{"data":{"text/plain":["{'ko': '원하시는 색상을 회신해 주시면 바로 제작 들어가겠습니다.',\n"," 'en': 'If you reply to the color you want, we will start making it right away.'}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# 정규화 데이터 예시 출력\n","train_normalized[0]"]},{"cell_type":"markdown","metadata":{"id":"UXw5eKeAECdm"},"source":["## 3. BPE 토크나이저"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EhZ-k1zZECdm"},"outputs":[],"source":["# bpe 토크나이저 훈련 함수\n","def train_bpe(\n","        texts: List[str],\n","        model_prefix: str,\n","        model_dir: Path=cfg.model_dir,\n","        vocab_size: int=cfg.vocab_size,\n","        char_coverage: float=cfg.coverage\n","        ) -> Path:\n","    \"\"\"\n","    BPE 모델 학습 후 model_path 반환\n","\n","    args:\n","        - text: 학습에 사용할 문장 리스트\n","        - model_prefix: 출력 모델의 prefix\n","        - model_dir: 모델 저장 경로\n","\n","    returns:\n","        - model_path\n","    \"\"\"\n","    model_dir.mkdir(parents=True, exist_ok=True)\n","\n","    # 훈련용 임시 텍스트 파일 생성\n","    train_txt_path = model_dir / f\"{model_prefix}.txt\"\n","    with open(train_txt_path, \"w\", encoding=\"utf-8\") as f:\n","        for line in texts:\n","            # 줄 단위로 기록 (SentencePiece는 한 줄을 힌 문장으로 인식)\n","            f.write(line.replace(\"\\n\", \" \").strip() + \"\\n\")\n","\n","    # special token ids 지정:\n","    # pad_id=0, bos_id=1, eos_id=2, unk_id=3로 고정\n","    spm.SentencePieceTrainer.train(\n","        input=str(train_txt_path),\n","        model_prefix=str(model_dir / model_prefix),\n","        vocab_size=vocab_size,\n","        model_type=\"bpe\",  # BPE 사용\n","        character_coverage=char_coverage,\n","\n","        # 특수 토큰 지정\n","        pad_id=0,\n","        bos_id=1,\n","        eos_id=2,\n","        unk_id=3,\n","\n","        # 학습 안정성/일반성 옵션\n","        normalization_rule_name=\"nmt_nfkc\",\n","    )\n","\n","    model_path = model_dir / f\"{model_prefix}.model\"\n","    return model_path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ru4udbcYECdn"},"outputs":[],"source":["# 토크나이저 래퍼 생성\n","class SentencePieceTokenizer:\n","    \"\"\"\n","    토크나이저 래퍼\n","\n","    - encode: 텍스트 -> 토큰 id\n","    - decode: 토큰 id -> 텍스트\n","    \"\"\"\n","    def __init__(self, model_path: Path):\n","        self.tokenizer = spm.SentencePieceProcessor()\n","        self.tokenizer.load(str(model_path))\n","\n","        # 특수 토큰 id (정수)\n","        self.pad_id = self.tokenizer.pad_id()\n","        self.unk_id = self.tokenizer.unk_id()\n","        self.bos_id = self.tokenizer.bos_id()\n","        self.eos_id = self.tokenizer.eos_id()\n","\n","        # 안전성 체크\n","        assert self.pad_id >= 0\n","        assert self.bos_id >= 0\n","        assert self.eos_id >= 0\n","\n","    def encode(self, text_str: str, max_len: int) -> List[int]:\n","        sub_ids = self.tokenizer.encode(text_str, out_type=int)\n","\n","        max_content_len = max_len - 2\n","        sub_ids = sub_ids[:max_content_len]\n","\n","        return [self.bos_id] + sub_ids + [self.eos_id]\n","\n","    def decode(self, token_ids: List[int]) -> str:\n","        filtered_ids = [\n","            tid for tid in token_ids\n","            if tid not in (self.bos_id, self.eos_id, self.pad_id)\n","        ]\n","        return self.tokenizer.decode(filtered_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tcepQLjkECdn","outputId":"add0cc32-0fd2-441c-af81-dbe9f2b5f4d8"},"outputs":[{"name":"stderr","output_type":"stream","text":["sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n","trainer_spec {\n","  input: models/bpe_ko.txt\n","  input_format: \n","  model_prefix: models/bpe_ko\n","  model_type: BPE\n","  vocab_size: 4000\n","  self_test_sample_size: 0\n","  character_coverage: 1\n","  input_sentence_size: 0\n","  shuffle_input_sentence: 1\n","  seed_sentencepiece_size: 1000000\n","  shrinking_factor: 0.75\n","  max_sentence_length: 4192\n","  num_threads: 16\n","  num_sub_iterations: 2\n","  max_sentencepiece_length: 16\n","  split_by_unicode_script: 1\n","  split_by_number: 1\n","  split_by_whitespace: 1\n","  split_digits: 0\n","  pretokenization_delimiter: \n","  treat_whitespace_as_suffix: 0\n","  allow_whitespace_only_pieces: 0\n","  required_chars: \n","  byte_fallback: 0\n","  vocabulary_output_piece_score: 1\n","  train_extremely_large_corpus: 0\n","  seed_sentencepieces_file: \n","  hard_vocab_limit: 1\n","  use_all_vocab: 0\n","  unk_id: 3\n","  bos_id: 1\n","  eos_id: 2\n","  pad_id: 0\n","  unk_piece: <unk>\n","  bos_piece: <s>\n","  eos_piece: </s>\n","  pad_piece: <pad>\n","  unk_surface:  ⁇ \n","  enable_differential_privacy: 0\n","  differential_privacy_noise_level: 0\n","  differential_privacy_clipping_threshold: 0\n","}\n","normalizer_spec {\n","  name: nmt_nfkc\n","  add_dummy_prefix: 1\n","  remove_extra_whitespaces: 1\n","  escape_whitespaces: 1\n","  normalization_rule_tsv: \n","}\n","denormalizer_spec {}\n","trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n","trainer_interface.cc(186) LOG(INFO) Loading corpus: models/bpe_ko.txt\n","trainer_interface.cc(148) LOG(INFO) Loaded 1000000 lines\n","trainer_interface.cc(125) LOG(WARNING) Too many sentences are loaded! (1200000), which may slow down training.\n","trainer_interface.cc(127) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n","trainer_interface.cc(130) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n","trainer_interface.cc(411) LOG(INFO) Loaded all 1200000 sentences\n","trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n","trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n","trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n","trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n","trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n","trainer_interface.cc(541) LOG(INFO) all chars count=33242977\n","trainer_interface.cc(562) LOG(INFO) Alphabet size=2170\n","trainer_interface.cc(563) LOG(INFO) Final character coverage=1\n","trainer_interface.cc(594) LOG(INFO) Done! preprocessed 1200000 sentences.\n","trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 1200000\n","trainer_interface.cc(611) LOG(INFO) Done! 642697\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=623337 min_freq=533\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=88918 size=20 all=103183 active=9529 piece=▁아\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=49148 size=40 all=108106 active=14452 piece=▁거\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=41351 size=60 all=110826 active=17172 piece=▁있는\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=32164 size=80 all=112862 active=19208 piece=▁만\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28175 size=100 all=115519 active=21865 piece=▁방\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=28159 min_freq=437\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=24798 size=120 all=117696 active=7832 piece=시면\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22347 size=140 all=120240 active=10376 piece=▁아니\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19520 size=160 all=122531 active=12667 piece=다고\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=18115 size=180 all=125751 active=15887 piece=적인\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16633 size=200 all=127711 active=17847 piece=▁것입니다\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=16559 min_freq=379\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14913 size=220 all=129795 active=8467 piece=▁불\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14121 size=240 all=131749 active=10421 piece=▁저희가\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12947 size=260 all=133627 active=12299 piece=하면\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12082 size=280 all=135820 active=14492 piece=▁그것\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11545 size=300 all=137353 active=16025 piece=해주\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=11528 min_freq=340\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10898 size=320 all=138754 active=7989 piece=▁주시\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10300 size=340 all=140182 active=9417 piece=▁어떤\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9665 size=360 all=141266 active=10501 piece=▁문의\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9025 size=380 all=142985 active=12220 piece=▁양\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8723 size=400 all=144893 active=14128 piece=▁다양한\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=8685 min_freq=314\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8159 size=420 all=146004 active=8352 piece=▁이상\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7778 size=440 all=147813 active=10161 piece=▁도움\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7360 size=460 all=150848 active=13196 piece=▁앞\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7046 size=480 all=152389 active=14737 piece=▁이런\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6704 size=500 all=154746 active=17094 piece=▁쉽\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6698 min_freq=285\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6392 size=520 all=155849 active=8808 piece=▁야\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6126 size=540 all=157702 active=10661 piece=▁주셔서\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5892 size=560 all=159580 active=12539 piece=▁받아\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5667 size=580 all=160585 active=13544 piece=▁거래\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5543 size=600 all=162373 active=15332 piece=▁첨\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5539 min_freq=264\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5359 size=620 all=164144 active=9876 piece=▁작업\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5146 size=640 all=166290 active=12022 piece=▁유지\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4969 size=660 all=168291 active=14023 piece=▁같아요\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4777 size=680 all=169520 active=15252 piece=▁여러분\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4648 size=700 all=170493 active=16225 piece=▁출시\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4647 min_freq=245\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4542 size=720 all=171595 active=9566 piece=▁돌\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4413 size=740 all=173165 active=11136 piece=▁첨부\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4250 size=760 all=174445 active=12416 piece=신가요\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4100 size=780 all=176270 active=14241 piece=▁해당\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3988 size=800 all=177749 active=15720 piece=▁프로젝\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3983 min_freq=230\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3882 size=820 all=179137 active=10271 piece=▁면\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3817 size=840 all=181367 active=12501 piece=▁주시기\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3733 size=860 all=182765 active=13899 piece=▁30\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3629 size=880 all=184667 active=15801 piece=그런데\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3504 size=900 all=186428 active=17562 piece=▁더욱\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3501 min_freq=215\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3436 size=920 all=188551 active=11441 piece=▁최대한\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3347 size=940 all=189674 active=12564 piece=▁은\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3280 size=960 all=190639 active=13529 piece=므로\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3190 size=980 all=192046 active=14936 piece=애하는\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3110 size=1000 all=193285 active=16175 piece=도를\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3106 min_freq=203\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3054 size=1020 all=194773 active=10995 piece=▁내일\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2996 size=1040 all=195983 active=12205 piece=계를\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2941 size=1060 all=196893 active=13115 piece=▁원하는\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2859 size=1080 all=198477 active=14699 piece=▁있다는\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2800 size=1100 all=199841 active=16063 piece=▁들었습니다\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2796 min_freq=192\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2736 size=1120 all=201226 active=11377 piece=▁응\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2695 size=1140 all=202288 active=12439 piece=▁돼요\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2648 size=1160 all=203754 active=13905 piece=▁이름\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2607 size=1180 all=205016 active=15167 piece=▁이동\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2550 size=1200 all=206341 active=16492 piece=▁그거\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2547 min_freq=183\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2485 size=1220 all=207754 active=11683 piece=▁저도\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2423 size=1240 all=208758 active=12687 piece=▁자신\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2365 size=1260 all=210490 active=14419 piece=▁그것을\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2312 size=1280 all=212026 active=15955 piece=▁될까요\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2273 size=1300 all=213539 active=17468 piece=▁쉬\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2272 min_freq=174\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2220 size=1320 all=214482 active=11543 piece=▁발전\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2189 size=1340 all=215566 active=12627 piece=▁청소\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2152 size=1360 all=216690 active=13751 piece=▁신뢰\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2107 size=1380 all=217560 active=14621 piece=▁납품\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2072 size=1400 all=218732 active=15793 piece=▁귀사에서\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2062 min_freq=166\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2038 size=1420 all=220067 active=12269 piece=으니\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1992 size=1440 all=220981 active=13183 piece=있는\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1964 size=1460 all=221857 active=14059 piece=▁압\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1922 size=1480 all=223126 active=15328 piece=▁고급\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1884 size=1500 all=224274 active=16476 piece=이를\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1882 min_freq=159\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1845 size=1520 all=225583 active=12377 piece=어나\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1822 size=1540 all=227091 active=13885 piece=▁덕\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1795 size=1560 all=228137 active=14931 piece=▁원하시는\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1760 size=1580 all=229112 active=15906 piece=▁부서\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1734 size=1600 all=230354 active=17148 piece=▁상담\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1730 min_freq=152\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1715 size=1620 all=231184 active=12258 piece=하십니까\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1684 size=1640 all=232241 active=13315 piece=▁이전\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1650 size=1660 all=233631 active=14705 piece=▁법\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1631 size=1680 all=235042 active=16116 piece=사에서\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1616 size=1700 all=236294 active=17368 piece=▁디지털\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1615 min_freq=145\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1604 size=1720 all=237734 active=13220 piece=▁있기\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1585 size=1740 all=238527 active=14013 piece=▁발표\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1562 size=1760 all=239210 active=14696 piece=▁저기\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1535 size=1780 all=240618 active=16104 piece=▁흡\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1517 size=1800 all=241580 active=17066 piece=▁팩\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1517 min_freq=140\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1505 size=1820 all=242233 active=12696 piece=▁원단\n","trainer_interface.cc(689) LOG(INFO) Saving model: models/bpe_ko.model\n","trainer_interface.cc(701) LOG(INFO) Saving vocabs: models/bpe_ko.vocab\n","sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n","trainer_spec {\n","  input: models/bpe_en.txt\n","  input_format: \n","  model_prefix: models/bpe_en\n","  model_type: BPE\n","  vocab_size: 4000\n","  self_test_sample_size: 0\n","  character_coverage: 1\n","  input_sentence_size: 0\n","  shuffle_input_sentence: 1\n","  seed_sentencepiece_size: 1000000\n","  shrinking_factor: 0.75\n","  max_sentence_length: 4192\n","  num_threads: 16\n","  num_sub_iterations: 2\n","  max_sentencepiece_length: 16\n","  split_by_unicode_script: 1\n","  split_by_number: 1\n","  split_by_whitespace: 1\n","  split_digits: 0\n","  pretokenization_delimiter: \n","  treat_whitespace_as_suffix: 0\n","  allow_whitespace_only_pieces: 0\n","  required_chars: \n","  byte_fallback: 0\n","  vocabulary_output_piece_score: 1\n","  train_extremely_large_corpus: 0\n","  seed_sentencepieces_file: \n","  hard_vocab_limit: 1\n","  use_all_vocab: 0\n","  unk_id: 3\n","  bos_id: 1\n","  eos_id: 2\n","  pad_id: 0\n","  unk_piece: <unk>\n","  bos_piece: <s>\n","  eos_piece: </s>\n","  pad_piece: <pad>\n","  unk_surface:  ⁇ \n","  enable_differential_privacy: 0\n","  differential_privacy_noise_level: 0\n","  differential_privacy_clipping_threshold: 0\n","}\n","normalizer_spec {\n","  name: nmt_nfkc\n","  add_dummy_prefix: 1\n","  remove_extra_whitespaces: 1\n","  escape_whitespaces: 1\n","  normalization_rule_tsv: \n","}\n","denormalizer_spec {}\n","trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n","trainer_interface.cc(186) LOG(INFO) Loading corpus: models/bpe_en.txt\n","trainer_interface.cc(148) LOG(INFO) Loaded 1000000 lines\n","trainer_interface.cc(125) LOG(WARNING) Too many sentences are loaded! (1200000), which may slow down training.\n","trainer_interface.cc(127) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n","trainer_interface.cc(130) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n","trainer_interface.cc(411) LOG(INFO) Loaded all 1200000 sentences\n","trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>\n","trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>\n","trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>\n","trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>\n","trainer_interface.cc(432) LOG(INFO) Normalizing sentences...\n","trainer_interface.cc(541) LOG(INFO) all chars count=66465404\n","trainer_interface.cc(552) LOG(INFO) Done: 100% characters are covered.\n","trainer_interface.cc(562) LOG(INFO) Alphabet size=205\n","trainer_interface.cc(563) LOG(INFO) Final character coverage=1\n","trainer_interface.cc(594) LOG(INFO) Done! preprocessed 1200000 sentences.\n","trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 1200000\n","trainer_interface.cc(611) LOG(INFO) Done! 175369\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1481386 min_freq=158\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=413531 size=20 all=4177 active=2114 piece=▁b\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=263373 size=40 all=5194 active=3131 piece=▁in\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=155312 size=60 all=6242 active=4179 piece=▁pro\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=104873 size=80 all=7602 active=5539 piece=ith\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=82120 size=100 all=8671 active=6608 piece=▁produ\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=81837 min_freq=5590\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=69710 size=120 all=9745 active=2072 piece=all\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=57887 size=140 all=10927 active=3254 piece=▁Th\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=50415 size=160 all=12223 active=4550 piece=ood\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43698 size=180 all=13325 active=5652 piece=▁ch\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=37885 size=200 all=14345 active=6672 piece=and\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=37724 min_freq=5212\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=35099 size=220 all=15641 active=2165 piece=hen\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=30697 size=240 all=16424 active=2948 piece=▁would\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=28113 size=260 all=17262 active=3786 piece=up\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=25315 size=280 all=18311 active=4835 piece=og\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22927 size=300 all=19465 active=5989 piece=▁In\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=22870 min_freq=4504\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21772 size=320 all=20300 active=1796 piece=▁don\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=20540 size=340 all=20973 active=2469 piece=▁because\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=19107 size=360 all=21902 active=3398 piece=ious\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=17756 size=380 all=22672 active=4168 piece=▁He\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16621 size=400 all=23362 active=4858 piece=ans\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=16569 min_freq=3403\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=15259 size=420 all=24229 active=1921 piece=▁just\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=14399 size=440 all=24762 active=2454 piece=▁pur\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13524 size=460 all=25213 active=2905 piece=▁after\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12874 size=480 all=25542 active=3234 piece=ents\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=12092 size=500 all=26102 active=3794 piece=rough\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=12089 min_freq=2745\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11479 size=520 all=26533 active=1721 piece=itt\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10916 size=540 all=26922 active=2110 piece=▁car\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10430 size=560 all=27498 active=2686 piece=hed\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=10073 size=580 all=28086 active=3274 piece=ild\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9540 size=600 all=28516 active=3704 piece=▁does\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=9536 min_freq=2304\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9124 size=620 all=28947 active=1856 piece=▁email\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8724 size=640 all=29202 active=2111 piece=▁num\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8391 size=660 all=29695 active=2604 piece=▁call\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8071 size=680 all=30124 active=3033 piece=ull\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7859 size=700 all=30413 active=3322 piece=▁number\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7835 min_freq=1995\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7678 size=720 all=30855 active=1962 piece=ason\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7448 size=740 all=31214 active=2321 piece=▁using\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7181 size=760 all=31521 active=2628 piece=▁fl\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6956 size=780 all=31854 active=2961 piece=▁each\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6694 size=800 all=32408 active=3515 piece=▁purchase\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=6663 min_freq=1726\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6509 size=820 all=32806 active=2019 piece=The\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6366 size=840 all=33305 active=2518 piece=sel\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6112 size=860 all=33816 active=3029 piece=ual\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5960 size=880 all=34159 active=3372 piece=▁Let\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5820 size=900 all=34639 active=3852 piece=▁difficult\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5817 min_freq=1503\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5698 size=920 all=34892 active=1982 piece=ety\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5568 size=940 all=35265 active=2355 piece=ife\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5424 size=960 all=35682 active=2772 piece=▁partic\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5181 size=980 all=35974 active=3064 piece=cture\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5010 size=1000 all=36376 active=3466 piece=▁small\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=5005 min_freq=1343\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4845 size=1020 all=36654 active=2093 piece=▁within\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4767 size=1040 all=36895 active=2334 piece=als\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4620 size=1060 all=37291 active=2730 piece=▁doesn\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4530 size=1080 all=37598 active=3037 piece=▁contain\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4400 size=1100 all=38042 active=3481 piece=▁fil\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4399 min_freq=1195\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4302 size=1120 all=38507 active=2352 piece=▁money\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4191 size=1140 all=38822 active=2667 piece=▁100\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4102 size=1160 all=39123 active=2968 piece=ages\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4029 size=1180 all=39325 active=3170 piece=▁Dear\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3916 size=1200 all=39477 active=3322 piece=reen\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3910 min_freq=1081\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3838 size=1220 all=39927 active=2401 piece=▁Why\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3735 size=1240 all=40169 active=2643 piece=▁ingred\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3633 size=1260 all=40364 active=2838 piece=▁experience\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3538 size=1280 all=40616 active=3090 piece=▁review\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3476 size=1300 all=40858 active=3332 piece=ien\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3475 min_freq=978\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3383 size=1320 all=41072 active=2217 piece=▁sir\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3301 size=1340 all=41376 active=2521 piece=ised\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3243 size=1360 all=41735 active=2880 piece=▁video\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3185 size=1380 all=41991 active=3136 piece=▁ingredients\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3118 size=1400 all=42161 active=3306 piece=▁catalog\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3113 min_freq=912\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3058 size=1420 all=42420 active=2363 piece=▁mor\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2998 size=1440 all=42581 active=2524 piece=▁love\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2939 size=1460 all=42741 active=2684 piece=▁Of\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2878 size=1480 all=43133 active=3076 piece=▁Because\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2812 size=1500 all=43390 active=3333 piece=▁mind\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2811 min_freq=846\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2768 size=1520 all=43595 active=2370 piece=▁apply\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2699 size=1540 all=43733 active=2508 piece=room\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2658 size=1560 all=44018 active=2793 piece=ption\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2629 size=1580 all=44256 active=3031 piece=orts\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2578 size=1600 all=44521 active=3296 piece=▁treat\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2575 min_freq=786\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2535 size=1620 all=44682 active=2382 piece=▁fabric\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2495 size=1640 all=45109 active=2809 piece=▁based\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2458 size=1660 all=45250 active=2950 piece=▁inconven\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2428 size=1680 all=45637 active=3337 piece=▁bott\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2390 size=1700 all=45890 active=3590 piece=▁benef\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2386 min_freq=731\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2363 size=1720 all=46082 active=2480 piece=ization\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2320 size=1740 all=46278 active=2676 piece=▁red\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2293 size=1760 all=46433 active=2831 piece=▁aff\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2255 size=1780 all=46697 active=3095 piece=▁Ji\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2224 size=1800 all=46979 active=3377 piece=▁swe\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2222 min_freq=686\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2192 size=1820 all=47219 active=2578 piece=▁energy\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2162 size=1840 all=47552 active=2911 piece=ift\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2140 size=1860 all=47940 active=3299 piece=▁safety\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2105 size=1880 all=48079 active=3438 piece=▁cli\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2072 size=1900 all=48334 active=3693 piece=▁Be\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2070 min_freq=638\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2042 size=1920 all=48543 active=2590 piece=▁remember\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2018 size=1940 all=48828 active=2875 piece=pping\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2001 size=1960 all=49005 active=3052 piece=▁pretty\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1979 size=1980 all=49159 active=3206 piece=▁On\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1949 size=2000 all=49408 active=3455 piece=▁hands\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1946 min_freq=593\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1925 size=2020 all=49590 active=2648 piece=▁report\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1884 size=2040 all=49918 active=2976 piece=▁trying\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1844 size=2060 all=50043 active=3101 piece=▁thinking\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1813 size=2080 all=50187 active=3245 piece=pose\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1789 size=2100 all=50388 active=3446 piece=▁platform\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1787 min_freq=559\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1763 size=2120 all=50602 active=2733 piece=▁heat\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1744 size=2140 all=50718 active=2849 piece=▁smell\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1723 size=2160 all=50862 active=2993 piece=to\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1704 size=2180 all=51140 active=3271 piece=▁bought\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1689 size=2200 all=51250 active=3381 piece=▁concer\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1684 min_freq=529\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1663 size=2220 all=51435 active=2743 piece=got\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1641 size=2240 all=51606 active=2914 piece=ills\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1618 size=2260 all=51791 active=3099 piece=▁Not\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1593 size=2280 all=51932 active=3240 piece=▁dom\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1574 size=2300 all=52105 active=3413 piece=fortunately\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1573 min_freq=496\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1556 size=2320 all=52242 active=2740 piece=▁happened\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1536 size=2340 all=52315 active=2813 piece=▁Pro\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1519 size=2360 all=52447 active=2945 piece=▁image\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1492 size=2380 all=52625 active=3123 piece=Okay\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1477 size=2400 all=52915 active=3413 piece=▁separate\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1476 min_freq=468\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1463 size=2420 all=53009 active=2739 piece=▁chair\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1447 size=2440 all=53143 active=2873 piece=▁represent\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1432 size=2460 all=53229 active=2959 piece=▁manufacturing\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1412 size=2480 all=53478 active=3208 piece=▁stain\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1398 size=2500 all=53581 active=3311 piece=▁achie\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1398 min_freq=446\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1376 size=2520 all=53645 active=2739 piece=uff\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1363 size=2540 all=53858 active=2952 piece=▁sale\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1342 size=2560 all=54137 active=3231 piece=▁final\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1325 size=2580 all=54265 active=3359 piece=bon\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1302 size=2600 all=54541 active=3635 piece=▁increasing\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1301 min_freq=425\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1285 size=2620 all=54641 active=2827 piece=go\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1266 size=2640 all=54788 active=2974 piece=.\"\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1251 size=2660 all=54984 active=3170 piece=▁running\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1237 size=2680 all=55255 active=3441 piece=▁offers\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1228 size=2700 all=55341 active=3527 piece=ission\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1228 min_freq=408\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1213 size=2720 all=55549 active=2965 piece=▁above\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1197 size=2740 all=55700 active=3116 piece=▁strateg\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1181 size=2760 all=55862 active=3278 piece=seok\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1169 size=2780 all=55971 active=3387 piece=▁itself\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1154 size=2800 all=56173 active=3589 piece=antly\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1154 min_freq=389\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1138 size=2820 all=56404 active=3024 piece=▁ones\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1128 size=2840 all=56545 active=3165 piece=▁consumers\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1112 size=2860 all=56685 active=3305 piece=▁rich\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1098 size=2880 all=56761 active=3381 piece=ysis\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1084 size=2900 all=56939 active=3559 piece=road\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1083 min_freq=372\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1073 size=2920 all=57039 active=2935 piece=▁wood\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1060 size=2940 all=57195 active=3091 piece=otton\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1047 size=2960 all=57391 active=3287 piece=▁recent\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1029 size=2980 all=57519 active=3415 piece=▁milk\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1021 size=3000 all=57719 active=3615 piece=When\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1021 min_freq=356\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1006 size=3020 all=57877 active=3043 piece=▁manual\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=994 size=3040 all=58083 active=3249 piece=▁continuous\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=985 size=3060 all=58265 active=3431 piece=▁roll\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=971 size=3080 all=58414 active=3580 piece=▁story\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=962 size=3100 all=58536 active=3702 piece=▁vehicles\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=961 min_freq=342\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=954 size=3120 all=58741 active=3132 piece=na\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=942 size=3140 all=58904 active=3295 piece=▁option\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=934 size=3160 all=59110 active=3501 piece=hold\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=929 size=3180 all=59293 active=3684 piece=▁tow\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=925 size=3200 all=59367 active=3758 piece=aught\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=925 min_freq=328\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=917 size=3220 all=59482 active=3077 piece=▁foods\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=909 size=3240 all=59592 active=3187 piece=▁created\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=901 size=3260 all=59705 active=3300 piece=▁Wednes\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=892 size=3280 all=59786 active=3381 piece=clock\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=884 size=3300 all=59848 active=3443 piece=▁established\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=883 min_freq=315\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=874 size=3320 all=60010 active=3155 piece=▁phys\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=864 size=3340 all=60085 active=3230 piece=iber\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=856 size=3360 all=60198 active=3343 piece=▁Min\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=850 size=3380 all=60341 active=3486 piece=more\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=843 size=3400 all=60480 active=3625 piece=ades\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=843 min_freq=305\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=831 size=3420 all=60599 active=3119 piece=vant\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=824 size=3440 all=60764 active=3284 piece=▁cute\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=813 size=3460 all=60848 active=3368 piece=▁spray\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=804 size=3480 all=60891 active=3411 piece=▁fan\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=798 size=3500 all=61065 active=3585 piece=▁cooperate\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=797 min_freq=293\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=790 size=3520 all=61149 active=3138 piece=▁susp\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=783 size=3540 all=61255 active=3244 piece=▁setting\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=773 size=3560 all=61431 active=3420 piece=reet\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=769 size=3580 all=61580 active=3569 piece=▁sports\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=758 size=3600 all=61706 active=3695 piece=igned\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=758 min_freq=282\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=752 size=3620 all=61788 active=3155 piece=▁placed\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=745 size=3640 all=61858 active=3225 piece=▁logo\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=738 size=3660 all=62002 active=3369 piece=iding\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=733 size=3680 all=62144 active=3511 piece=iversity\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=728 size=3700 all=62264 active=3631 piece=▁bags\n","bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=728 min_freq=272\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=721 size=3720 all=62380 active=3230 piece=▁sides\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=716 size=3740 all=62519 active=3369 piece=sp\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=711 size=3760 all=62732 active=3582 piece=▁becomes\n","bpe_model_trainer.cc(268) LOG(INFO) Added: freq=707 size=3780 all=62866 active=3716 piece=▁update\n","trainer_interface.cc(689) LOG(INFO) Saving model: models/bpe_en.model\n","trainer_interface.cc(701) LOG(INFO) Saving vocabs: models/bpe_en.vocab\n"]}],"source":["\"\"\"\n","bpe 토크나이저 학습\n","\n","- 한국어, 영어 언어별 토그나이저 훈련 및 생성\n","- 생성 파일\n","    bpe_ko.model\n","    bpe_ko.vocab\n","    bpe_en.model\n","    bpe_en.vocab\n","\"\"\"\n","# 훈련 데이터에서 언어별 분리\n","ko_text = [p[\"ko\"] for p in train_normalized]\n","en_text = [p[\"en\"] for p in train_normalized]\n","\n","# 한국어 BPE 학습\n","ko_bpe_path = train_bpe(\n","    texts=ko_text,\n","    model_prefix=\"bpe_ko\",\n",")\n","\n","# 영어 BPE 학습\n","en_bpe_path = train_bpe(\n","    texts=en_text,\n","    model_prefix=\"bpe_en\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kow9v1_5ECdn","outputId":"cef8a4fb-22ed-4ecc-bd81-122c4e502329"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1, 350, 1831, 2]\n","[1, 383, 3798, 560, 3817, 2611, 2]\n"]}],"source":["# 생성 토크나이저 로드\n","ko_tok = SentencePieceTokenizer(ko_bpe_path)\n","en_tok = SentencePieceTokenizer(en_bpe_path)\n","\n","# 토크나이저 동작 확인\n","print(ko_tok.encode(\"안녕하세요.\", 20))\n","print(en_tok.encode(\"Heollo, friends\", 20))"]},{"cell_type":"markdown","metadata":{"id":"Ow5ZNMUzECdo"},"source":["# 4. 데이터셋/데이터로더"]},{"cell_type":"markdown","metadata":{"id":"eExcp7xwECdo"},"source":["## 4.1. 데이터셋"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lcpELFo0ECdo"},"outputs":[],"source":["# 데이터셋 클래스 정의\n","class TranslationDataset(Dataset):\n","    \"\"\"\n","    번역 데이터셋 클래스\n","\n","    args:\n","        - pairs: 한영 번역 데이터 리스트\n","        - src_tok: 소스언어 토크나이저\n","        - tgt_tok: 타겟언어 토크나이저\n","        - max_len_src: 소스 언어 인코딩 최대 허용 길이\n","        - max_len_tgt: 타겟 언어 인코딩 최대 허용 길이\n","\n","    returns:\n","        - dataset: 튜플로 감싸진 한영 데이터셋\n","    \"\"\"\n","    def __init__(\n","            self,\n","            pairs: List[Dict[str, str]],\n","            src_tok: SentencePieceTokenizer,\n","            tgt_tok: SentencePieceTokenizer,\n","            max_len_src: int=cfg.max_len,\n","            max_len_tgt: int=cfg.max_len,\n","            ):\n","\n","        self.pairs = pairs\n","        self.src_tok = src_tok\n","        self.tgt_tok = tgt_tok\n","        self.max_len_src = max_len_src\n","        self.max_len_tgt = max_len_tgt\n","\n","    def __len__(self):\n","        return len(self.pairs)\n","\n","    def __getitem__(self, idx):\n","        item = self.pairs[idx]\n","        src_ids = self.src_tok.encode(item[\"ko\"], self.max_len_src)\n","        tgt_ids = self.tgt_tok.encode(item[\"en\"], self.max_len_tgt)\n","        return src_ids, tgt_ids"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4zHA5ddECdp"},"outputs":[],"source":["# 훈련/검증 데이터셋 생선\n","train_ds = TranslationDataset(train_normalized, ko_tok, en_tok)\n","val_ds = TranslationDataset(val_normalized, ko_tok, en_tok)"]},{"cell_type":"markdown","metadata":{"id":"EkRBSL_DECdp"},"source":["## 4.2. 데이터로더(with Collate_fn)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b1ajkRBZECdp"},"outputs":[],"source":["# 커스텀 collate_fn 함수 생성\n","def collate_fn(pad_id_src: int, pad_id_tgt: int):\n","    def _fn(batch):\n","        src, tgt = zip(*batch)\n","        src_len = max(len(x) for x in src)\n","        tgt_len = max(len(x) for x in tgt)\n","\n","        src_tensor = torch.full((len(src), src_len), pad_id_src, dtype=torch.long)\n","        tgt_tensor = torch.full((len(tgt), tgt_len), pad_id_tgt, dtype=torch.long)\n","\n","        for idx, ko in enumerate(src):\n","            src_tensor[idx, :len(ko)] = torch.tensor(ko)\n","        for idx, en in enumerate(tgt):\n","            tgt_tensor[idx, :len(en)] = torch.tensor(en)\n","\n","        return src_tensor, tgt_tensor\n","\n","    return _fn\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EGQJCp69ECdp"},"outputs":[],"source":["# 훈련/검증 데이터로더 생성\n","train_loader = DataLoader(\n","    train_ds,\n","    batch_size=cfg.batch_size,\n","    shuffle=True,\n","    num_workers=cfg.num_workers,\n","    collate_fn=collate_fn(ko_tok.pad_id, en_tok.pad_id)\n",")\n","\n","val_loader = DataLoader(\n","    val_ds,\n","    batch_size=cfg.batch_size,\n","    shuffle=False,\n","    num_workers=cfg.num_workers,\n","    collate_fn=collate_fn(ko_tok.pad_id, en_tok.pad_id)\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UiewL_-TECdp","outputId":"0d42b2b6-2f49-4eb5-ca66-0697a7cef158"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[   1,    6, 1901,  ...,    0,    0,    0],\n","        [   1,  413,  341,  ...,    0,    0,    0],\n","        [   1,    6,   53,  ...,    0,    0,    0],\n","        ...,\n","        [   1,  220, 1888,  ...,    0,    0,    0],\n","        [   1,  414, 1472,  ...,    0,    0,    0],\n","        [   1,   94, 1972,  ...,    0,    0,    0]])\n","tensor([[   1,   38,  540,  ...,    0,    0,    0],\n","        [   1,  489,  700,  ...,    0,    0,    0],\n","        [   1,   38, 1088,  ...,    0,    0,    0],\n","        ...,\n","        [   1,  233,   30,  ...,    0,    0,    0],\n","        [   1, 2215, 3817,  ...,    0,    0,    0],\n","        [   1, 1162,   30,  ...,    0,    0,    0]])\n"]}],"source":["# 샘플 확인\n","src_sam, tgt_sam = next(iter(train_loader))\n","\n","print(src_sam)\n","print(tgt_sam)"]},{"cell_type":"markdown","metadata":{"id":"CDhueQDnECdq"},"source":["# 5. 모델링"]},{"cell_type":"markdown","metadata":{"id":"hUz5raxIECdq"},"source":["## 5.1. Seq2Seq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y4RWS-CsECdq"},"outputs":[],"source":["\"\"\"\n","Encoder 정의\n","\n","입력 문장(한국어)을 읽어서\n","문맥 표현(hidden states)을 생성하는 역할\n","\"\"\"\n","class Encoder(nn.Module):\n","    def __init__(self, vocab, emb, hid, pad_id):\n","        \"\"\"\n","        args:\n","            - vocab (int): 입력 언어 vocabulary 크기 (한국어 BPE vocab size)\n","            - emb (int): 임베딩 차원 (각 토큰을 몇 차원 벡터로 표현할지)\n","            - hid (int): GRU 은닉 상태 차원\n","            - pad_id (int): PAD 토큰 id (embedding에서 무시하기 위해 필요)\n","        \"\"\"\n","        super().__init__()\n","\n","        # Embedding layer\n","        # 입력: (batch, seq_len) -> 토큰 ID\n","        # 출력: (batch, seq_len, emb) -> 임베딩 벡터\n","        self.emb = nn.Embedding(\n","            num_embeddings=vocab,\n","            embedding_dim=emb,\n","            padding_idx=pad_id\n","        )\n","\n","        # GRU\n","        # batch_first=True -> 입력/출력 텐서 shape를 (B, T, D)로 통일\n","        self.gru = nn.GRU(\n","            input_size=emb,\n","            hidden_size=hid,\n","            batch_first=True\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x: (batch_size, src_seq_len) 한국어 문장이 토큰 id로 변환된 상태\n","        \"\"\"\n","        #  토큰 id -> 임베딩 벡터\n","        # (B, T) -> (B, T, emb)\n","        x = self.emb(x)\n","\n","        # GRU 통과\n","        # outputs: (B, T, hid)\n","        # - 모든 타임스텝의 은닉 상태 (attention에서 사용)\n","        # hidden: (1, B, hid)\n","        # - 마지막 타임스텝 은닉 상태 (decoder 초기 상태)\n","        outputs, hidden = self.gru(x)\n","\n","        return outputs, hidden\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u_Nsxd5aECdr"},"outputs":[],"source":["\"\"\"\n","Decoder 정의\n","\n","이전 토큰과 hidden state를 받아\n","다음 단어(영어)를 하나 생성\n","\"\"\"\n","class Decoder(nn.Module):\n","    def __init__(self, vocab, emb, hid, pad_id):\n","        \"\"\"\n","        args:\n","            vocab (int): 출력 언어 vocabulary 크기 (영어 BPE vocab size)\n","            emb (int): 임베딩 차원\n","            hid (int): GRU 은닉 상태 차원\n","            pad_id (int): PAD 토큰 id\n","        \"\"\"\n","        super().__init__()\n","\n","        # 출력 토큰용 embedding\n","        self.emb = nn.Embedding(\n","            num_embeddings=vocab,\n","            embedding_dim=emb,\n","            padding_idx=pad_id\n","        )\n","\n","        # GRU\n","        # 입력은 \"현재 단어 임베딩 1개\"\n","        self.gru = nn.GRU(\n","            input_size=emb,\n","            hidden_size=hid,\n","            batch_first=True\n","        )\n","\n","        # Linear layer\n","        # GRU hidden -> vocabulary 크기의 logits으로 선형변환\n","        self.fc = nn.Linear(hid, vocab)\n","\n","    def forward(self, x, hidden):\n","        \"\"\"\n","        x: (batch_size,) 이전 시점의 토큰 id (영어 단어 1개)\n","\n","        hidden: (1, batch_size, hid) 이전 시점의 GRU 은닉 상태\n","        \"\"\"\n","\n","        # 단어 id -> embedding\n","        # unsqueeze(1): (B,) → (B, 1)\n","        # embedding 후: (B, 1, emb)\n","        x = self.emb(x.unsqueeze(1))\n","\n","        # GRU 한 스텝 실행\n","        # out: (B, 1, hid)\n","        # hidden: (1, B, hid)\n","        out, hidden = self.gru(x, hidden)\n","\n","        # 단어 예측 점수(logits) 생성\n","        # out.squeeze(1): (B, hid)\n","        # logits: (B, vocab)\n","        logits = self.fc(out.squeeze(1))\n","\n","        return logits, hidden\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C0vh0c9QECdr"},"outputs":[],"source":["\"\"\"\n","Seq2Seq 정의\n","\n","- Encoder + Decoder를 묶은 전체 번역 모델\n","\"\"\"\n","class Seq2Seq(nn.Module):\n","    def __init__(self, enc, dec, bos_id, device=cfg.device):\n","        \"\"\"\n","        args:\n","            - enc : Encoder 객체\n","            - dec : Decoder 객체\n","            - bos_id : <BOS> 토큰 id (문장 시작)\n","            - device : cpu / cuda\n","        \"\"\"\n","        super().__init__()\n","        self.enc = enc\n","        self.dec = dec\n","        self.bos_id = bos_id\n","        self.device = device\n","\n","    def forward(self, src, tgt, teacher_forcing: float = cfg.teacher_forcing):\n","        \"\"\"\n","        args:\n","            - src: (B, src_len) 한국어 문장 토큰 id 시퀀스\n","\n","            - tgt: (B, tgt_len) 영어 문장 토큰 id 시퀀스\n","                 학습 시에는 정답 문장 전체가 들어옴\n","\n","            - teacher_forcing: 정답 단어를 다음 입력으로 쓸 확률\n","        \"\"\"\n","\n","        # 기본 정보 추출\n","        B, T = tgt.size()  # batch_size, target sequence length\n","        vocab = self.dec.fc.out_features  # 영어 vocab 크기\n","\n","        # 전체 시점의 출력을 저장할 텐서\n","        # shape: (B, T, vocab)\n","        logits_all = torch.zeros(B, T, vocab, device=self.device)\n","\n","        # Encoder 실행\n","        # enc_out: (B, src_len, hid)\n","        # hidden : (1, B, hid)\n","        enc_out, hidden = self.enc(src)\n","\n","        # 디코더 첫 입력은 <BOS>\n","        # tgt[:, 0] == BOS\n","        x = tgt[:, 0]\n","\n","        # Decoder 타임스텝 반복\n","        for t in range(1, T):\n","            # 한 단어 예측 (hidden은 인코더 유래)\n","            logits, hidden = self.dec(x, hidden)\n","\n","            # 현재 시점 예측 결과 저장\n","            logits_all[:, t] = logits\n","\n","            # Teacher Forcing 여부 결정\n","            use_tf = torch.rand(1).item() < teacher_forcing\n","\n","            # 다음 입력 결정\n","            # - teacher forcing: 정답 단어\n","            # - 아니면: 모델이 예측한 단어\n","            x = tgt[:, t] if use_tf else logits.argmax(dim=1)\n","\n","        return logits_all\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P2VYP3WxECdr"},"outputs":[],"source":["# 모델 생성\n","enc = Encoder(\n","    vocab=ko_tok.tokenizer.get_piece_size(),\n","    emb=cfg.emb_dim,\n","    hid=cfg.hid_dim,\n","    pad_id=ko_tok.pad_id,\n",")\n","\n","dec = Decoder(\n","    vocab=en_tok.tokenizer.get_piece_size(),\n","    emb=cfg.emb_dim,\n","    hid=cfg.hid_dim,\n","    pad_id=en_tok.pad_id,\n",")\n","\n","model_s2s = Seq2Seq(enc, dec, bos_id=en_tok.bos_id).to(cfg.device)"]},{"cell_type":"markdown","metadata":{"id":"4pjZsPHzECds"},"source":["## 5.2. Attention Seq2Seq"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EHOKKLkVECds"},"outputs":[],"source":["\"\"\"\n","Scaled Luong Attention (dot-product attention + scaling) 설계\n","\n","- 디코더의 현재 상태(query)가 인코더 출력(keys/values) 중 어디에 가장 집중해야 하는지를 계산\n","- Scaled Luong attention에서는 별도의 Query, Key, Value를 선형층으로 만들지 않고 GRU의 은닉 상태를 그대로 사용한다.\n","\"\"\"\n","class ScaledLuongAttention(nn.Module):\n","    def __init__(self, hid_dim):\n","        \"\"\"\n","        args:\n","            - hid_dim (int): encoder / decoder GRU의 은닉 차원. dot-product의 분산을 안정화하기 위한 스케일링에 사용\n","        \"\"\"\n","        super().__init__()\n","\n","        # 제곱근 스케일링을 통해 dot(query, key)의 값이 hid_dim에 비례해 커지는 것을 방지\n","        self.scale = math.sqrt(hid_dim)\n","\n","    def forward(self, query, keys, values, mask=None):\n","        \"\"\"\n","        query:\n","            shape = (B, hid) 디코더의 현재 hidden state\n","\n","        keys:\n","            shape = (B, T_src, hid) 인코더의 모든 타임스텝 hidden states\n","\n","        values:\n","            shape = (B, T_src, hid) 실제로 가중합에 사용될 벡터\n","            Luong에서는 keys == values\n","\n","        mask:\n","            shape = (B, T_src)\n","            PAD 토큰 위치를 무시하기 위한 마스크\n","        \"\"\"\n","\n","        # Attention score 계산 (dot-product)\n","        # query.unsqueeze(2): (B, hid) -> (B, hid, 1)\n","        # keys:               (B, T_src, hid)\n","        #\n","        # torch.bmm:\n","        # (B, T_src, hid) @ (B, hid, 1)\n","        # (B, T_src, 1)\n","        scores = torch.bmm(keys, query.unsqueeze(2)).squeeze(2)   # (B, T_src)\n","\n","        # Scaling\n","        # dot-product 값이 너무 커지는 것을 방지\n","        # softmax가 과도하게 한 위치만 선택하는 문제 완화\n","        scores = scores / self.scale\n","\n","        # PAD 위치 마스킹\n","        # mask == 0 인 위치는 PAD\n","        # 해당 위치 score를 매우 작은 값으로 설정\n","        if mask is not None:\n","            scores = scores.masked_fill(mask == 0, -1e9)\n","\n","        # Attention weight 계산\n","        attn = torch.softmax(scores, dim=1)  # (B, T_src)\n","\n","        # Context vector 계산 (가중합)\n","        # attn.unsqueeze(1): (B, 1, T_src)\n","        # values:            (B, T_src, hid)\n","        #\n","        # 결과: (B, 1, hid) -> squeeze -> (B, hid)\n","        context = torch.bmm(attn.unsqueeze(1), values).squeeze(1)\n","\n","        return context\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CxTufoM-ECds"},"outputs":[],"source":["# 어텐션 디코더\n","class AttnDecoder(nn.Module):\n","    \"\"\"\n","    Attention을 사용하는 디코더\n","\n","    구조:\n","    - 이전 단어 embedding\n","    - attention으로 얻은 context\n","    - embddding과 context를 concat\n","    - GRU\n","    - 다음 단어 예측\n","    \"\"\"\n","    def __init__(self, vocab, emb, hid, pad_id):\n","        super().__init__()\n","\n","        # 출력 토큰 embedding\n","        self.emb = nn.Embedding(\n","            num_embeddings=vocab,\n","            embedding_dim=emb,\n","            padding_idx=pad_id\n","        )\n","\n","        # Attention 모듈\n","        self.attn = ScaledLuongAttention(hid)\n","\n","        # GRU\n","        # 입력 차원 = (embedding + context)\n","        # context는 encoder hidden(hid)\n","        self.gru = nn.GRU(\n","            input_size=emb + hid,\n","            hidden_size=hid,\n","            batch_first=True\n","        )\n","\n","        # 출력 projection\n","        # GRU 출력 + context를 결합해 vocab logits 생성\n","        self.fc = nn.Linear(hid * 2, vocab)\n","\n","    def forward(self, x, hidden, enc_out, mask):\n","        \"\"\"\n","        args:\n","            - x: shape = (B,) 이전 시점의 출력 토큰 id\n","            - hidden: shape = (1, B, hid) 이전 시점의 디코더 hidden state\n","            - enc_out: shape = (B, T_src, hid) 인코더의 모든 타임스텝 출력\n","            - mask: shape = (B, T_src) PAD 위치 마스크\n","        \"\"\"\n","\n","        # 이전 단어 embedding\n","        # (B,) -> (B, 1) -> (B, 1, emb)\n","        # GRU는 (batch, time, feature) 형태의 입력을 기대하므로 시간축(unsqueeze(1))을 추가\n","        emb = self.emb(x.unsqueeze(1))\n","\n","        # Attention 계산\n","        # hidden[-1]:\n","        #   (1, B, hid) -> (B, hid)\n","        #   마지막 레이어의 현재 hidden state\n","        context = self.attn(\n","            hidden[-1],   # query\n","            enc_out,      # keys\n","            enc_out,      # values\n","            mask\n","        )  # (B, hid)\n","\n","        # GRU 입력 구성\n","        # emb:     (B, 1, emb)\n","        # context: (B, hid) -> (B, 1, hid)\n","        gru_in = torch.cat(\n","            [emb, context.unsqueeze(1)],\n","            dim=2\n","        )  # -> (B, 1, emb + hid)\n","\n","        # GRU 한 스텝 실행\n","        out, hidden = self.gru(gru_in, hidden)\n","        # out:    (B, 1, hid)\n","        # hidden: (1, B, hid)\n","\n","        # 단어 예측\n","        # out.squeeze(1): (B, hid)\n","        # context:        (B, hid)\n","        logits = self.fc(\n","            torch.cat([out.squeeze(1), context], dim=1)\n","        )  # -> (B, vocab)\n","\n","        return logits, hidden\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MRc3gAJLECdt"},"outputs":[],"source":["\"\"\"\n","Encoder + Attention Decoder를 결합한 전체 번역 모델\n","\"\"\"\n","class Seq2SeqWithAttention(nn.Module):\n","    def __init__(self, enc, dec, bos_id, pad_id_src, device=cfg.device):\n","        super().__init__()\n","        self.enc = enc\n","        self.dec = dec\n","        self.bos_id = bos_id\n","        self.pad_id_src = pad_id_src\n","        self.device = device\n","\n","    def forward(self, src, tgt, teacher_forcing: float=cfg.teacher_forcing):\n","        \"\"\"\n","        args:\n","            - src: shape = (B, T_src)\n","            - tgt: shape = (B, T_tgt)\n","            - teacher_forcing: 다음 입력으로 정답을 쓸 확률\n","        \"\"\"\n","\n","        # 기본 정보\n","        B, T = tgt.size()\n","        vocab = self.dec.fc.out_features\n","\n","        # 전체 타임스텝 출력 저장용\n","        logits_all = torch.zeros(\n","            B, T, vocab,\n","            device=self.device\n","        )\n","\n","        # Encoder 실행\n","        enc_out, hidden = self.enc(src)\n","        # enc_out: (B, T_src, hid)\n","        # hidden:  (1, B, hid)\n","\n","        # PAD 마스크 생성\n","        mask = (src != self.pad_id_src)\n","\n","        # 디코더 시작 입력: <BOS>\n","        x = tgt[:, 0]\n","\n","        # 디코딩 루프\n","        for t in range(1, T):\n","            logits, hidden = self.dec(\n","                x,\n","                hidden,\n","                enc_out,\n","                mask\n","            )\n","\n","            logits_all[:, t] = logits\n","\n","            # teacher forcing 여부 결정\n","            use_tf = torch.rand(1).item() < teacher_forcing\n","\n","            # 다음 입력 선택\n","            x = tgt[:, t] if use_tf else logits.argmax(dim=1)\n","\n","        return logits_all\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W9Wn336wECdt"},"outputs":[],"source":["# 모델 생성\n","enc_attn = Encoder(\n","    vocab=ko_tok.tokenizer.get_piece_size(),\n","    emb=cfg.emb_dim,\n","    hid=cfg.hid_dim,\n","    pad_id=ko_tok.pad_id,\n",")\n","\n","dec_attn = AttnDecoder(\n","    vocab=en_tok.tokenizer.get_piece_size(),\n","    emb=cfg.emb_dim,\n","    hid=cfg.hid_dim,\n","    pad_id=en_tok.pad_id,\n",")\n","\n","model_attn = Seq2SeqWithAttention(\n","    enc_attn,\n","    dec_attn,\n","    bos_id=en_tok.bos_id,\n","    pad_id_src=ko_tok.pad_id,\n",").to(cfg.device)"]},{"cell_type":"markdown","metadata":{"id":"fJPK4V8VECdz"},"source":["# 6. 훈련"]},{"cell_type":"markdown","metadata":{"id":"qYsIfkIiECdz"},"source":["## 6.1. 유틸"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L_0GObFEECdz"},"outputs":[],"source":["# 조기 종료 클래스\n","class EarlyStopping:\n","    def __init__(self, patience: int=cfg.patience, min_delta: float = cfg.min_delta, save_path: str | Path = cfg.model_dir / \"best_model.pt\"):\n","        \"\"\"\n","        Args:\n","            patience (int): 개선이 없을 때 허용 epoch 수\n","            min_delta (float): 최소 개선 폭\n","            save_path (str | Path): best model 저장 경로\n","        \"\"\"\n","        self.patience = patience\n","        self.min_delta = min_delta\n","        self.best_loss = float(\"inf\")\n","        self.counter = 0\n","        self.save_path = Path(save_path)\n","\n","    def step(self, val_loss: float, model: torch.nn.Module) -> bool:\n","        \"\"\"\n","        Args:\n","            val_loss (float): 현재 epoch의 validation loss\n","            model (nn.Module): 현재 모델\n","\n","        Returns:\n","            bool: True면 학습 중단, False면 계속\n","        \"\"\"\n","        if val_loss < self.best_loss - self.min_delta:\n","            # 성능 개선\n","            self.best_loss = val_loss\n","            self.counter = 0\n","\n","            # best model 저장\n","            torch.save(\n","                {\"model_state\": model.state_dict()},\n","                self.save_path\n","            )\n","            print(f\"Validation loss improved. Best model saved to {self.save_path}\")\n","\n","            return False\n","        else:\n","            self.counter += 1\n","            print(f\"EarlyStopping counter: {self.counter} / {self.patience}\")\n","\n","            return self.counter >= self.patience"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8FnnySXfECd0"},"outputs":[],"source":["# 훈련 함수\n","def my_trainer(model, loader, optim, loss_fn, grad_clip: float=cfg.grad_clip, device=cfg.device):\n","    model.train()\n","    total = 0\n","\n","    for src, tgt in loader:\n","        src, tgt = src.to(device), tgt.to(device)\n","        optim.zero_grad()\n","\n","        # logits shape: (B, T, V)\n","        logits = model(src, tgt)\n","\n","        # CrossEntropyLoss를 위한 입력으로 input : (N, C), target: (N,)로 변환\n","        loss = loss_fn(\n","            logits[:, 1:].reshape(-1, logits.size(-1)),\n","            tgt[:, 1:].reshape(-1),\n","        )\n","\n","        loss.backward()\n","\n","        # gradient clipping (폭주 방지)\n","        nn.utils.clip_grad_norm_(\n","            model.parameters(),\n","            max_norm=grad_clip\n","        )\n","\n","        optim.step()\n","        total += loss.item()\n","\n","    return total / len(loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9oxMvqu1ECd0"},"outputs":[],"source":["# 검증 함수\n","def my_evaluate(model, loader, loss_fn, device=cfg.device):\n","    model.eval()\n","    total = 0\n","\n","    with torch.no_grad():\n","        for src, tgt in loader:\n","            src, tgt = src.to(device), tgt.to(device)\n","\n","            # logits shape: (B, T, V)\n","            logits = model(src, tgt)\n","\n","            # CrossEntropyLoss를 위한 입력으로 input : (N, C), target: (N,)로 변환\n","            loss = loss_fn(\n","                logits[:, 1:].reshape(-1, logits.size(-1)),\n","                tgt[:, 1:].reshape(-1),\n","            )\n","\n","            total += loss.item()\n","\n","    return total / len(loader)"]},{"cell_type":"markdown","metadata":{"id":"ZEXQ_spJECd0"},"source":["## 6.2. 훈련 루프 실행"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NMEbXFlSECd0"},"outputs":[],"source":["# 손실함수, 최적화 알고리즘\n","optimizer_s2s = torch.optim.Adam(model_s2s.parameters(), lr=cfg.lr)\n","optimizer_attn = torch.optim.Adam(model_attn.parameters(), lr=cfg.lr)\n","loss_fn = nn.CrossEntropyLoss(ignore_index=en_tok.pad_id)\n","\n","# 조기종료 인스턴스\n","s2s_early_stopping = EarlyStopping(save_path=cfg.model_dir / \"s2s_model.pt\")\n","attn_early_stopping = EarlyStopping(save_path=cfg.model_dir / \"attn_model.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vUcN8NI5ECd1","outputId":"112acf12-3c26-4631-902e-3373cdf82c3b"},"outputs":[{"name":"stderr","output_type":"stream","text":["processing:  10%|█         | 1/10 [29:12<4:22:52, 1752.52s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/s2s_model.pt\n","[Epoch 1] train loss = 4.2094\n","[Epoch 1] val loss = 3.7189\n"]},{"name":"stderr","output_type":"stream","text":["processing:  20%|██        | 2/10 [48:15<3:05:51, 1393.90s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/s2s_model.pt\n","[Epoch 2] train loss = 3.5309\n","[Epoch 2] val loss = 3.4134\n"]},{"name":"stderr","output_type":"stream","text":["processing:  30%|███       | 3/10 [1:07:54<2:31:11, 1295.87s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/s2s_model.pt\n","[Epoch 3] train loss = 3.2980\n","[Epoch 3] val loss = 3.2560\n"]},{"name":"stderr","output_type":"stream","text":["processing:  40%|████      | 4/10 [1:26:53<2:03:23, 1233.96s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/s2s_model.pt\n","[Epoch 4] train loss = 3.1645\n","[Epoch 4] val loss = 3.1649\n"]},{"name":"stderr","output_type":"stream","text":["processing:  50%|█████     | 5/10 [1:45:29<1:39:17, 1191.52s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/s2s_model.pt\n","[Epoch 5] train loss = 3.0778\n","[Epoch 5] val loss = 3.1094\n"]},{"name":"stderr","output_type":"stream","text":["processing:  60%|██████    | 6/10 [2:03:57<1:17:32, 1163.01s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/s2s_model.pt\n","[Epoch 6] train loss = 3.0120\n","[Epoch 6] val loss = 3.0571\n"]},{"name":"stderr","output_type":"stream","text":["processing:  70%|███████   | 7/10 [2:22:30<57:20, 1146.78s/it]  "]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/s2s_model.pt\n","[Epoch 7] train loss = 2.9615\n","[Epoch 7] val loss = 3.0211\n"]},{"name":"stderr","output_type":"stream","text":["processing:  80%|████████  | 8/10 [2:57:09<48:06, 1443.25s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/s2s_model.pt\n","[Epoch 8] train loss = 2.9196\n","[Epoch 8] val loss = 2.9898\n"]},{"name":"stderr","output_type":"stream","text":["processing:  90%|█████████ | 9/10 [3:54:34<34:29, 2069.33s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/s2s_model.pt\n","[Epoch 9] train loss = 2.8840\n","[Epoch 9] val loss = 2.9717\n"]},{"name":"stderr","output_type":"stream","text":["processing: 100%|██████████| 10/10 [4:20:12<00:00, 1561.22s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/s2s_model.pt\n","[Epoch 10] train loss = 2.8528\n","[Epoch 10] val loss = 2.9487\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# s2s 모델 훈련\n","for epoch in tqdm(range(1, cfg.epochs+1), desc=\"processing\"):\n","    train_loss = my_trainer(\n","        model_s2s,\n","        train_loader,\n","        optimizer_s2s,\n","        loss_fn,\n","    )\n","\n","    val_loss = my_evaluate(\n","        model_s2s,\n","        val_loader,\n","        loss_fn\n","    )\n","\n","    s2s_early_stopping.step(\n","        val_loss,\n","        model_s2s\n","        )\n","\n","    print(f\"[Epoch {epoch}] train loss = {train_loss:.4f}\")\n","    print(f\"[Epoch {epoch}] val loss = {val_loss:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pZ02ddRWECd1","outputId":"f0dae731-696a-4d4c-e428-529e25dc107c"},"outputs":[{"name":"stderr","output_type":"stream","text":["processing:  20%|██        | 1/5 [56:29<3:45:59, 3389.98s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/s2s_model.pt\n","[Epoch 1] train loss = 2.8291\n","[Epoch 1] val loss = 2.9377\n"]},{"name":"stderr","output_type":"stream","text":["processing:  40%|████      | 2/5 [1:29:23<2:07:49, 2556.50s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/s2s_model.pt\n","[Epoch 2] train loss = 2.8044\n","[Epoch 2] val loss = 2.9132\n"]},{"name":"stderr","output_type":"stream","text":["processing:  60%|██████    | 3/5 [2:13:08<1:26:16, 2588.07s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/s2s_model.pt\n","[Epoch 3] train loss = 2.7856\n","[Epoch 3] val loss = 2.8991\n"]},{"name":"stderr","output_type":"stream","text":["processing:  80%|████████  | 4/5 [3:04:49<46:30, 2790.42s/it]  "]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/s2s_model.pt\n","[Epoch 4] train loss = 2.7671\n","[Epoch 4] val loss = 2.8862\n"]},{"name":"stderr","output_type":"stream","text":["processing: 100%|██████████| 5/5 [4:00:01<00:00, 2880.36s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/s2s_model.pt\n","[Epoch 5] train loss = 2.7520\n","[Epoch 5] val loss = 2.8748\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# s2s 모델 추가 훈련 (5 에포크)\n","# 첫 10 에포크에서 조기종료가 되지 않았다는 점에서 개선 여지가 있고\n","# BLEU 점수가 아직 충분하지 않음\n","for epoch in tqdm(range(1, 6), desc=\"processing\"):\n","    train_loss = my_trainer(\n","        model_s2s,\n","        train_loader,\n","        optimizer_s2s,\n","        loss_fn,\n","    )\n","\n","    val_loss = my_evaluate(\n","        model_s2s,\n","        val_loader,\n","        loss_fn\n","    )\n","\n","    s2s_early_stopping.step(\n","        val_loss,\n","        model_s2s\n","        )\n","\n","    print(f\"[Epoch {epoch}] train loss = {train_loss:.4f}\")\n","    print(f\"[Epoch {epoch}] val loss = {val_loss:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cSVQC3VhECd1","outputId":"eabfc04f-6d85-431b-9a02-76b317e93d57"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["ckpt_s2s = torch.load(cfg.model_dir / \"s2s_model.pt\", map_location=cfg.device)\n","model_s2s.load_state_dict(ckpt_s2s[\"model_state\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LkM0EQiaECd2","outputId":"0e7fe785-4185-4733-fd63-0d7381694f59"},"outputs":[{"name":"stderr","output_type":"stream","text":["processing:  10%|█         | 1/10 [38:40<5:48:02, 2320.28s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/attn_model.pt\n","[Epoch 1] train loss = 3.5154\n","[Epoch 1] val loss = 3.0478\n"]},{"name":"stderr","output_type":"stream","text":["processing:  20%|██        | 2/10 [1:54:38<8:04:55, 3637.00s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/attn_model.pt\n","[Epoch 2] train loss = 2.9020\n","[Epoch 2] val loss = 2.8666\n"]},{"name":"stderr","output_type":"stream","text":["processing:  30%|███       | 3/10 [3:12:18<7:58:47, 4103.90s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/attn_model.pt\n","[Epoch 3] train loss = 2.7373\n","[Epoch 3] val loss = 2.7896\n"]},{"name":"stderr","output_type":"stream","text":["processing:  40%|████      | 4/10 [4:14:04<6:34:41, 3946.87s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/attn_model.pt\n","[Epoch 4] train loss = 2.6418\n","[Epoch 4] val loss = 2.7319\n"]},{"name":"stderr","output_type":"stream","text":["processing:  50%|█████     | 5/10 [4:58:37<4:50:36, 3487.32s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/attn_model.pt\n","[Epoch 5] train loss = 2.5773\n","[Epoch 5] val loss = 2.7040\n"]},{"name":"stderr","output_type":"stream","text":["processing:  60%|██████    | 6/10 [6:14:24<4:16:30, 3847.67s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/attn_model.pt\n","[Epoch 6] train loss = 2.5284\n","[Epoch 6] val loss = 2.6837\n"]},{"name":"stderr","output_type":"stream","text":["processing:  70%|███████   | 7/10 [7:31:16<3:24:52, 4097.55s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/attn_model.pt\n","[Epoch 7] train loss = 2.4911\n","[Epoch 7] val loss = 2.6586\n"]},{"name":"stderr","output_type":"stream","text":["processing:  80%|████████  | 8/10 [8:48:29<2:22:16, 4268.04s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/attn_model.pt\n","[Epoch 8] train loss = 2.4568\n","[Epoch 8] val loss = 2.6480\n"]},{"name":"stderr","output_type":"stream","text":["processing:  90%|█████████ | 9/10 [10:05:42<1:13:02, 4382.13s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/attn_model.pt\n","[Epoch 9] train loss = 2.4326\n","[Epoch 9] val loss = 2.6306\n"]},{"name":"stderr","output_type":"stream","text":["processing: 100%|██████████| 10/10 [11:23:28<00:00, 4100.81s/it] "]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/attn_model.pt\n","[Epoch 10] train loss = 2.4110\n","[Epoch 10] val loss = 2.6170\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# attn s2s 모델 훈련 (5 에포크)\n","for epoch in tqdm(range(1, cfg.epochs+1), desc=\"processing\"):\n","    train_loss = my_trainer(\n","        model_attn,\n","        train_loader,\n","        optimizer_attn,\n","        loss_fn,\n","    )\n","\n","    val_loss = my_evaluate(\n","        model_attn,\n","        val_loader,\n","        loss_fn\n","    )\n","\n","    attn_early_stopping.step(\n","    val_loss,\n","    model_attn\n","    )\n","\n","    print(f\"[Epoch {epoch}] train loss = {train_loss:.4f}\")\n","    print(f\"[Epoch {epoch}] val loss = {val_loss:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X2XRAavbECd3","outputId":"f1efcdb4-a6de-485d-92f5-728c9965ec55"},"outputs":[{"name":"stderr","output_type":"stream","text":["processing:  20%|██        | 1/5 [41:32<2:46:10, 2492.66s/it]"]},{"name":"stdout","output_type":"stream","text":["EarlyStopping counter: 1 / 3\n","[Epoch 1] train loss = 2.3223\n","[Epoch 1] val loss = 2.5823\n"]},{"name":"stderr","output_type":"stream","text":["processing:  40%|████      | 2/5 [1:46:07<2:45:17, 3305.85s/it]"]},{"name":"stdout","output_type":"stream","text":["EarlyStopping counter: 2 / 3\n","[Epoch 2] train loss = 2.3112\n","[Epoch 2] val loss = 2.5885\n"]},{"name":"stderr","output_type":"stream","text":["processing:  60%|██████    | 3/5 [2:57:53<2:05:24, 3762.39s/it]"]},{"name":"stdout","output_type":"stream","text":["EarlyStopping counter: 3 / 3\n","[Epoch 3] train loss = 2.3028\n","[Epoch 3] val loss = 2.5753\n"]},{"name":"stderr","output_type":"stream","text":["processing:  80%|████████  | 4/5 [3:35:59<52:59, 3179.51s/it]  "]},{"name":"stdout","output_type":"stream","text":["EarlyStopping counter: 4 / 3\n","[Epoch 4] train loss = 2.2910\n","[Epoch 4] val loss = 2.5769\n"]},{"name":"stderr","output_type":"stream","text":["processing: 100%|██████████| 5/5 [4:22:49<00:00, 3153.96s/it]"]},{"name":"stdout","output_type":"stream","text":["Validation loss improved. Best model saved to models/attn_model.pt\n","[Epoch 5] train loss = 2.2827\n","[Epoch 5] val loss = 2.5703\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# attn s2s 모델 추가 훈련\n","# 첫 10 에포크에서 조기종료가 되지 않았다는 점에서 개선 여지가 있고\n","# BLEU 점수가 아직 충분하지 않음\n","for epoch in tqdm(range(1, 6), desc=\"processing\"):\n","    train_loss = my_trainer(\n","        model_attn,\n","        train_loader,\n","        optimizer_attn,\n","        loss_fn,\n","    )\n","\n","    val_loss = my_evaluate(\n","        model_attn,\n","        val_loader,\n","        loss_fn\n","    )\n","\n","    attn_early_stopping.step(\n","    val_loss,\n","    model_attn\n","    )\n","\n","    print(f\"[Epoch {epoch}] train loss = {train_loss:.4f}\")\n","    print(f\"[Epoch {epoch}] val loss = {val_loss:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fldBnRsXECd4","outputId":"c3baf56b-bde1-4389-981e-8992f9a9c9cf"},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["ckpt_attn = torch.load(cfg.model_dir / \"attn_model.pt\", map_location=cfg.device)\n","model_attn.load_state_dict(ckpt_attn[\"model_state\"])"]},{"cell_type":"markdown","metadata":{"id":"W0tLtmZGECd4"},"source":["# 7. 추론 및 평가"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GOqSH_D4ECd4"},"outputs":[],"source":["# 추론 함수 (Greedy)\n","@torch.no_grad()\n","def translate(model, src_text, src_tok=ko_tok, tgt_tok=en_tok, device=cfg.device, max_len: int=cfg.max_len):\n","    \"\"\"\n","    추론 함수\n","\n","    args:\n","        - model   : 사용 모델\n","        - src_text: 소스 언어 텍스트\n","        - src_tok : 소스 언어 토크나이저\n","        - tgt_tok : 타겟 언어 토크나이저\n","        - device  : 디바이스\n","        - max_len : 최대 길이\n","\n","    returns:\n","        - translation: 타겟 언어 번역\n","    \"\"\"\n","    model.eval()\n","\n","    # 소스 문장 토큰화\n","    src_ids = src_tok.encode(src_text, max_len)\n","    src = torch.tensor(src_ids).unsqueeze(0).to(device)\n","\n","    # 인코더 출력 결과\n","    enc_out, hidden = model.enc(src)\n","\n","    # 어텐션용 마스크\n","    mask = (src != src_tok.pad_id)\n","\n","    # 디코더용 첫 번째 토큰\n","    x = torch.tensor([tgt_tok.bos_id], device=device)\n","    out = []\n","\n","    # 일반 seq2seq인가 attention seq2seq인가에 따라 분기\n","    for _ in range(max_len):\n","        if isinstance(model, Seq2SeqWithAttention):\n","            logits, hidden = model.dec(x, hidden, enc_out, mask)\n","        else:\n","            logits, hidden = model.dec(x, hidden)\n","\n","        x = logits.argmax(1)\n","        if x.item() == tgt_tok.eos_id:\n","            break\n","        out.append(x.item())\n","\n","    return tgt_tok.decode(out)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W1_wsAvoECd5","outputId":"2f952f6c-f3e0-4291-d15d-cbf69e442183"},"outputs":[{"name":"stdout","output_type":"stream","text":["KO: 날 좋아해?\n","S2S EN: I like to do it like this?\n","S2S with Attn EN: You like me?\n"]}],"source":["# 추론 테스트\n","src = \"날 좋아해?\"\n","\n","# seq2seq\n","pred_s = translate(\n","    model_s2s,\n","    src,\n",")\n","\n","# attn seq2seq\n","pred_a = translate(\n","    model_attn,\n","    src,\n",")\n","\n","print(\"KO:\", src)\n","print(\"S2S EN:\", pred_s)\n","print(\"S2S with Attn EN:\", pred_a)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dBbh0inmECd6","outputId":"9832593c-d33b-4182-be6e-b9f982ecf082"},"outputs":[{"name":"stdout","output_type":"stream","text":["KO: 오픈과 동시에 이벤트를 진행하려고 합니다.\n","S2S EN: We are planning to open a event and opening the same time.\n","S2S with Attn EN: We're going to hold an event at the same time.\n"]}],"source":["# 검증 데이터셋 샘플 번역\n","s_src = val_normalized[15][\"ko\"]\n","\n","# seq2seq\n","p_pred_s = translate(\n","    model_s2s,\n","    s_src,\n",")\n","\n","# attn seq2seq\n","p_pred_a = translate(\n","    model_attn,\n","    s_src,\n",")\n","\n","print(\"KO:\", s_src)\n","print(\"S2S EN:\", p_pred_s)\n","print(\"S2S with Attn EN:\", p_pred_a)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pwcX5moUECd6"},"outputs":[],"source":["def trans_with_refer(model, loader, src_tok=ko_tok, tgt_tok=en_tok, device=cfg.device):\n","    model.eval()\n","    hyps = []\n","    refs = []\n","\n","    with torch.no_grad():\n","        for src, tgt in loader:\n","            src = src.to(device)\n","\n","            for i in range(src.size(0)):\n","                # src 토큰 -> 문자열\n","                src_text = src_tok.decode(src[i].tolist())\n","\n","                # 모델 추론\n","                pred_text = translate(\n","                    model,\n","                    src_text,\n","                    src_tok=src_tok,\n","                    tgt_tok=tgt_tok,\n","                    device=device\n","                )\n","\n","                # 정답 텍스트\n","                ref_text = tgt_tok.decode(tgt[i].tolist())\n","\n","                hyps.append(pred_text)\n","                refs.append(ref_text)\n","\n","    return hyps, refs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SdoJcfx-ECd7","outputId":"6c549fc9-3123-4a96-8e43-425cdc99bad0"},"outputs":[{"name":"stdout","output_type":"stream","text":["BLEU score (s2s): 16.87\n"]}],"source":["# BLEU 평가(s2s)\n","hyps, refs = trans_with_refer(\n","    model_s2s,\n","    val_loader,\n",")\n","\n","s2s_bleu = sacrebleu.corpus_bleu(hyps, [refs])\n","\n","print(f\"BLEU score (s2s): {s2s_bleu.score:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v133-ysIECd7","outputId":"7bf2fd41-681d-4cd5-ce90-69d49e5c063d"},"outputs":[{"name":"stdout","output_type":"stream","text":["BLEU score (s2s with attn): 21.81\n"]}],"source":["# BLEU 평가(s2s attn)\n","h_hyps, r_refs = trans_with_refer(\n","    model_attn,\n","    val_loader,\n",")\n","\n","attn_bleu = sacrebleu.corpus_bleu(h_hyps, [r_refs])\n","\n","print(f\"BLEU score (s2s with attn): {attn_bleu.score:.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"2zKWmHAhECd8"},"source":["## 7.1 정량적 평가"]},{"cell_type":"markdown","metadata":{"id":"9eDsv8W0ECd8"},"source":["### BLEU 평가\n","- Seq2Seq (10 에포크): 14.01\n","- Seq2Seq (15 에포크): 16.87\n","- Seq2Seq with Attention (10 에포크): 19.38\n","- Seq2Seq with Attention (15 에포크): 21.81"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":".venv (3.12.3)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}