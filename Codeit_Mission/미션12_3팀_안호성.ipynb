{"cells":[{"cell_type":"markdown","metadata":{"id":"9bcZc42Acap0"},"source":["# 문서 요약"]},{"cell_type":"markdown","metadata":{"id":"p5JWyAnQdj2b"},"source":["## 개요\n","\n","- 사용 데이터: 신문 기사, 사설, 법률\n","\n","- 설계\n","| 태스크 | 기술 |\n","|----------------|-----------------------------|\n","| 데이터 전처리 | 데이터 로드, 전처리 |\n","| 사용 모델 | KoBART, Qwen3-4b  |\n","| 훈련 | 정식 Fine-Tuning, QLora Fine-Tuning |"]},{"cell_type":"markdown","metadata":{"id":"Ho_-cURBi2An"},"source":["# 1. 데이터 전처리"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mZJ2FnTeHlpL","outputId":"b6d3c55e-bddf-409b-e135-6511bbf6728c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: datasets in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (4.4.2)\n","Requirement already satisfied: filelock in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets) (3.20.1)\n","Requirement already satisfied: numpy>=1.17 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=21.0.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets) (22.0.0)\n","Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets) (0.4.0)\n","Requirement already satisfied: pandas in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets) (2.3.3)\n","Requirement already satisfied: requests>=2.32.2 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets) (2.32.5)\n","Requirement already satisfied: httpx<1.0.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets) (0.28.1)\n","Requirement already satisfied: tqdm>=4.66.3 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets) (3.6.0)\n","Requirement already satisfied: multiprocess<0.70.19 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets) (0.70.18)\n","Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n","Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets) (0.36.0)\n","Requirement already satisfied: packaging in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets) (6.0.3)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n","Requirement already satisfied: anyio in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.12.0)\n","Requirement already satisfied: certifi in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2025.11.12)\n","Requirement already satisfied: httpcore==1.* in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n","Requirement already satisfied: idna in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n","Requirement already satisfied: h11>=0.16 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.6.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.3)\n","Requirement already satisfied: six>=1.5 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n","Collecting JPype1>=0.7.0 (from konlpy)\n","  Downloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: lxml>=4.1.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from konlpy) (6.0.2)\n","Requirement already satisfied: numpy>=1.6 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from konlpy) (1.26.4)\n","Requirement already satisfied: packaging in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from JPype1>=0.7.0->konlpy) (25.0)\n","Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (495 kB)\n","Installing collected packages: JPype1, konlpy\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [konlpy]\n","\u001b[1A\u001b[2KSuccessfully installed JPype1-1.6.0 konlpy-0.6.0\n"]}],"source":["!pip install datasets\n","!pip install konlpy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TEP7ZqpiHlpP","outputId":"9089929f-3d0b-4380-812a-d0201ec47cd7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found existing installation: transformers 4.48.3\n","Uninstalling transformers-4.48.3:\n","  Successfully uninstalled transformers-4.48.3\n","Found existing installation: tokenizers 0.21.0\n","Uninstalling tokenizers-0.21.0:\n","  Successfully uninstalled tokenizers-0.21.0\n","Found existing installation: accelerate 0.34.2\n","Uninstalling accelerate-0.34.2:\n","  Successfully uninstalled accelerate-0.34.2\n","Found existing installation: peft 0.13.2\n","Uninstalling peft-0.13.2:\n","  Successfully uninstalled peft-0.13.2\n","Found existing installation: datasets 2.21.0\n","Uninstalling datasets-2.21.0:\n","  Successfully uninstalled datasets-2.21.0\n","Found existing installation: safetensors 0.4.5\n","Uninstalling safetensors-0.4.5:\n","  Successfully uninstalled safetensors-0.4.5\n"]}],"source":["!pip uninstall -y transformers tokenizers accelerate peft datasets safetensors"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tkEXe4tTHlpQ","outputId":"9c885c82-81b8-4e54-a7dc-a17db32bb2e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers==4.48.3\n","  Using cached transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n","Collecting tokenizers==0.21.0\n","  Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Collecting accelerate==0.34.2\n","  Using cached accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\n","Collecting peft==0.13.2\n","  Using cached peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n","Collecting datasets==2.21.0\n","  Using cached datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n","Collecting safetensors==0.4.5\n","  Using cached safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n","Requirement already satisfied: filelock in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from transformers==4.48.3) (3.20.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from transformers==4.48.3) (0.36.0)\n","Requirement already satisfied: numpy>=1.17 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from transformers==4.48.3) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from transformers==4.48.3) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from transformers==4.48.3) (6.0.3)\n","Requirement already satisfied: regex!=2019.12.17 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from transformers==4.48.3) (2025.11.3)\n","Requirement already satisfied: requests in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from transformers==4.48.3) (2.32.5)\n","Requirement already satisfied: tqdm>=4.27 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from transformers==4.48.3) (4.67.1)\n","Requirement already satisfied: psutil in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from accelerate==0.34.2) (7.1.3)\n","Requirement already satisfied: torch>=1.10.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from accelerate==0.34.2) (2.11.0.dev20251215+cu128)\n","Requirement already satisfied: pyarrow>=15.0.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets==2.21.0) (22.0.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets==2.21.0) (0.3.8)\n","Requirement already satisfied: pandas in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets==2.21.0) (2.3.3)\n","Requirement already satisfied: xxhash in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets==2.21.0) (3.6.0)\n","Requirement already satisfied: multiprocess in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets==2.21.0) (0.70.16)\n","Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets==2.21.0) (2024.6.1)\n","Requirement already satisfied: aiohttp in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets==2.21.0) (3.13.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (1.2.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from aiohttp->datasets==2.21.0) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from aiohttp->datasets==2.21.0) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from aiohttp->datasets==2.21.0) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from aiohttp->datasets==2.21.0) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from aiohttp->datasets==2.21.0) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from aiohttp->datasets==2.21.0) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from aiohttp->datasets==2.21.0) (1.22.0)\n","Requirement already satisfied: idna>=2.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.21.0) (3.11)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from requests->transformers==4.48.3) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from requests->transformers==4.48.3) (2.6.2)\n","Requirement already satisfied: certifi>=2017.4.17 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from requests->transformers==4.48.3) (2025.11.12)\n","Requirement already satisfied: setuptools in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.34.2) (78.1.0)\n","Requirement already satisfied: sympy>=1.13.3 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.34.2) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.34.2) (3.6.1)\n","Requirement already satisfied: jinja2 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.34.2) (3.1.6)\n","Requirement already satisfied: cuda-bindings==12.9.4 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.34.2) (12.9.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.34.2) (12.8.93)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.34.2) (12.8.90)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.34.2) (12.8.90)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.34.2) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.34.2) (12.8.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.34.2) (11.3.3.83)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.34.2) (10.3.9.90)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.34.2) (11.7.3.90)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.34.2) (12.5.8.93)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.34.2) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.28.9 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.34.2) (2.28.9)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.34.2) (3.4.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.34.2) (12.8.90)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.34.2) (12.8.93)\n","Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.34.2) (1.13.1.3)\n","Requirement already satisfied: triton==3.6.0+git8fedd49b in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch>=1.10.0->accelerate==0.34.2) (3.6.0+git8fedd49b)\n","Requirement already satisfied: cuda-pathfinder~=1.1 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from cuda-bindings==12.9.4->torch>=1.10.0->accelerate==0.34.2) (1.2.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.10.0->accelerate==0.34.2) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from jinja2->torch>=1.10.0->accelerate==0.34.2) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from pandas->datasets==2.21.0) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from pandas->datasets==2.21.0) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from pandas->datasets==2.21.0) (2025.3)\n","Requirement already satisfied: six>=1.5 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.21.0) (1.17.0)\n","Using cached transformers-4.48.3-py3-none-any.whl (9.7 MB)\n","Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n","Using cached accelerate-0.34.2-py3-none-any.whl (324 kB)\n","Using cached peft-0.13.2-py3-none-any.whl (320 kB)\n","Using cached datasets-2.21.0-py3-none-any.whl (527 kB)\n","Using cached safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\n","Installing collected packages: safetensors, tokenizers, transformers, datasets, accelerate, peft\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [peft][32m5/6\u001b[0m [peft]erate]s]\n","\u001b[1A\u001b[2KSuccessfully installed accelerate-0.34.2 datasets-2.21.0 peft-0.13.2 safetensors-0.4.5 tokenizers-0.21.0 transformers-4.48.3\n"]}],"source":["!pip install -U \\\n","  \"transformers==4.48.3\" \\\n","  \"tokenizers==0.21.0\" \\\n","  \"accelerate==0.34.2\" \\\n","  \"peft==0.13.2\" \\\n","  \"datasets==2.21.0\" \\\n","  \"safetensors==0.4.5\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8EJsszE6HlpR","outputId":"ce749163-6ecd-4e08-e867-8c35c9e26b6c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting bitsandbytes\n","  Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n","Collecting trl\n","  Downloading trl-0.26.2-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: torch<3,>=2.3 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from bitsandbytes) (2.11.0.dev20251215+cu128)\n","Requirement already satisfied: numpy>=1.17 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from bitsandbytes) (1.26.4)\n","Requirement already satisfied: packaging>=20.9 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from bitsandbytes) (25.0)\n","Requirement already satisfied: filelock in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (3.20.1)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n","Requirement already satisfied: setuptools in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (78.1.0)\n","Requirement already satisfied: sympy>=1.13.3 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (3.6.1)\n","Requirement already satisfied: jinja2 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (2024.6.1)\n","Requirement already satisfied: cuda-bindings==12.9.4 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.9.4)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (11.3.3.83)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (10.3.9.90)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (11.7.3.90)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.5.8.93)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.28.9 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (2.28.9)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (3.4.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.90)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (12.8.93)\n","Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (1.13.1.3)\n","Requirement already satisfied: triton==3.6.0+git8fedd49b in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from torch<3,>=2.3->bitsandbytes) (3.6.0+git8fedd49b)\n","Requirement already satisfied: cuda-pathfinder~=1.1 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from cuda-bindings==12.9.4->torch<3,>=2.3->bitsandbytes) (1.2.2)\n","Collecting accelerate>=1.4.0 (from trl)\n","  Using cached accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n","Collecting datasets>=3.0.0 (from trl)\n","  Using cached datasets-4.4.2-py3-none-any.whl.metadata (19 kB)\n","Collecting transformers>=4.56.1 (from trl)\n","  Downloading transformers-4.57.5-py3-none-any.whl.metadata (43 kB)\n","Requirement already satisfied: psutil in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (7.1.3)\n","Requirement already satisfied: pyyaml in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (6.0.3)\n","Requirement already satisfied: huggingface_hub>=0.21.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (0.36.0)\n","Requirement already satisfied: safetensors>=0.4.3 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from accelerate>=1.4.0->trl) (0.4.5)\n","Requirement already satisfied: pyarrow>=21.0.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (22.0.0)\n","Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (0.3.8)\n","Requirement already satisfied: pandas in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (2.3.3)\n","Requirement already satisfied: requests>=2.32.2 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (2.32.5)\n","Requirement already satisfied: httpx<1.0.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (0.28.1)\n","Requirement already satisfied: tqdm>=4.66.3 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (4.67.1)\n","Requirement already satisfied: xxhash in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (3.6.0)\n","Requirement already satisfied: multiprocess<0.70.19 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from datasets>=3.0.0->trl) (0.70.16)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (3.13.2)\n","Requirement already satisfied: anyio in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (4.12.0)\n","Requirement already satisfied: certifi in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (2025.11.12)\n","Requirement already satisfied: httpcore==1.* in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (1.0.9)\n","Requirement already satisfied: idna in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=3.0.0->trl) (3.11)\n","Requirement already satisfied: h11>=0.16 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=3.0.0->trl) (0.16.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.2.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (25.4.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.8.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (6.7.0)\n","Requirement already satisfied: propcache>=0.2.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (0.4.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=3.0.0->trl) (1.22.0)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2.6.2)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n","Requirement already satisfied: regex!=2019.12.17 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from transformers>=4.56.1->trl) (2025.11.3)\n","Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.56.1->trl)\n","  Downloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from pandas->datasets>=3.0.0->trl) (2025.3)\n","Requirement already satisfied: six>=1.5 in /home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl) (1.17.0)\n","Downloading bitsandbytes-0.49.1-py3-none-manylinux_2_24_x86_64.whl (59.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading trl-0.26.2-py3-none-any.whl (518 kB)\n","Using cached accelerate-1.12.0-py3-none-any.whl (380 kB)\n","Using cached datasets-4.4.2-py3-none-any.whl (512 kB)\n","Downloading transformers-4.57.5-py3-none-any.whl (12.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: tokenizers, transformers, datasets, bitsandbytes, accelerate, trl\n","\u001b[2K  Attempting uninstall: tokenizers\n","\u001b[2K    Found existing installation: tokenizers 0.21.0\n","\u001b[2K    Uninstalling tokenizers-0.21.0:\n","\u001b[2K      Successfully uninstalled tokenizers-0.21.0\n","\u001b[2K  Attempting uninstall: transformers\n","\u001b[2K    Found existing installation: transformers 4.48.3\n","\u001b[2K    Uninstalling transformers-4.48.3:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [transformers]\n","\u001b[2K      Successfully uninstalled transformers-4.48.3━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [transformers]\n","\u001b[2K  Attempting uninstall: datasetsm━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [transformers]\n","\u001b[2K    Found existing installation: datasets 2.21.0━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [transformers]\n","\u001b[2K    Uninstalling datasets-2.21.0:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [transformers]\n","\u001b[2K      Successfully uninstalled datasets-2.21.0━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/6\u001b[0m [transformers]\n","\u001b[2K  Attempting uninstall: accelerate90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [bitsandbytes]\n","\u001b[2K    Found existing installation: accelerate 0.34.2━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [bitsandbytes]\n","\u001b[2K    Uninstalling accelerate-0.34.2:[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [bitsandbytes]\n","\u001b[2K      Successfully uninstalled accelerate-0.34.2━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/6\u001b[0m [bitsandbytes]\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [trl]\u001b[32m5/6\u001b[0m [trl]lerate]\n","\u001b[1A\u001b[2KSuccessfully installed accelerate-1.12.0 bitsandbytes-0.49.1 datasets-4.4.2 tokenizers-0.22.2 transformers-4.57.5 trl-0.26.2\n"]}],"source":["!pip install -U bitsandbytes trl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fpt4gH1BHlpT"},"outputs":[],"source":["!pip -q install soxr"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XjRazwiZHlpU"},"outputs":[],"source":["!pip -q install evaluate rouge_score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UONlBt_AbFnB"},"outputs":[],"source":["# 파일 경로 및 파일 읽기 라이브러리\n","from pathlib import Path\n","from dataclasses import dataclass\n","import random\n","import zipfile\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import json\n","from typing import Dict, List, Any, Tuple, Optional\n","\n","# 파이토치 라이브러리\n","import torch\n","\n","# 허깅페이스 관련 라이브러리\n","from datasets import (\n","    load_dataset,\n","    Dataset,\n","    DatasetDict\n",")\n","from transformers import (\n","    BartTokenizerFast,\n","    DataCollatorForSeq2Seq,\n","    BartForConditionalGeneration,\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    DataCollatorForLanguageModeling,\n","    TrainingArguments,\n","    Trainer,\n","    BitsAndBytesConfig\n",")\n","from trl import SFTTrainer, SFTConfig\n","from peft import (\n","    LoraConfig,\n","    PeftModel\n","    )\n","import evaluate\n","\n","# 훈련 시 시각화\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Gx0MqW8QXZi"},"outputs":[],"source":["\"\"\"\n","Config 클래스 정의\n","\n","- 상수 변수 정의\n","- @dataclass 데코레이터 추가\n","\"\"\"\n","@dataclass\n","class Config:\n","    # 데이터 폴더 생성\n","    root = Path(\".\")\n","    raw_dir = root / \"data\"\n","    data_dir = raw_dir / \"summarization\"\n","    model_dir = root / \"models\"\n","\n","    # kobart 데이터셋용 변수 선언\n","    text_max_len = 512\n","    summary_max_len = 256\n","\n","    # 시스템 프롬프트\n","    sft_prompt = \"다음 문서를 핵심만 유지하며 간결한 한국어로 요약하시오.\"\n","\n","    # qwen 데이터셋용 길이 제한\n","    qwen_max_char = 2500\n","    qwen_max_sumary = 600\n","\n","    # 랜덤 추출용 변수\n","    seed = 42\n","\n","    # 학습용 변수 선언: 학습률, 에포크, 인내심, 최소 개선 폭\n","    lr = 5e-5\n","    epochs = 1\n","\n","    # 디바이스 설정\n","    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","\n","    # 후처리 함수 설정\n","    def __post_init__(self):\n","        self.raw_dir.mkdir(parents=True, exist_ok=True)\n","        self.data_dir.mkdir(parents=True, exist_ok=True)\n","        self.model_dir.mkdir(parents=True, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wf-eejLTbJAh"},"outputs":[],"source":["# 변수 생성 및 폴더 생성\n","cfg = Config()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VH4hq7i5Hlpa"},"outputs":[],"source":["\"\"\"\n","데이터 다운로드\n","\n","- 미션 페이지에서 데이터 다운로드\n","\"\"\"\n","with zipfile.ZipFile(f\"{str(cfg.raw_dir)}/summarization.zip\") as z:\n","    z.extractall(path=str(cfg.raw_dir))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pUlziIg5Hlpa"},"outputs":[],"source":["# JSON 파일 로드 함수\n","def load_json(json_path: Path) -> Dict:\n","    with open(str(json_path), \"r\", encoding=\"utf-8\") as j:\n","        data = json.load(j)\n","    return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDwjnXGeHlpb"},"outputs":[],"source":["# JSON 파일 저장 함수\n","def save_jsonl(samples: List[Dict[str, str]], out_path: Path) -> None:\n","    out_path.parent.mkdir(parents=True, exist_ok=True)\n","    with out_path.open(\"w\", encoding=\"utf-8\") as f:\n","        for s in samples:\n","            f.write(json.dumps(s, ensure_ascii=False) + \"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h1uiGwqtHlpb"},"outputs":[],"source":["# json 데이터 불러오기\n","raw_train_edit = load_json(cfg.data_dir / \"train_original_editorial.json\")\n","raw_train_law = load_json(cfg.data_dir / \"train_original_law.json\")\n","raw_train_news = load_json(cfg.data_dir / \"train_original_news.json\")\n","raw_val_edit = load_json(cfg.data_dir / \"valid_original_editorial.json\")\n","raw_val_law = load_json(cfg.data_dir / \"valid_original_law.json\")\n","raw_val_news = load_json(cfg.data_dir / \"valid_original_news.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uTHkQiSeHlpb","outputId":"f4c4b542-3635-47b4-e88c-6c75dc191b55"},"outputs":[{"name":"stdout","output_type":"stream","text":["원문: 이명박 대통령이 어제 30대 그룹 총수를 모아놓고 \"시대적 요구는 역시 총수가 앞장서야 한다. 이미 상당한 변화의 조짐이 있다는 것을 고맙게 생각한다. 총수들께서 직접 관심을 가져주시면 빨리 전파돼 긍정적인 평가를 받을 수 있다고 본다\"고 말했다. 언뜻 보아 무슨 말인지 불분명하나 이 대통령이 지난 8ㆍ15 연설 후 정몽준 의원, 정몽구 현대차 회장이 각각 2000억원과 5000억원을 기부한 사실과 '공생발전'이란 화두를 연결하면 금방 짐작이 간다. 다른 그룹 총수들도 좀 나서라고 은근히 떠민 것이다. 이 대통령은 기부에 대한 후속 선언이 나오지 않은 탓인지 총수들의 사회공헌 방안에 불만을 표시했다는 후문이다. 최근 미국 프랑스 벨기에 등에서 부유세가 거론되고 독일조차 2년간 한시적으로 5%의 자산세를 거둬 약 155조원을 마련하자는 논의가 있었다. 이런 흐름에 한국만 동떨어져 있기는 어려운 게 글로벌 시대의 특징이다. 항간에는 이번 회동 후 삼성을 비롯해 몇몇 그룹이 노블레스 오블리주 방안을 준비하고 있다는 말이 나도는데 대통령의 강요나 포퓰리즘에 의한 압박보다 자발적 문화로 만들어가야 효과가 큰 법이다. 그런 면에서 재계에 적절한 방안 마련을 맡기고 정치권이나 여론은 너무 압박하지 말고 시간을 줘야 한다. 국가채무 문제로 글로벌 경기 침체 우려가 큰 상황에서 기업들은 '생존'에 큰 부담을 느끼고 있기 때문이다. 이날 전경련에 따르면 30대 그룹은 올해 고용 12만4000명, 투자 114조원 등 '선물'을 준비했다. 세계적인 더블딥이 우려되는 상황에서 공격경영이 어렵겠지만 연초 한 번 발표한 내용을 약간 수정해 내놓은 전경련의 행태는 답답하다. 설립 50주년이 됐으면 좀 더 창의적이고 유연하게 바뀔 때도 됐다. 허창수 전경련 회장은 \"대기업ㆍ중소기업이 서로 공생하고 발전할 수 있도록 노력하겠다. 기업이 사회적 책임을 다하겠다\"는 원론적인 발언에 그쳐 전경련 특유의 무미건조함을 드러냈다. 한편 이건희 삼성전자 회장은 \"중소기업계 협력을 강화해 국제적으로 경쟁력 있는 기업 생태계를 만들어 나가겠다\"고 발언했고, 정몽구 회장은 \"이제 1차 협력업체는 경쟁력을 확보한 만큼 2ㆍ3차 협력업체 지원에 힘쓰겠다\"고 했는데 의미 있는 내용이라고 본다. 그대로 실천하면 동반성장 생태계는 한층 강화될 것이다.\n","요약본: 이명박 대통령은 어제 30대 그룹 총수를 모아놓고 시대적 요구는 역시 총수가 앞장서야 한다고 발언하며 기부문화 확산을 은근히 강조했으나 대통령의 강요나 포퓰리즘에 의한 압박보다는 자발적 문화로 구축해야 효과가 더 큰 법으로 방안 마련을 맡기고 시간을 줘야 할 것으로 보인다.\n","훈련 데이터 전체 길이: 325072\n","<class 'dict'>\n"]}],"source":["# 훈련 데이터 샘플 출력\n","print(f\"원문: {' '.join([c.get(\"sentence\") for d in raw_train_edit[\"documents\"][0][\"text\"] for c in d if c.get(\"sentence\")])}\")\n","print(f\"요약본: {' '.join(raw_train_edit[\"documents\"][0][\"abstractive\"])}\")\n","print(f\"훈련 데이터 전체 길이: {len(raw_train_edit[\"documents\"]) + len(raw_train_law[\"documents\"]) + len(raw_train_news[\"documents\"])}\")\n","print(type(raw_train_edit))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hu3W9RnFHlpc","outputId":"a2dd0c0c-f4cf-4826-d003-433b7cc2e751"},"outputs":[{"name":"stdout","output_type":"stream","text":["원문: 더불어민주당 이해찬 대표가 30 일 오후 국회에서 기자간담회를 열고 조국 전 법무부 장관 사태와 관련해 \"국민 여러분께 매우 송구하다\"고 밝혔다. 더불어민주당 이해찬 대표가 30 일 기자간담회를 열고 '조국 사태'와 관련, \"국민 여러분께 매우 송구하다\"는 입장을 밝혔다. 이 대표는 \"검찰 개혁이란 대의에 집중하다 보니, 국민 특히 청년이 느꼈을 불공정에 대한 상대적 박탈감, 좌절감을 깊이 있게 헤아리지 못했다\"며 \"여당 대표로서 무거운 책임감을 느낀다\"고 머리를 숙였다. 조국 전 법무부 장관이 14 일 사퇴한 이후 이 대표가 당 안팎의 쇄신 요구에 대해 입장을 표명한 것은 이번이 처음이다. 청와대와 여당은 '조국 정국'을 거치며 분출된 '공정'과 '정의'의 민심을 받들어 검찰 개혁에 매진하겠다면서도 두 달간 극심한 분열과 갈등을 초래한데 대해선 진지하게 성찰하는 모습을 보이지 않았다. 그나마 초선인 이철희 의원이 \"당이 대통령 뒤에 비겁하게 숨어 있었다\"고 비판했고, 표창원 의원은 \"책임을 느끼는 분들이 각자 형태로 그 책임감을 행동으로 옮겨야 할 때\"라고 지적했다. 뒤늦게나마 이 대표가 자성의 목소리를 내긴 했으나 당 안팎의 쇄신 요구에 어떻게 응할지 구체적 플랜을 제시하지 못해 여전히 안이하다는 지적도 나온다. 이 대표는 28 일 윤호중 사무총장을 단장으로 하는 총선기획단을 발족했고 조만간 인재영입위원회도 출범시킬 계획이라고 밝혔다. 이 대표는 \"민주당의 가치를 공유하는 참신한 인물을 영입해 준비된 정책과 인물로 승부하겠다\"고 다짐했다. 하지만 당 일각에선 \"총선기획단장을 비롯한 당직 인선부터 쇄신 의지를 보여야 한다\"는 비판의 목소리가 나온다. 무조건 물러나는 게 능사는 아니지만 국정 혼선을 초래한 데 대해 당 지도부가 겸허하게 책임지는 모습을 보이는 게 쇄신의 출발점이 돼야 한다는 지적도 있다. 선거는 대중의 이해와 요구를 잘 대표하는 정치인을 뽑는 행위다. 민생을 외면하며 낡은 이념과 진영 싸움에 매몰된 구시대 인물들을 과감히 물갈이하라는 게 국민의 요구다. 대신 4 차 산업혁명의 거센 파고를 헤쳐나갈 전문성을 갖춘 젊고 유능한 인재들을 널리 구해야 하다. 이해찬 대표의 이날 유감 표명이 여권 전반의 대대적인 인적 쇄신으로 이어지길 기대한다.\n","요약본: 이해찬 대표가 조국 사태와 관련 송구한 입장 표명이 과감한 인적 쇄신으로 이어져야 한다.\n","훈련 데이터 전체 길이: 40134\n","<class 'dict'>\n"]}],"source":["# 검증 데이터 샘플 출력\n","print(f\"원문: {' '.join([c.get(\"sentence\") for d in raw_val_edit[\"documents\"][0][\"text\"] for c in d if c.get(\"sentence\")])}\")\n","print(f\"요약본: {' '.join(raw_val_edit[\"documents\"][0][\"abstractive\"])}\")\n","print(f\"훈련 데이터 전체 길이: {len(raw_val_edit[\"documents\"]) + len(raw_val_law[\"documents\"]) + len(raw_val_news[\"documents\"])}\")\n","print(type(raw_val_edit))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3qQPOGNCHlpc"},"outputs":[],"source":["# 데이터 통합 함수 생성\n","def unifing_data(*json_data: Tuple) -> List[Dict[str, str]]:\n","    \"\"\"\n","    각 분할 파일을 하나로 정리\n","\n","    Arg:\n","        Tuple: json 데이터 튜플\n","\n","    Returns:\n","        unified: 각 원소는 {'text': str, 'summary': str}\n","    \"\"\"\n","    pairs_list = []\n","    for json_f in json_data:\n","        if isinstance(json_f, dict):\n","            for text in json_f[\"documents\"]:\n","                string_d = \" \".join([d.get(\"sentence\") for i in text[\"text\"] for d in i if d.get(\"sentence\")])\n","                summary_d = text[\"abstractive\"]\n","                summary_d = \" \".join(summary_d)\n","                pairs_list.append({\"text\": string_d, \"summary\": summary_d})\n","\n","        else:\n","            raise TypeError(f\"Unsupported JSON root type: {type(json_f)}\")\n","\n","\n","    # 검증\n","    normalized_pairs: List[Dict[str, str]] = []\n","    for item in pairs_list:\n","\n","        # 불량 데이터 거르기\n","        full_text = item.get(\"text\", None)\n","        summary_text = item.get(\"summary\", None)\n","\n","        if full_text is None or summary_text is None:\n","            continue\n","\n","        # 문자열이 아니면 강제로 변환\n","        normalized_pairs.append({\"text\": str(full_text), \"summary\": str(summary_text)})\n","\n","    return normalized_pairs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"syF217ZGHlpd"},"outputs":[],"source":["# 훈련 데이터 통합\n","train_normalized = unifing_data(raw_train_edit, raw_train_law, raw_train_news)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3Xw4PrUnHlpd","outputId":"c81ffc05-ddb6-4659-b95f-dd434ef69c44"},"outputs":[{"name":"stdout","output_type":"stream","text":["통합 훈련 데이터 샘플: {'text': '이명박 대통령이 어제 30대 그룹 총수를 모아놓고 \"시대적 요구는 역시 총수가 앞장서야 한다. 이미 상당한 변화의 조짐이 있다는 것을 고맙게 생각한다. 총수들께서 직접 관심을 가져주시면 빨리 전파돼 긍정적인 평가를 받을 수 있다고 본다\"고 말했다. 언뜻 보아 무슨 말인지 불분명하나 이 대통령이 지난 8ㆍ15 연설 후 정몽준 의원, 정몽구 현대차 회장이 각각 2000억원과 5000억원을 기부한 사실과 \\'공생발전\\'이란 화두를 연결하면 금방 짐작이 간다. 다른 그룹 총수들도 좀 나서라고 은근히 떠민 것이다. 이 대통령은 기부에 대한 후속 선언이 나오지 않은 탓인지 총수들의 사회공헌 방안에 불만을 표시했다는 후문이다. 최근 미국 프랑스 벨기에 등에서 부유세가 거론되고 독일조차 2년간 한시적으로 5%의 자산세를 거둬 약 155조원을 마련하자는 논의가 있었다. 이런 흐름에 한국만 동떨어져 있기는 어려운 게 글로벌 시대의 특징이다. 항간에는 이번 회동 후 삼성을 비롯해 몇몇 그룹이 노블레스 오블리주 방안을 준비하고 있다는 말이 나도는데 대통령의 강요나 포퓰리즘에 의한 압박보다 자발적 문화로 만들어가야 효과가 큰 법이다. 그런 면에서 재계에 적절한 방안 마련을 맡기고 정치권이나 여론은 너무 압박하지 말고 시간을 줘야 한다. 국가채무 문제로 글로벌 경기 침체 우려가 큰 상황에서 기업들은 \\'생존\\'에 큰 부담을 느끼고 있기 때문이다. 이날 전경련에 따르면 30대 그룹은 올해 고용 12만4000명, 투자 114조원 등 \\'선물\\'을 준비했다. 세계적인 더블딥이 우려되는 상황에서 공격경영이 어렵겠지만 연초 한 번 발표한 내용을 약간 수정해 내놓은 전경련의 행태는 답답하다. 설립 50주년이 됐으면 좀 더 창의적이고 유연하게 바뀔 때도 됐다. 허창수 전경련 회장은 \"대기업ㆍ중소기업이 서로 공생하고 발전할 수 있도록 노력하겠다. 기업이 사회적 책임을 다하겠다\"는 원론적인 발언에 그쳐 전경련 특유의 무미건조함을 드러냈다. 한편 이건희 삼성전자 회장은 \"중소기업계 협력을 강화해 국제적으로 경쟁력 있는 기업 생태계를 만들어 나가겠다\"고 발언했고, 정몽구 회장은 \"이제 1차 협력업체는 경쟁력을 확보한 만큼 2ㆍ3차 협력업체 지원에 힘쓰겠다\"고 했는데 의미 있는 내용이라고 본다. 그대로 실천하면 동반성장 생태계는 한층 강화될 것이다.', 'summary': '이명박 대통령은 어제 30대 그룹 총수를 모아놓고 시대적 요구는 역시 총수가 앞장서야 한다고 발언하며 기부문화 확산을 은근히 강조했으나 대통령의 강요나 포퓰리즘에 의한 압박보다는 자발적 문화로 구축해야 효과가 더 큰 법으로 방안 마련을 맡기고 시간을 줘야 할 것으로 보인다.'}\n","통합 훈련 데이터 길이: 325072\n"]}],"source":["# 정규화 데이터 예시 출력\n","print(f\"통합 훈련 데이터 샘플: {train_normalized[0]}\")\n","\n","# 누락 샘플이 없는지 확인\n","print(f\"통합 훈련 데이터 길이: {len(train_normalized)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mEUz0GGIHlpe"},"outputs":[],"source":["# 검증 데이터 통합\n","val_normalized = unifing_data(raw_val_edit, raw_val_law, raw_val_news)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mjQWAh13Hlpe","outputId":"5a6749e9-ba7e-40ec-a8dd-ea39992dc7cf"},"outputs":[{"name":"stdout","output_type":"stream","text":["통합 검증 데이터 샘플: {'text': '더불어민주당 이해찬 대표가 30 일 오후 국회에서 기자간담회를 열고 조국 전 법무부 장관 사태와 관련해 \"국민 여러분께 매우 송구하다\"고 밝혔다. 더불어민주당 이해찬 대표가 30 일 기자간담회를 열고 \\'조국 사태\\'와 관련, \"국민 여러분께 매우 송구하다\"는 입장을 밝혔다. 이 대표는 \"검찰 개혁이란 대의에 집중하다 보니, 국민 특히 청년이 느꼈을 불공정에 대한 상대적 박탈감, 좌절감을 깊이 있게 헤아리지 못했다\"며 \"여당 대표로서 무거운 책임감을 느낀다\"고 머리를 숙였다. 조국 전 법무부 장관이 14 일 사퇴한 이후 이 대표가 당 안팎의 쇄신 요구에 대해 입장을 표명한 것은 이번이 처음이다. 청와대와 여당은 \\'조국 정국\\'을 거치며 분출된 \\'공정\\'과 \\'정의\\'의 민심을 받들어 검찰 개혁에 매진하겠다면서도 두 달간 극심한 분열과 갈등을 초래한데 대해선 진지하게 성찰하는 모습을 보이지 않았다. 그나마 초선인 이철희 의원이 \"당이 대통령 뒤에 비겁하게 숨어 있었다\"고 비판했고, 표창원 의원은 \"책임을 느끼는 분들이 각자 형태로 그 책임감을 행동으로 옮겨야 할 때\"라고 지적했다. 뒤늦게나마 이 대표가 자성의 목소리를 내긴 했으나 당 안팎의 쇄신 요구에 어떻게 응할지 구체적 플랜을 제시하지 못해 여전히 안이하다는 지적도 나온다. 이 대표는 28 일 윤호중 사무총장을 단장으로 하는 총선기획단을 발족했고 조만간 인재영입위원회도 출범시킬 계획이라고 밝혔다. 이 대표는 \"민주당의 가치를 공유하는 참신한 인물을 영입해 준비된 정책과 인물로 승부하겠다\"고 다짐했다. 하지만 당 일각에선 \"총선기획단장을 비롯한 당직 인선부터 쇄신 의지를 보여야 한다\"는 비판의 목소리가 나온다. 무조건 물러나는 게 능사는 아니지만 국정 혼선을 초래한 데 대해 당 지도부가 겸허하게 책임지는 모습을 보이는 게 쇄신의 출발점이 돼야 한다는 지적도 있다. 선거는 대중의 이해와 요구를 잘 대표하는 정치인을 뽑는 행위다. 민생을 외면하며 낡은 이념과 진영 싸움에 매몰된 구시대 인물들을 과감히 물갈이하라는 게 국민의 요구다. 대신 4 차 산업혁명의 거센 파고를 헤쳐나갈 전문성을 갖춘 젊고 유능한 인재들을 널리 구해야 하다. 이해찬 대표의 이날 유감 표명이 여권 전반의 대대적인 인적 쇄신으로 이어지길 기대한다.', 'summary': '이해찬 대표가 조국 사태와 관련 송구한 입장 표명이 과감한 인적 쇄신으로 이어져야 한다.'}\n","통합 검증 데이터 길이: 40134\n"]}],"source":["# 정규화 데이터 예시 출력\n","print(f\"통합 검증 데이터 샘플: {val_normalized[0]}\")\n","\n","# 누락 샘플이 없는지 확인\n","print(f\"통합 검증 데이터 길이: {len(val_normalized)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"saWcLJFbHlpe","outputId":"934c9001-e01a-47e5-e340-b89521e8e93d"},"outputs":[{"name":"stdout","output_type":"stream","text":["통합 데이터 파일 저장 완료!\n"]}],"source":["# 통합 데이터 저장\n","save_jsonl(train_normalized, cfg.data_dir / \"train_normalized.jsonl\")\n","save_jsonl(val_normalized, cfg.data_dir / \"val_normalized.jsonl\")\n","print(\"통합 데이터 파일 저장 완료!\")"]},{"cell_type":"markdown","metadata":{"id":"Q1MQjWWDHlpf"},"source":["## EXTRA: Qwen용 데이터셋"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"idaCYOvQHlpf"},"outputs":[],"source":["# qwen용 데이터셋을 만들기 위한 데이터 샘플 생성 함수\n","def make_sft_sample(\n","        text: str,\n","        summary: str,\n","        instruction: str=cfg.sft_prompt,\n","        max_input_len: int=cfg.qwen_max_char,\n","        max_output_len: int=cfg.qwen_max_sumary\n","        ) -> Dict[str, str]:\n","    \"\"\"\n","    Qwen SFT 학습 샘플 생성\n","    \"\"\"\n","    # 길이 컷 (OOM 방지)\n","    if len(text) > max_input_len:\n","        text = text[:max_input_len]\n","\n","    if len(summary) > max_output_len:\n","        summary = summary[:max_output_len]\n","\n","    return {\n","        \"instruction\": instruction,\n","        \"input\": text,\n","        \"output\": summary,\n","    }\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bTN1M8Z_Hlpg","outputId":"c44624bd-01d6-4f1a-f17a-089a58ac8b97"},"outputs":[{"name":"stdout","output_type":"stream","text":["훈련 데이터 변환 완료\n","검증 데이터 변환 완료\n"]}],"source":["# 데이터 변환용 리스트 초기화\n","qwen_train_processed = []\n","qwen_val_processed = []\n","\n","for data in train_normalized:\n","    text = data[\"text\"]\n","    summary = data[\"summary\"]\n","\n","    one_line = make_sft_sample(text, summary)\n","    qwen_train_processed.append(one_line)\n","\n","print(\"훈련 데이터 변환 완료\")\n","\n","for data in val_normalized:\n","    text = data[\"text\"]\n","    summary = data[\"summary\"]\n","\n","    one_line = make_sft_sample(text, summary)\n","    qwen_val_processed.append(one_line)\n","\n","print(\"검증 데이터 변환 완료\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"djVtNrsnHlpg","outputId":"089cfa03-2f1e-476a-c864-d6a9700de896"},"outputs":[{"name":"stdout","output_type":"stream","text":["통합 훈련 데이터 샘플: {'instruction': '다음 문서를 핵심만 유지하며 간결한 한국어로 요약하시오.', 'input': '이명박 대통령이 어제 30대 그룹 총수를 모아놓고 \"시대적 요구는 역시 총수가 앞장서야 한다. 이미 상당한 변화의 조짐이 있다는 것을 고맙게 생각한다. 총수들께서 직접 관심을 가져주시면 빨리 전파돼 긍정적인 평가를 받을 수 있다고 본다\"고 말했다. 언뜻 보아 무슨 말인지 불분명하나 이 대통령이 지난 8ㆍ15 연설 후 정몽준 의원, 정몽구 현대차 회장이 각각 2000억원과 5000억원을 기부한 사실과 \\'공생발전\\'이란 화두를 연결하면 금방 짐작이 간다. 다른 그룹 총수들도 좀 나서라고 은근히 떠민 것이다. 이 대통령은 기부에 대한 후속 선언이 나오지 않은 탓인지 총수들의 사회공헌 방안에 불만을 표시했다는 후문이다. 최근 미국 프랑스 벨기에 등에서 부유세가 거론되고 독일조차 2년간 한시적으로 5%의 자산세를 거둬 약 155조원을 마련하자는 논의가 있었다. 이런 흐름에 한국만 동떨어져 있기는 어려운 게 글로벌 시대의 특징이다. 항간에는 이번 회동 후 삼성을 비롯해 몇몇 그룹이 노블레스 오블리주 방안을 준비하고 있다는 말이 나도는데 대통령의 강요나 포퓰리즘에 의한 압박보다 자발적 문화로 만들어가야 효과가 큰 법이다. 그런 면에서 재계에 적절한 방안 마련을 맡기고 정치권이나 여론은 너무 압박하지 말고 시간을 줘야 한다. 국가채무 문제로 글로벌 경기 침체 우려가 큰 상황에서 기업들은 \\'생존\\'에 큰 부담을 느끼고 있기 때문이다. 이날 전경련에 따르면 30대 그룹은 올해 고용 12만4000명, 투자 114조원 등 \\'선물\\'을 준비했다. 세계적인 더블딥이 우려되는 상황에서 공격경영이 어렵겠지만 연초 한 번 발표한 내용을 약간 수정해 내놓은 전경련의 행태는 답답하다. 설립 50주년이 됐으면 좀 더 창의적이고 유연하게 바뀔 때도 됐다. 허창수 전경련 회장은 \"대기업ㆍ중소기업이 서로 공생하고 발전할 수 있도록 노력하겠다. 기업이 사회적 책임을 다하겠다\"는 원론적인 발언에 그쳐 전경련 특유의 무미건조함을 드러냈다. 한편 이건희 삼성전자 회장은 \"중소기업계 협력을 강화해 국제적으로 경쟁력 있는 기업 생태계를 만들어 나가겠다\"고 발언했고, 정몽구 회장은 \"이제 1차 협력업체는 경쟁력을 확보한 만큼 2ㆍ3차 협력업체 지원에 힘쓰겠다\"고 했는데 의미 있는 내용이라고 본다. 그대로 실천하면 동반성장 생태계는 한층 강화될 것이다.', 'output': '이명박 대통령은 어제 30대 그룹 총수를 모아놓고 시대적 요구는 역시 총수가 앞장서야 한다고 발언하며 기부문화 확산을 은근히 강조했으나 대통령의 강요나 포퓰리즘에 의한 압박보다는 자발적 문화로 구축해야 효과가 더 큰 법으로 방안 마련을 맡기고 시간을 줘야 할 것으로 보인다.'}\n","통합 훈련 데이터 길이: 325072\n"]}],"source":["# qwen 데이터 예시 출력\n","print(f\"통합 훈련 데이터 샘플: {qwen_train_processed[0]}\")\n","\n","# 누락 샘플이 없는지 확인\n","print(f\"통합 훈련 데이터 길이: {len(qwen_train_processed)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kiAa2cxUHlph","outputId":"e00e6e4f-b992-4157-dffb-85c65b49e82b"},"outputs":[{"name":"stdout","output_type":"stream","text":["통합 검증 데이터 샘플: {'instruction': '다음 문서를 핵심만 유지하며 간결한 한국어로 요약하시오.', 'input': '더불어민주당 이해찬 대표가 30 일 오후 국회에서 기자간담회를 열고 조국 전 법무부 장관 사태와 관련해 \"국민 여러분께 매우 송구하다\"고 밝혔다. 더불어민주당 이해찬 대표가 30 일 기자간담회를 열고 \\'조국 사태\\'와 관련, \"국민 여러분께 매우 송구하다\"는 입장을 밝혔다. 이 대표는 \"검찰 개혁이란 대의에 집중하다 보니, 국민 특히 청년이 느꼈을 불공정에 대한 상대적 박탈감, 좌절감을 깊이 있게 헤아리지 못했다\"며 \"여당 대표로서 무거운 책임감을 느낀다\"고 머리를 숙였다. 조국 전 법무부 장관이 14 일 사퇴한 이후 이 대표가 당 안팎의 쇄신 요구에 대해 입장을 표명한 것은 이번이 처음이다. 청와대와 여당은 \\'조국 정국\\'을 거치며 분출된 \\'공정\\'과 \\'정의\\'의 민심을 받들어 검찰 개혁에 매진하겠다면서도 두 달간 극심한 분열과 갈등을 초래한데 대해선 진지하게 성찰하는 모습을 보이지 않았다. 그나마 초선인 이철희 의원이 \"당이 대통령 뒤에 비겁하게 숨어 있었다\"고 비판했고, 표창원 의원은 \"책임을 느끼는 분들이 각자 형태로 그 책임감을 행동으로 옮겨야 할 때\"라고 지적했다. 뒤늦게나마 이 대표가 자성의 목소리를 내긴 했으나 당 안팎의 쇄신 요구에 어떻게 응할지 구체적 플랜을 제시하지 못해 여전히 안이하다는 지적도 나온다. 이 대표는 28 일 윤호중 사무총장을 단장으로 하는 총선기획단을 발족했고 조만간 인재영입위원회도 출범시킬 계획이라고 밝혔다. 이 대표는 \"민주당의 가치를 공유하는 참신한 인물을 영입해 준비된 정책과 인물로 승부하겠다\"고 다짐했다. 하지만 당 일각에선 \"총선기획단장을 비롯한 당직 인선부터 쇄신 의지를 보여야 한다\"는 비판의 목소리가 나온다. 무조건 물러나는 게 능사는 아니지만 국정 혼선을 초래한 데 대해 당 지도부가 겸허하게 책임지는 모습을 보이는 게 쇄신의 출발점이 돼야 한다는 지적도 있다. 선거는 대중의 이해와 요구를 잘 대표하는 정치인을 뽑는 행위다. 민생을 외면하며 낡은 이념과 진영 싸움에 매몰된 구시대 인물들을 과감히 물갈이하라는 게 국민의 요구다. 대신 4 차 산업혁명의 거센 파고를 헤쳐나갈 전문성을 갖춘 젊고 유능한 인재들을 널리 구해야 하다. 이해찬 대표의 이날 유감 표명이 여권 전반의 대대적인 인적 쇄신으로 이어지길 기대한다.', 'output': '이해찬 대표가 조국 사태와 관련 송구한 입장 표명이 과감한 인적 쇄신으로 이어져야 한다.'}\n","통합 검증 데이터 길이: 40134\n"]}],"source":["# qwen 데이터 예시 출력\n","print(f\"통합 검증 데이터 샘플: {qwen_val_processed[0]}\")\n","\n","# 누락 샘플이 없는지 확인\n","print(f\"통합 검증 데이터 길이: {len(qwen_val_processed)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M0r5YK-_Hlpi","outputId":"0023d7bc-4cf6-49bc-d211-d06cbc627610"},"outputs":[{"name":"stdout","output_type":"stream","text":["Qwen용 데이터 파일 저장 완료!\n"]}],"source":["# Qwen용 데이터 저장\n","save_jsonl(qwen_train_processed, cfg.data_dir / \"qwen_train_processed.jsonl\")\n","save_jsonl(qwen_val_processed, cfg.data_dir / \"qwen_val_processed.jsonl\")\n","print(\"Qwen용 데이터 파일 저장 완료!\")"]},{"cell_type":"markdown","metadata":{"id":"RyFwoQAeHlpi"},"source":["# 2. 데이터셋"]},{"cell_type":"markdown","metadata":{"id":"sIODxSsMHlpi"},"source":["## 2.1. KoBART"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HTR1AgUZHlpj","outputId":"88adbf51-5767-4add-8dac-eb49a69608e5"},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating train split: 325072 examples [00:01, 215016.60 examples/s]\n","Generating validation split: 40134 examples [00:00, 206946.73 examples/s]\n"]}],"source":["# 저장한 데이터셋 불러오기 (메모리 절약)\n","kobart_ds = load_dataset(\n","    \"json\",\n","    data_files={\n","        \"train\": str(cfg.data_dir / \"train_normalized.jsonl\"),\n","        \"validation\": str(cfg.data_dir / \"val_normalized.jsonl\"),\n","    }\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P7xZnByfHlpj"},"outputs":[],"source":["# train_normalized와 val_normalized로 Dataset 객체 생성\n","# 훈련 시간 단축을 위한 데이터셋 랜덤 셔플 및 부분 선별\n","train_dataset = kobart_ds[\"train\"].shuffle(seed=cfg.seed).select(range(50000))\n","val_dataset = kobart_ds[\"validation\"].shuffle(seed=cfg.seed).select(range(2000))\n","\n","# DatasetDict 형태로 통합 (훈련/검증)\n","kobart_dataset = DatasetDict({\n","    \"train\": train_dataset,\n","    \"validation\": val_dataset\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G9gwg4jRHlpk","outputId":"d287c3e0-5dff-489d-c0d6-6bcc9bc9398f"},"outputs":[{"name":"stderr","output_type":"stream","text":["You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n","The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n","The class this function is called from is 'BartTokenizerFast'.\n"]}],"source":["# KoBART용 Fast Tokenizer 사용\n","kobart_id = \"gogamza/kobart-base-v1\"\n","kobart_tokenizer = BartTokenizerFast.from_pretrained(kobart_id)\n","\n","# KoBART용 collate_fn 불러오기\n","kobart_collator = DataCollatorForSeq2Seq(tokenizer=kobart_tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zoYwgOw4Hlpl"},"outputs":[],"source":["# 데이터샘플 토큰화 함수\n","def kobart_tokenize_function(\n","        example: Dict[str, List[str]]\n","        ) -> Dict[str, List[List[int]]]:\n","    model_inputs = kobart_tokenizer(\n","        example[\"text\"],\n","        max_length=cfg.text_max_len,\n","        truncation=True\n","    )\n","    labels = kobart_tokenizer(\n","        text_target=example[\"summary\"],\n","        max_length=cfg.summary_max_len,\n","        truncation=True\n","    )\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yLkiq6AEHlpl","outputId":"94f7053e-cf36-40ba-efd7-2062dfc41c86"},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 50000/50000 [00:08<00:00, 5717.04 examples/s]\n","Map: 100%|██████████| 2000/2000 [00:00<00:00, 6093.40 examples/s]\n"]}],"source":["# 토큰화 함수 적용\n","kobart_dataset = kobart_dataset.map(kobart_tokenize_function, batched=True)"]},{"cell_type":"markdown","metadata":{"id":"mu3eFnDIHlpm"},"source":["## 2.2. Qwen3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7CcR-2rFHlpm","outputId":"a6dd93cf-f987-467c-c707-c3402ac623bb"},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating train split: 325072 examples [00:01, 220495.80 examples/s]\n","Generating validation split: 40134 examples [00:00, 187610.63 examples/s]\n"]}],"source":["# 저장한 데이터셋 불러오기 (메모리 절약)\n","qwen_ds = load_dataset(\n","    \"json\",\n","    data_files={\n","        \"train\": str(cfg.data_dir / \"qwen_train_processed.jsonl\"),\n","        \"validation\": str(cfg.data_dir / \"qwen_val_processed.jsonl\"),\n","    }\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cEIZBsDjHlpx"},"outputs":[],"source":["# qwen_train_processed와 qwen_val_processed로 Dataset 객체 생성\n","# 훈련 시간 단축을 위한 데이터셋 랜덤 셔플 및 부분 선별\n","qwen_train_dataset = qwen_ds[\"train\"].shuffle(seed=cfg.seed).select(range(20000))\n","qwen_val_dataset = qwen_ds[\"validation\"].shuffle(seed=cfg.seed).select(range(1000))\n","\n","# DatasetDict 형태로 통합 (훈련/검증)\n","qwen_dataset = DatasetDict({\n","    \"train\": qwen_train_dataset,\n","    \"validation\": qwen_val_dataset\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tNM7sUwcHlpy"},"outputs":[],"source":["qwen_id = \"Qwen/Qwen3-4B\"\n","qwen_tokenizer = AutoTokenizer.from_pretrained(qwen_id, use_fast=True)\n","\n","if qwen_tokenizer.pad_token is None:\n","    qwen_tokenizer.pad_token = qwen_tokenizer.eos_token"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tn3el5BsHlpy"},"outputs":[],"source":["def qwen_format_dataset_batch(batch):\n","    texts = []\n","    for inst, inp, out in zip(batch[\"instruction\"], batch[\"input\"], batch[\"output\"]):\n","        texts.append(\n","            \"### Instruction:\\n\"\n","            f\"{inst}\\n\\n\"\n","            \"### Input:\\n\"\n","            f\"{inp}\\n\\n\"\n","            \"### Output:\\n\"\n","            f\"{out}\"\n","        )\n","    return {\"text\": texts}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yyvcQWY4Hlpz","outputId":"9e048a61-98df-4ce3-9821-72ad7bf82b73"},"outputs":[{"name":"stderr","output_type":"stream","text":["Map: 100%|██████████| 20000/20000 [00:00<00:00, 41019.97 examples/s]\n","Map: 100%|██████████| 1000/1000 [00:00<00:00, 37252.24 examples/s]\n"]}],"source":["qwen_dataset = qwen_dataset.map(qwen_format_dataset_batch, batched=True)"]},{"cell_type":"markdown","metadata":{"id":"oqPBFlPmHlpz"},"source":["# 3. 모델 파인튜닝"]},{"cell_type":"markdown","metadata":{"id":"OZ0I2oILHlp0"},"source":["## 3.1. KoBART 파인튜닝"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vOCKe2XCHlp0","outputId":"e460a999-d988-457a-b956-baf3cffb9a7e"},"outputs":[{"name":"stderr","output_type":"stream","text":["You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"]}],"source":["# KoBART 요약 모델 로드\n","kobart_model = BartForConditionalGeneration.from_pretrained(kobart_id).to(cfg.device)\n","\n","# Hugging Face Trainer의 학습 설정(하이퍼파라미터) 묶음\n","kobart_args = TrainingArguments(\n","    # output_dir: 학습 산출물 저장 경로\n","    # 체크포인트(중간 저장), 최종 모델, 학습 로그 등이 이 경로에 저장\n","    output_dir=str(cfg.model_dir / \"kobart_summarizer\"),\n","    # 디바이스(GPU) 1개당 학습 배치 크기\n","    per_device_train_batch_size=2,\n","    # 디바이스 1개당 평가 배치 크기\n","    per_device_eval_batch_size=2,\n","    # 학습률\n","    learning_rate=cfg.lr,\n","    # 에포크 수\n","    num_train_epochs=1,\n","    # 가중치 감쇠(L2 정규화 성격)\n","    # 과적합을 억제하고, 파라미터가 지나치게 커지는 것을 방지\n","    weight_decay=0.01,\n","    # 평가 수행 기준\n","    # \"steps\": 일정 step마다 평가 수행\n","    # \"epoch\": epoch 끝날 때마다 평가 수행\n","    eval_strategy=\"steps\",\n","    # 몇 step마다 평가할지 지정하는 파라미터\n","    eval_steps=500,\n","    # 몇 step마다 로그를 출력할지 지정하는 파라미터\n","    logging_steps=100,\n","    # save_steps: 몇 step마다 체크포인트를 저장하는 파라미터\n","    save_steps=500,\n","    # 체크포인트 최대 개수 제한\n","    save_total_limit=2,\n","    # fp16: mixed precision(반정밀도) 사용 여부\n","    # GPU가 있을 때  VRAM 사용량 감소\n","    fp16=torch.cuda.is_available(),\n","    # report_to: 외부 로깅 도구 사용 여부\n","    report_to=\"none\",\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3bc-tdQUHlp0","outputId":"5eae0efc-b114-4863-bc94-2cae5bf75db9"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_1379/2538476431.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  kobart_trainer = Trainer(\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='25000' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [25000/25000 25:35, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>2.086300</td>\n","      <td>1.803897</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>1.899900</td>\n","      <td>1.750412</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.884500</td>\n","      <td>1.721366</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>1.911400</td>\n","      <td>1.714548</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>1.722400</td>\n","      <td>1.715675</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>1.904600</td>\n","      <td>1.676376</td>\n","    </tr>\n","    <tr>\n","      <td>3500</td>\n","      <td>1.776300</td>\n","      <td>1.681219</td>\n","    </tr>\n","    <tr>\n","      <td>4000</td>\n","      <td>1.776000</td>\n","      <td>1.666294</td>\n","    </tr>\n","    <tr>\n","      <td>4500</td>\n","      <td>1.724300</td>\n","      <td>1.750922</td>\n","    </tr>\n","    <tr>\n","      <td>5000</td>\n","      <td>1.741000</td>\n","      <td>1.647183</td>\n","    </tr>\n","    <tr>\n","      <td>5500</td>\n","      <td>1.680000</td>\n","      <td>1.654417</td>\n","    </tr>\n","    <tr>\n","      <td>6000</td>\n","      <td>1.808200</td>\n","      <td>1.631309</td>\n","    </tr>\n","    <tr>\n","      <td>6500</td>\n","      <td>1.698900</td>\n","      <td>1.655435</td>\n","    </tr>\n","    <tr>\n","      <td>7000</td>\n","      <td>1.697500</td>\n","      <td>1.625847</td>\n","    </tr>\n","    <tr>\n","      <td>7500</td>\n","      <td>1.611100</td>\n","      <td>1.624375</td>\n","    </tr>\n","    <tr>\n","      <td>8000</td>\n","      <td>1.642300</td>\n","      <td>1.616153</td>\n","    </tr>\n","    <tr>\n","      <td>8500</td>\n","      <td>1.615600</td>\n","      <td>1.621504</td>\n","    </tr>\n","    <tr>\n","      <td>9000</td>\n","      <td>1.675400</td>\n","      <td>1.596859</td>\n","    </tr>\n","    <tr>\n","      <td>9500</td>\n","      <td>1.742600</td>\n","      <td>1.599930</td>\n","    </tr>\n","    <tr>\n","      <td>10000</td>\n","      <td>1.699400</td>\n","      <td>1.597773</td>\n","    </tr>\n","    <tr>\n","      <td>10500</td>\n","      <td>1.768400</td>\n","      <td>1.583075</td>\n","    </tr>\n","    <tr>\n","      <td>11000</td>\n","      <td>1.604700</td>\n","      <td>1.587643</td>\n","    </tr>\n","    <tr>\n","      <td>11500</td>\n","      <td>1.703400</td>\n","      <td>1.570724</td>\n","    </tr>\n","    <tr>\n","      <td>12000</td>\n","      <td>1.714600</td>\n","      <td>1.572929</td>\n","    </tr>\n","    <tr>\n","      <td>12500</td>\n","      <td>1.758400</td>\n","      <td>1.570042</td>\n","    </tr>\n","    <tr>\n","      <td>13000</td>\n","      <td>1.673500</td>\n","      <td>1.579841</td>\n","    </tr>\n","    <tr>\n","      <td>13500</td>\n","      <td>1.600500</td>\n","      <td>1.563747</td>\n","    </tr>\n","    <tr>\n","      <td>14000</td>\n","      <td>1.648300</td>\n","      <td>1.551148</td>\n","    </tr>\n","    <tr>\n","      <td>14500</td>\n","      <td>1.643200</td>\n","      <td>1.544361</td>\n","    </tr>\n","    <tr>\n","      <td>15000</td>\n","      <td>1.537100</td>\n","      <td>1.549237</td>\n","    </tr>\n","    <tr>\n","      <td>15500</td>\n","      <td>1.622100</td>\n","      <td>1.546351</td>\n","    </tr>\n","    <tr>\n","      <td>16000</td>\n","      <td>1.637700</td>\n","      <td>1.533202</td>\n","    </tr>\n","    <tr>\n","      <td>16500</td>\n","      <td>1.548800</td>\n","      <td>1.538377</td>\n","    </tr>\n","    <tr>\n","      <td>17000</td>\n","      <td>1.621700</td>\n","      <td>1.526311</td>\n","    </tr>\n","    <tr>\n","      <td>17500</td>\n","      <td>1.705400</td>\n","      <td>1.517271</td>\n","    </tr>\n","    <tr>\n","      <td>18000</td>\n","      <td>1.463500</td>\n","      <td>1.526394</td>\n","    </tr>\n","    <tr>\n","      <td>18500</td>\n","      <td>1.583500</td>\n","      <td>1.509499</td>\n","    </tr>\n","    <tr>\n","      <td>19000</td>\n","      <td>1.592500</td>\n","      <td>1.508930</td>\n","    </tr>\n","    <tr>\n","      <td>19500</td>\n","      <td>1.462800</td>\n","      <td>1.505866</td>\n","    </tr>\n","    <tr>\n","      <td>20000</td>\n","      <td>1.554600</td>\n","      <td>1.498281</td>\n","    </tr>\n","    <tr>\n","      <td>20500</td>\n","      <td>1.481900</td>\n","      <td>1.496309</td>\n","    </tr>\n","    <tr>\n","      <td>21000</td>\n","      <td>1.498600</td>\n","      <td>1.490821</td>\n","    </tr>\n","    <tr>\n","      <td>21500</td>\n","      <td>1.564100</td>\n","      <td>1.484208</td>\n","    </tr>\n","    <tr>\n","      <td>22000</td>\n","      <td>1.583400</td>\n","      <td>1.482589</td>\n","    </tr>\n","    <tr>\n","      <td>22500</td>\n","      <td>1.500600</td>\n","      <td>1.482088</td>\n","    </tr>\n","    <tr>\n","      <td>23000</td>\n","      <td>1.573000</td>\n","      <td>1.477359</td>\n","    </tr>\n","    <tr>\n","      <td>23500</td>\n","      <td>1.520400</td>\n","      <td>1.474877</td>\n","    </tr>\n","    <tr>\n","      <td>24000</td>\n","      <td>1.572800</td>\n","      <td>1.474037</td>\n","    </tr>\n","    <tr>\n","      <td>24500</td>\n","      <td>1.474700</td>\n","      <td>1.473496</td>\n","    </tr>\n","    <tr>\n","      <td>25000</td>\n","      <td>1.524000</td>\n","      <td>1.473071</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/home/ahnhs2k/pytorch-demo/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'forced_eos_token_id': 1}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["KoBART 학습 및 저장 완료\n"]}],"source":["# Hugging Face Trainer 생성\n","kobart_trainer = Trainer(\n","    model=kobart_model,\n","    args=kobart_args,\n","    train_dataset=kobart_dataset[\"train\"],\n","    eval_dataset=kobart_dataset[\"validation\"],\n","    data_collator=kobart_collator,\n","    tokenizer=kobart_tokenizer,\n",")\n","\n","# 학습 시작\n","kobart_trainer.train()\n","\n","\n","# 학습된 모델 저장\n","# KoBART는 모델(가중치) + 토크나이저(단어사전/토큰화 규칙)가 \"세트\"로 움직이므로 둘 다 저장\n","kobart_trainer.save_model(str(cfg.model_dir / \"kobart_summarizer\"))\n","\n","# 토크나이저 vocab, merges, tokenizer_config 등 저장\n","kobart_tokenizer.save_pretrained(str(cfg.model_dir / \"kobart_summarizer\"))\n","\n","print(\"KoBART 학습 및 저장 완료\")\n"]},{"cell_type":"markdown","metadata":{"id":"fQXe0CiJHlp1"},"source":["## 3.2. Qwen QLora 파인튜닝"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZiEJabcwHlp1"},"outputs":[],"source":["\"\"\"\n","BitsAndBytesConfig: QLoRA(4bit) 로딩을 위한 양자화 설정\n","\n","Base Model 가중치(weight)를 4bit로 양자화해서 GPU 메모리를 크게 줄이고\n","LoRA 어댑터만 학습(업데이트)하는 방식으로 fine-tuning을 수행한다.\n","\n","BitsAndBytesConfig는 bitsandbytes 라이브러리를 이용해\n","모델을 4bit/8bit 형태로 로드할 때 필요한 옵션들을 담는 설정 객체다.\n","\"\"\"\n","\n","bnb_config = BitsAndBytesConfig(\n","    # load_in_4bit=True\n","    # 모델의 base weight를 4bit 정밀도로 로드한다.\n","    load_in_4bit=True,\n","    # 실제 연산(Compute)은 어떤 dtype으로 할지 지정.\n","    bnb_4bit_compute_dtype=torch.bfloat16,\n","    # bnb_4bit_quant_type=\"nf4\"\n","    # 4bit 양자화 방식(Quantization Type)을 지정한다.\n","    # \"nf4\"(NormalFloat4)는 QLoRA 논문/실무에서 가장 많이 쓰는 방식 중 하나로, 단순 4bit보다 정보 보존이 좋아서 품질 손실을 줄이기 유리하다.\n","    bnb_4bit_quant_type=\"nf4\",\n","    # bnb_4bit_use_double_quant=True\n","    # Double Quantization(이중 양자화)을 사용할지 지정\n","    # 4bit로 줄인 가중치를 다시 한 번 더 효율적으로 압축/표현해서 VRAM을 추가로 절약하는 옵션\n","    bnb_4bit_use_double_quant=True,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_QfLZZzQHlp2","outputId":"270be897-6290-44ce-9a59-040fe3e88fb2"},"outputs":[{"name":"stderr","output_type":"stream","text":["Fetching 3 files: 100%|██████████| 3/3 [11:41<00:00, 233.89s/it]\n","Loading checkpoint shards: 100%|██████████| 3/3 [00:11<00:00,  3.82s/it]\n"]}],"source":["# Qwen 모델 로드 (4bit QLoRA 학습 준비)\n","qwen_model = AutoModelForCausalLM.from_pretrained(\n","    qwen_id,\n","    quantization_config=bnb_config,\n","    device_map=\"auto\",\n",")\n","\n","# 메모리 절약 설정 Gradient Checkpointing 활성화\n","# gradient checkpointing은 학습 시 중간 activation(중간 계산 결과)을 전부 저장하지 않고 일부만 저장하는 방식\n","qwen_model.gradient_checkpointing_enable()\n","\n","# 메모리 절약 설정 use_cache 비활성화\n","# use_cache는 \"추론(inference)\"에서 속도를 올리기 위해 past_key_values(Attention 캐시)를 저장하는 기능\n","# 하지만 \"학습(training)\"에서는 이 캐시가 오히려 GPU 메모리를 크게 잡아먹고, gradient checkpointing과 충돌/경고를 유발하는 경우가 많으므로 False\n","qwen_model.config.use_cache = False\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rHeVu1goHlp3"},"outputs":[],"source":["\"\"\"\n","LoRAConfig: QLoRA(LoRA + 4bit) 학습을 위한 어댑터 설정\n","LoRA(Parameter-Efficient Fine-Tuning)는\n","- 원본(Base) 모델의 가중치는 동결(freeze)하고\n","- 일부 선형 레이어에만 작은 저차원 행렬(LoRA 어댑터)을 추가하여\n","  학습 비용(VRAM/시간)을 크게 줄이면서도 성능을 확보하는 기법\n","\n","QLoRA에서는\n","- Base Model: 4bit 양자화된 상태로 유지\n","- LoRA Adapter: FP16/FP32로 학습\n","이 조합을 사용\n","\"\"\"\n","\n","lora_config = LoraConfig(\n","    # r (rank): LoRA의 저차원(rank) 크기\n","    # - LoRA 어댑터는 (원래 가중치 행렬 W 대신)\n","    #   W + (A @ B) 형태로 업데이트를 수행\n","    # - 이때 A, B의 차원을 결정하는 것이 r\n","\n","    # - r이 클수록 표현력 증가 (더 많은 정보를 학습 가능)\n","\n","    # - r이 작을수록 학습은 가벼워지지만 표현력이 제한됨\n","    r=8,\n","    # lora_alpha: LoRA scaling 계수\n","    # - LoRA 업데이트 (A @ B)에 곱해지는 스케일링 값\n","    # - 실제 업데이트는 대략:\n","    #   (lora_alpha / r) * (A @ B)\n","\n","    # - 관례적으로 lora_alpha는 r의 2배 정도로 설정\n","    #   (r=8 -> alpha=16, r=16 -> alpha=32 등)\n","\n","    # - 너무 작으면 학습 효과가 약해지고, 너무 크면 학습이 불안정해질 수 있다.\n","    lora_alpha=16,\n","    # lora_dropout: LoRA 어댑터에 적용되는 dropout 비율\n","    lora_dropout=0.05,\n","    # bias: bias 파라미터 처리 방식\n","    # - \"none\": bias는 학습하지 않음 (가장 일반적인 설정)\n","    # - \"all\": 모든 bias를 학습\n","    # - \"lora_only\": LoRA가 적용된 레이어의 bias만 학습\n","\n","    # - QLoRA에서는 보통 \"none\"을 사용해 학습 파라미터 수를 최소화\n","    bias=\"none\",\n","    # task_type: LoRA를 적용할 모델의 태스크 유형\n","    # - \"CAUSAL_LM\": Causal Language Modeling\n","    #   (다음 토큰 예측 기반 모델, Qwen/LLaMA 계열)\n","\n","    # - 이 값에 따라 LoRA가 내부적으로\n","    #   어떤 forward 구조를 기준으로 동작할지 결정된다.\n","    task_type=\"CAUSAL_LM\",\n","    # target_modules: LoRA를 적용할 레이어 이름들\n","    # - LoRA는 \"모든 레이어\"가 아니라,\n","    #   특정 선형(linear) 레이어에만 삽입된다.\n","\n","    # - 아래 모듈들은 Qwen 계열 Transformer에서 주로 사용되는 핵심 투영 레이어들:\n","\n","    #   q_proj, k_proj, v_proj:\n","    #     - Attention의 Query / Key / Value projection\n","\n","    #   o_proj:\n","    #     - Attention 출력 projection\n","\n","    #   gate_proj, up_proj, down_proj:\n","    #     - FFN(Feed-Forward Network, MLP) 내부의 projection 레이어들\n","    target_modules=[\n","        \"q_proj\",\n","        \"k_proj\",\n","        \"v_proj\",\n","        \"o_proj\",\n","        \"gate_proj\",\n","        \"up_proj\",\n","        \"down_proj\",\n","    ],\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_VdPKcrrHlp9"},"outputs":[],"source":["qwen_args = SFTConfig(\n","    output_dir=str(cfg.model_dir / \"qwen_summarizer_qlora\"),\n","    per_device_train_batch_size=1,\n","    per_device_eval_batch_size=1,\n","    gradient_accumulation_steps=8, # 여러 그레디언트 모아서 한 번에 업데이트\n","    learning_rate=2e-4,\n","    num_train_epochs=1,\n","    logging_steps=50,\n","    save_total_limit=2,\n","    save_steps=300,\n","    eval_strategy=\"steps\",\n","    eval_steps=300,\n","    bf16=torch.cuda.is_available(),\n","    report_to=\"none\",\n","    dataset_text_field=\"text\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gsfBRxOtHlp-","outputId":"20836df9-d2ce-4eef-f722-5c0c2faa729d"},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='2500' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [2500/2500 3:20:45, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Entropy</th>\n","      <th>Num Tokens</th>\n","      <th>Mean Token Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>300</td>\n","      <td>1.705500</td>\n","      <td>1.685624</td>\n","      <td>1.690506</td>\n","      <td>1963708.000000</td>\n","      <td>0.614289</td>\n","    </tr>\n","    <tr>\n","      <td>600</td>\n","      <td>1.703200</td>\n","      <td>1.662069</td>\n","      <td>1.666654</td>\n","      <td>3933986.000000</td>\n","      <td>0.617698</td>\n","    </tr>\n","    <tr>\n","      <td>900</td>\n","      <td>1.691400</td>\n","      <td>1.647017</td>\n","      <td>1.626818</td>\n","      <td>5898205.000000</td>\n","      <td>0.620226</td>\n","    </tr>\n","    <tr>\n","      <td>1200</td>\n","      <td>1.681700</td>\n","      <td>1.636094</td>\n","      <td>1.639056</td>\n","      <td>7887692.000000</td>\n","      <td>0.622147</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>1.650900</td>\n","      <td>1.627955</td>\n","      <td>1.627508</td>\n","      <td>9860540.000000</td>\n","      <td>0.623774</td>\n","    </tr>\n","    <tr>\n","      <td>1800</td>\n","      <td>1.650000</td>\n","      <td>1.622054</td>\n","      <td>1.629847</td>\n","      <td>11832016.000000</td>\n","      <td>0.624774</td>\n","    </tr>\n","    <tr>\n","      <td>2100</td>\n","      <td>1.642100</td>\n","      <td>1.618354</td>\n","      <td>1.620977</td>\n","      <td>13814710.000000</td>\n","      <td>0.625449</td>\n","    </tr>\n","    <tr>\n","      <td>2400</td>\n","      <td>1.651400</td>\n","      <td>1.617045</td>\n","      <td>1.620867</td>\n","      <td>15788845.000000</td>\n","      <td>0.625590</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Qwen QLoRA 학습 및 저장 완료\n"]}],"source":["qwen_trainer = SFTTrainer(\n","    model=qwen_model,\n","    args=qwen_args,\n","    train_dataset=qwen_dataset[\"train\"],\n","    eval_dataset=qwen_dataset[\"validation\"],\n","    peft_config=lora_config,\n",")\n","\n","qwen_trainer.train()\n","\n","qwen_trainer.model.save_pretrained(str(cfg.model_dir / \"qwen_summarizer_qlora\"))\n","qwen_tokenizer.save_pretrained(str(cfg.model_dir / \"qwen_summarizer_qlora\"))\n","\n","print(\"Qwen QLoRA 학습 및 저장 완료\")"]},{"cell_type":"markdown","metadata":{"id":"gawVTVdOHlp_"},"source":["# 4. 테스트 및 평가"]},{"cell_type":"markdown","metadata":{"id":"hXg3hCgiHlp_"},"source":["## 4.1. KoBART"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IV7arE-MHlqA","outputId":"fec72fc3-c8cf-4fbc-a4bb-9c487db71c54"},"outputs":[{"name":"stderr","output_type":"stream","text":["You passed `num_labels=3` which is incompatible to the `id2label` map of length `2`.\n"]},{"data":{"text/plain":["BartForConditionalGeneration(\n","  (model): BartModel(\n","    (shared): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n","    (encoder): BartEncoder(\n","      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n","      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n","      (layers): ModuleList(\n","        (0-5): 6 x BartEncoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): GELUActivation()\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (decoder): BartDecoder(\n","      (embed_tokens): BartScaledWordEmbedding(30000, 768, padding_idx=3)\n","      (embed_positions): BartLearnedPositionalEmbedding(1028, 768)\n","      (layers): ModuleList(\n","        (0-5): 6 x BartDecoderLayer(\n","          (self_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (activation_fn): GELUActivation()\n","          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): BartAttention(\n","            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n","            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n","          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n","          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (lm_head): Linear(in_features=768, out_features=30000, bias=False)\n",")"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["# 토크나이저 로드\n","kobart_tokenizer = AutoTokenizer.from_pretrained(str(cfg.model_dir / \"kobart_summarizer\"))\n","\n","# 모델 로드 (Seq2Seq)\n","kobart_model = BartForConditionalGeneration.from_pretrained(str(cfg.model_dir / \"kobart_summarizer\")).to(cfg.device)\n","kobart_model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AhpCpNgiHlqA"},"outputs":[],"source":["# 추론 함수\n","def summarize_kobart(text: str, max_input_len: int=cfg.text_max_len, max_new_tokens: int=cfg.summary_max_len) -> str:\n","    inputs = kobart_tokenizer(\n","        text,\n","        return_tensors=\"pt\",\n","        truncation=True,\n","        max_length=max_input_len,\n","    ).to(cfg.device)\n","\n","    with torch.no_grad():\n","        output_ids = kobart_model.generate(\n","            **inputs,\n","            max_new_tokens=max_new_tokens,\n","            num_beams=4,\n","            early_stopping=True,\n","        )\n","\n","    return kobart_tokenizer.decode(output_ids[0], skip_special_tokens=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JWBNBX0XHlqB","outputId":"c6833939-f8dd-4c0f-9d7d-14bea026e93a"},"outputs":[{"name":"stdout","output_type":"stream","text":["KoBART 요약: 대한상공회의소 지속성장이니셔티브(SGI)는 8 일 하나의 산업을 둘러싸고 나뭇가지처럼 얽혀있는 연관 규제를 도식화한 '규제트리'를 공개하고 국내 신산업이 경쟁력을 갖추기 위해서는 대못규제, 중복규제, 소극규제 등 3 대 규제를 풀어야 한다고 주장했다.                                                                                                                                                                                                \n"]}],"source":["sample_text = kobart_ds[\"validation\"][2000][\"text\"]\n","print(\"KoBART 요약:\", summarize_kobart(sample_text))"]},{"cell_type":"markdown","metadata":{"id":"--RhWAjkHlqC"},"source":["## 4.2. Qwen"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3cYrnZS2HlqC","outputId":"8bb4b9e7-ed31-40b4-c018-d0f48efbfa39"},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.97s/it]\n"]},{"data":{"text/plain":["PeftModelForCausalLM(\n","  (base_model): LoraModel(\n","    (model): Qwen3ForCausalLM(\n","      (model): Qwen3Model(\n","        (embed_tokens): Embedding(151936, 2560)\n","        (layers): ModuleList(\n","          (0-35): 36 x Qwen3DecoderLayer(\n","            (self_attn): Qwen3Attention(\n","              (q_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=4096, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=4096, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (k_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=1024, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (v_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=1024, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=1024, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (o_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=4096, out_features=2560, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=4096, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=2560, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n","              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n","            )\n","            (mlp): Qwen3MLP(\n","              (gate_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=9728, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (up_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=2560, out_features=9728, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=2560, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=9728, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (down_proj): lora.Linear4bit(\n","                (base_layer): Linear4bit(in_features=9728, out_features=2560, bias=False)\n","                (lora_dropout): ModuleDict(\n","                  (default): Dropout(p=0.05, inplace=False)\n","                )\n","                (lora_A): ModuleDict(\n","                  (default): Linear(in_features=9728, out_features=8, bias=False)\n","                )\n","                (lora_B): ModuleDict(\n","                  (default): Linear(in_features=8, out_features=2560, bias=False)\n","                )\n","                (lora_embedding_A): ParameterDict()\n","                (lora_embedding_B): ParameterDict()\n","                (lora_magnitude_vector): ModuleDict()\n","              )\n","              (act_fn): SiLUActivation()\n","            )\n","            (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n","            (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n","          )\n","        )\n","        (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n","        (rotary_emb): Qwen3RotaryEmbedding()\n","      )\n","      (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n","    )\n","  )\n",")"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["# base tokenizer 로드\n","qwen_tokenizer = AutoTokenizer.from_pretrained(str(cfg.model_dir / \"qwen_summarizer_qlora\"), use_fast=True)\n","if qwen_tokenizer.pad_token is None:\n","    qwen_tokenizer.pad_token = qwen_tokenizer.eos_token\n","qwen_tokenizer.padding_side = \"right\"\n","\n","# base model 로드 (4bit)\n","qwen_model = AutoModelForCausalLM.from_pretrained(\n","    qwen_id,\n","    quantization_config=bnb_config,\n","    device_map=\"auto\",\n",")\n","qwen_model.eval()\n","\n","# LoRA adapter 적용\n","qwen_model = PeftModel.from_pretrained(qwen_model, str(cfg.model_dir / \"qwen_summarizer_qlora\"))\n","qwen_model.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gRZZVAlVHlqD"},"outputs":[],"source":["# qwen용 추론 함수\n","def summarize_qwen(text: str, instruction: str=cfg.sft_prompt, max_new_tokens: int=200) -> str:\n","    prompt = (\n","        \"### Instruction:\\n\"\n","        f\"{instruction}\\n\\n\"\n","        \"### Input:\\n\"\n","        f\"{text}\\n\\n\"\n","        \"### Output:\\n\"\n","    )\n","\n","    inputs = qwen_tokenizer(prompt, return_tensors=\"pt\").to(qwen_model.device)\n","\n","    with torch.no_grad():\n","        output_ids = qwen_model.generate(\n","            **inputs,\n","            max_new_tokens=max_new_tokens,\n","            do_sample=False,\n","        )\n","\n","    decoded = qwen_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","\n","    # 모델 출력에서 프롬프트 부분 제거 (Output 이후만 보기)\n","    if \"### Output:\" in decoded:\n","        decoded = decoded.split(\"### Output:\", 1)[-1].strip()\n","\n","    return decoded.strip()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ss6N1c_QHlqD","outputId":"1c984534-08ec-464c-feae-e89cb69825b6"},"outputs":[{"name":"stderr","output_type":"stream","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"name":"stdout","output_type":"stream","text":["Qwen 요약: 대한상공회의소 지속성장이니셔티브(SGI)는 8일 하나의 산업을 둘러싸고 나뭇가지처럼 얽혀있는 연관 규제를 도식화한 '규제트리'를 공개하고 \"이미 뒤처진 신산업 분야에서 경쟁국을 따라잡으려면 무엇보다 각종 규제를 정비하는 노력이 필요하다\"고 주장했으며, 이중삼중으로 쳐져 있는 규제를 중점관리해야 한다고 제안했다.\n"]}],"source":["print(\"Qwen 요약:\", summarize_qwen(sample_text))"]},{"cell_type":"markdown","metadata":{"id":"rzdKiNJgHlqE"},"source":["## 4.3. ROUGE 평가"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DmAG-aVtHlqE","outputId":"3addddc4-e9df-4f96-842a-c7e67a12fc36"},"outputs":[{"name":"stderr","output_type":"stream","text":["Downloading builder script: 6.14kB [00:00, 3.20MB/s]\n"]}],"source":["# ROUGE metric 로더\n","rouge = evaluate.load(\"rouge\")\n","\n","def compute_rouge(\n","    dataset,                                 # validation 데이터셋: [{\"text\":..., \"summary\":...}, ...]\n","    summarize_fn,                            # summarize_kobart or summarize_qwen\n","    n_samples=200,                           # 너무 오래걸리니까 일부만 샘플링\n","    max_new_tokens=cfg.summary_max_len,      # 생성 길이 제한(모델별로 조절)\n","    seed=cfg.seed\n","):\n","    \"\"\"\n","    validation set에서 ROUGE 점수를 계산\n","\n","    Args:\n","        dataset: List[Dict] 형태 (text/summary 포함)\n","        summarize_fn: 요약 생성 함수 (입력 text -> 요약문 str)\n","        n_samples: 평가에 사용할 샘플 개수 (전체가 많으면 일부만 사용)\n","        max_new_tokens: generate()에서 생성 최대 길이 (모델 길이 폭주 방지)\n","        seed: 셔플 시드\n","\n","    Returns:\n","        dict: rouge1/rouge2/rougeL/rougeLsum 점수\n","    \"\"\"\n","    # 샘플링 (평가 시간 단축)\n","    rng = random.Random(seed)\n","    idxs = list(range(len(dataset)))\n","    rng.shuffle(idxs)\n","    idxs = idxs[:n_samples]\n","\n","    preds = []\n","    refs = []\n","\n","    # 예측 요약 생성\n","    for i in tqdm(idxs, desc=\"Generating summaries\"):\n","        src = dataset[i][\"text\"]\n","        ref = dataset[i][\"summary\"]\n","\n","        pred = summarize_fn(src, max_new_tokens=max_new_tokens)\n","\n","        preds.append(pred)\n","        refs.append(ref)\n","\n","    # ROUGE 계산\n","    scores = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n","    return scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qc1KSHggHlqF","outputId":"c6f85ba7-d674-42cb-aca4-677e0a1e3b24"},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating summaries: 100%|██████████| 200/200 [03:55<00:00,  1.18s/it]"]},{"name":"stdout","output_type":"stream","text":["[KoBART ROUGE]\n","rouge1: 0.4055\n","rouge2: 0.1575\n","rougeL: 0.3908\n","rougeLsum: 0.3895\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# kobart rouge 점수 계산\n","kobart_rouge_scores = compute_rouge(\n","    dataset=kobart_ds[\"validation\"],\n","    summarize_fn=summarize_kobart,\n",")\n","\n","print(\"[KoBART ROUGE]\")\n","for k, v in kobart_rouge_scores.items():\n","    print(f\"{k}: {v:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FAWfWoIDHlqF","outputId":"90a66ba8-bdd5-46af-ae3a-a4e59b88f5a3"},"outputs":[{"name":"stderr","output_type":"stream","text":["Generating summaries: 100%|██████████| 100/100 [16:42<00:00, 10.02s/it]"]},{"name":"stdout","output_type":"stream","text":["[Qwen ROUGE]\n","rouge1: 0.3210\n","rouge2: 0.1155\n","rougeL: 0.3033\n","rougeLsum: 0.3024\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# kobart rouge 점수 계산\n","qwen_rouge_scores = compute_rouge(\n","    dataset=kobart_ds[\"validation\"],\n","    summarize_fn=summarize_qwen,\n","    n_samples=100,\n",")\n","\n","print(\"[Qwen ROUGE]\")\n","for k, v in qwen_rouge_scores.items():\n","    print(f\"{k}: {v:.4f}\")"]},{"cell_type":"markdown","source":["### 코멘트"],"metadata":{"id":"kSugBYiYIKSY"}},{"cell_type":"markdown","source":["- KoBART와 Qwen은 서로 다른 조건에서 훈련이 진행되었으며, 따라서 수치 비교는 정확하지 않습니다.\n","- 다만 KoBART는 훈련 시간 대비(KoBART는 약 25분, Qwen3-4B는 약 3시간 20분) 좋은 성능을 보였습니다.\n","- Qwen의 경우, 4B는 태스크 대비 과도하게 무거운 모델로 생각되며, 더 가벼운 모델이 바람직해 보입니다."],"metadata":{"id":"ZAhpU8HsIOay"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"a-ZON0uTHlqF"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[],"toc_visible":true},"kernelspec":{"display_name":".venv (3.12.3)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}